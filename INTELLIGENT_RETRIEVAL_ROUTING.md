# æ™ºèƒ½æ£€ç´¢è·¯ç”±æ–¹æ¡ˆ - è‡ªåŠ¨ç­›é€‰åˆ†é…chunksç»™å¯¹åº”æ¨¡å—

**æå‡ºæ—¶é—´**: 2025-12-25  
**æ ¸å¿ƒæ€æƒ³**: åœ¨æ£€ç´¢åï¼Œè‡ªåŠ¨åˆ¤æ–­æ¯ä¸ªchunkå±äºå“ªä¸ªæ¨¡å—ï¼Œåªå°†ç›¸å…³chunksåˆ†é…ç»™å¯¹åº”çš„Stage

---

## ğŸ¯ æ ¸å¿ƒç†å¿µ

### å½“å‰é—®é¢˜

```
æ£€ç´¢é˜¶æ®µï¼ˆæ‰€æœ‰Stageå…±ç”¨ï¼‰
  â†“
è·å¾—40ä¸ªchunksï¼ˆæ··åˆå†…å®¹ï¼‰
  â”œâ”€ 5ä¸ªåŸºæœ¬ä¿¡æ¯chunks
  â”œâ”€ 15ä¸ªæŠ€æœ¯å‚æ•°chunks
  â”œâ”€ 12ä¸ªå•†åŠ¡æ¡æ¬¾chunks
  â”œâ”€ 6ä¸ªè¯„åˆ†è§„åˆ™chunks
  â””â”€ 2ä¸ªæ— å…³chunks
  â†“
Stage 1 (base): æ”¶åˆ°å…¨éƒ¨40ä¸ªchunks âŒ
  â†’ åªéœ€è¦5ä¸ªï¼Œä½†è¦å¤„ç†40ä¸ª

Stage 2 (technical): æ”¶åˆ°å…¨éƒ¨40ä¸ªchunks âŒ
  â†’ åªéœ€è¦15ä¸ªï¼Œä½†è¦å¤„ç†40ä¸ª

Stage 3 (business): æ”¶åˆ°å…¨éƒ¨40ä¸ªchunks âŒ
  â†’ åªéœ€è¦12ä¸ªï¼Œä½†è¦å¤„ç†40ä¸ª

Stage 4 (scoring): æ”¶åˆ°å…¨éƒ¨40ä¸ªchunks âŒ
  â†’ åªéœ€è¦6ä¸ªï¼Œä½†è¦å¤„ç†40ä¸ª

é—®é¢˜: 
- LLMå¤„ç†å¤§é‡æ— å…³å†…å®¹ï¼Œé€Ÿåº¦æ…¢
- å‡†ç¡®åº¦é™ä½ï¼ˆå™ªéŸ³å¹²æ‰°ï¼‰
- æµªè´¹tokenæˆæœ¬
```

### ä¼˜åŒ–åçš„æ–¹æ¡ˆ

```
æ£€ç´¢é˜¶æ®µï¼ˆå…¨å±€æ£€ç´¢ï¼‰
  â†“
è·å¾—40ä¸ªchunksï¼ˆæ··åˆå†…å®¹ï¼‰
  â†“
æ™ºèƒ½è·¯ç”±/åˆ†ç±» âœ¨
  â”œâ”€ 5ä¸ª â†’ base_chunks
  â”œâ”€ 15ä¸ª â†’ technical_chunks
  â”œâ”€ 12ä¸ª â†’ business_chunks
  â””â”€ 6ä¸ª â†’ scoring_chunks
  â†“
Stage 1 (base): åªæ”¶åˆ°5ä¸ªç›¸å…³chunks âœ…
  â†’ Context: 6KB (åŸæ¥48KB)

Stage 2 (technical): åªæ”¶åˆ°15ä¸ªç›¸å…³chunks âœ…
  â†’ Context: 18KB (åŸæ¥48KB)

Stage 3 (business): åªæ”¶åˆ°12ä¸ªç›¸å…³chunks âœ…
  â†’ Context: 14KB (åŸæ¥48KB)

Stage 4 (scoring): åªæ”¶åˆ°6ä¸ªç›¸å…³chunks âœ…
  â†’ Context: 7KB (åŸæ¥48KB)

æ”¶ç›Š:
- Contextå‡å°‘60-85% âœ…
- LLMå¤„ç†é€Ÿåº¦æå‡50-70% âœ…
- å‡†ç¡®åº¦æå‡ï¼ˆå‡å°‘å™ªéŸ³ï¼‰ âœ…
- æˆæœ¬é™ä½60-85% âœ…
```

---

## ğŸ“‹ å®ç°æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆA: åŸºäºQueryçš„è‡ªç„¶åˆ†ç±»ï¼ˆâ­â­â­ ç®€å•ä½†ç²—ç³™ï¼‰

**åŸç†**: æ¯ä¸ªStageä½¿ç”¨ä¸“é—¨çš„æŸ¥è¯¢ï¼Œè‡ªç„¶è·å¾—ç›¸å…³chunks

```python
# Stage 1: åªç”¨baseç›¸å…³çš„æŸ¥è¯¢
queries_base = ["é¡¹ç›®åç§° æ‹›æ ‡äºº æŠ•æ ‡æˆªæ­¢æ—¶é—´ å¼€æ ‡æ—¶é—´"]
chunks_base = retrieve(queries_base, top_k=10)
â†’ è·å¾—10ä¸ªåŸºæœ¬ä¿¡æ¯ç›¸å…³çš„chunks

# Stage 2: åªç”¨technicalç›¸å…³çš„æŸ¥è¯¢
queries_tech = ["æŠ€æœ¯è¦æ±‚ æŠ€æœ¯è§„èŒƒ æŠ€æœ¯å‚æ•° è§„æ ¼å‹å·"]
chunks_tech = retrieve(queries_tech, top_k=20)
â†’ è·å¾—20ä¸ªæŠ€æœ¯å‚æ•°ç›¸å…³çš„chunks

# ä»¥æ­¤ç±»æ¨...
```

**ä¼˜ç‚¹**:
- âœ… å®ç°ç®€å•ï¼ˆ1-2å°æ—¶ï¼‰
- âœ… æ— éœ€é¢å¤–æ¨¡å‹
- âœ… é›¶å»¶è¿Ÿ

**ç¼ºç‚¹**:
- âŒ åˆ†ç±»ä¸å¤Ÿç²¾å‡†ï¼ˆqueryåŒ¹é…ä¸å®Œç¾ï¼‰
- âŒ å¯èƒ½æ¼æ‰ä¸€äº›ç›¸å…³å†…å®¹
- âŒ æ— æ³•å¤„ç†å¤šç±»åˆ«çš„chunks

**æ•ˆæœ**: Contextå‡å°‘30-50%

---

### æ–¹æ¡ˆB: å°æ¨¡å‹äºŒæ¬¡åˆ†ç±»ï¼ˆâ­â­â­â­â­ æ¨èï¼Œç²¾å‡†é«˜æ•ˆï¼‰

**åŸç†**: å…ˆå…¨å±€æ£€ç´¢ï¼Œå†ç”¨å°æ¨¡å‹å¿«é€Ÿåˆ†ç±»æ¯ä¸ªchunk

```python
# Step 1: å…¨å±€æ£€ç´¢ï¼ˆä¿è¯å¬å›ç‡ï¼‰
all_chunks = retrieve(all_queries, top_k=50)

# Step 2: ä½¿ç”¨å°æ¨¡å‹æ‰¹é‡åˆ†ç±»
classifier = FastClassifier()  # GPT-4o-mini æˆ– embeddingåˆ†ç±»å™¨
classifications = classifier.classify_batch(all_chunks, categories=[
    "base",        # åŸºæœ¬ä¿¡æ¯
    "technical",   # æŠ€æœ¯å‚æ•°
    "business",    # å•†åŠ¡æ¡æ¬¾
    "scoring",     # è¯„åˆ†è§„åˆ™
    "irrelevant"   # æ— å…³å†…å®¹
])

# Step 3: æŒ‰åˆ†ç±»åˆ†é…
chunks_by_stage = {
    "base": [chunk for chunk in all_chunks if classifications[chunk.id] == "base"],
    "technical": [chunk for chunk in all_chunks if classifications[chunk.id] == "technical"],
    "business": [chunk for chunk in all_chunks if classifications[chunk.id] == "business"],
    "scoring": [chunk for chunk in all_chunks if classifications[chunk.id] == "scoring"],
}

# Step 4: å„Stageä½¿ç”¨ä¸“å±chunks
Stage 1 â†’ chunks_by_stage["base"]
Stage 2 â†’ chunks_by_stage["technical"]
Stage 3 â†’ chunks_by_stage["business"]
Stage 4 â†’ chunks_by_stage["scoring"]
```

**åˆ†ç±»Promptç¤ºä¾‹**:

```python
CLASSIFICATION_PROMPT = """
ä½ æ˜¯æ‹›æ ‡æ–‡ä»¶å†…å®¹åˆ†ç±»ä¸“å®¶ã€‚è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µå±äºå“ªä¸ªç±»åˆ«ã€‚

ç±»åˆ«å®šä¹‰ï¼š
1. base - åŸºæœ¬ä¿¡æ¯ï¼šé¡¹ç›®åç§°ã€æ‹›æ ‡äººã€é‡‡è´­äººã€æŠ•æ ‡æˆªæ­¢æ—¶é—´ã€å¼€æ ‡æ—¶é—´ã€è”ç³»äººã€é¡¹ç›®é¢„ç®—ã€æœ€é«˜é™ä»·ã€ä¿è¯é‡‘ç­‰
2. technical - æŠ€æœ¯å‚æ•°ï¼šæŠ€æœ¯è¦æ±‚ã€æŠ€æœ¯è§„èŒƒã€è®¾å¤‡å‚æ•°ã€æ€§èƒ½æŒ‡æ ‡ã€åŠŸèƒ½è¦æ±‚ã€è§„æ ¼å‹å·ã€å“ç‰Œã€æè´¨ç­‰
3. business - å•†åŠ¡æ¡æ¬¾ï¼šä»˜æ¬¾æ–¹å¼ã€äº¤ä»˜æœŸã€è´¨ä¿æœŸã€éªŒæ”¶æ ‡å‡†ã€è¿çº¦è´£ä»»ã€å‘ç¥¨è¦æ±‚ã€åˆåŒæ¡æ¬¾ç­‰
4. scoring - è¯„åˆ†è§„åˆ™ï¼šè¯„æ ‡åŠæ³•ã€è¯„åˆ†æ ‡å‡†ã€è¯„å®¡ç»†åˆ™ã€åˆ†å€¼åˆ†é…ã€åŠ åˆ†é¡¹ã€å¦å†³æ¡ä»¶ç­‰
5. irrelevant - æ— å…³å†…å®¹ï¼šå°é¢ã€ç›®å½•ã€è¯´æ˜ã€å£°æ˜ç­‰

æ–‡æœ¬å†…å®¹ï¼š
{chunk_text}

è¯·åªè¿”å›ç±»åˆ«åç§°ï¼ˆbase/technical/business/scoring/irrelevantï¼‰ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""
```

**å®ç°ç»†èŠ‚**:

```python
class ChunkClassifier:
    """Chunkå†…å®¹åˆ†ç±»å™¨"""
    
    def __init__(self, model="gpt-4o-mini"):
        self.model = model
        self.cache = {}  # ç¼“å­˜åˆ†ç±»ç»“æœ
    
    async def classify_batch(
        self, 
        chunks: List[Chunk], 
        categories: List[str]
    ) -> Dict[str, str]:
        """
        æ‰¹é‡åˆ†ç±»chunks
        
        Returns:
            {chunk_id: category}
        """
        results = {}
        
        # æ£€æŸ¥ç¼“å­˜
        uncached_chunks = []
        for chunk in chunks:
            cache_key = self._get_cache_key(chunk)
            if cache_key in self.cache:
                results[chunk.chunk_id] = self.cache[cache_key]
            else:
                uncached_chunks.append(chunk)
        
        if not uncached_chunks:
            return results
        
        # æ‰¹é‡è°ƒç”¨LLMï¼ˆå¹¶å‘ï¼‰
        tasks = []
        for chunk in uncached_chunks:
            prompt = self._build_classification_prompt(chunk, categories)
            tasks.append(self._classify_one(chunk, prompt))
        
        classifications = await asyncio.gather(*tasks)
        
        # åˆå¹¶ç»“æœå’Œç¼“å­˜
        for chunk, category in zip(uncached_chunks, classifications):
            results[chunk.chunk_id] = category
            cache_key = self._get_cache_key(chunk)
            self.cache[cache_key] = category
        
        return results
    
    async def _classify_one(self, chunk: Chunk, prompt: str) -> str:
        """åˆ†ç±»å•ä¸ªchunk"""
        try:
            result = await llm_chat(
                messages=[{"role": "user", "content": prompt}],
                model_id=self.model,
                temperature=0.0,
                max_tokens=10  # åªéœ€è¦è¿”å›ä¸€ä¸ªç±»åˆ«å
            )
            category = result.strip().lower()
            return category if category in VALID_CATEGORIES else "irrelevant"
        except Exception as e:
            logger.warning(f"Classification failed for chunk {chunk.chunk_id}: {e}")
            return "irrelevant"
```

**ä¼˜ç‚¹**:
- âœ… åˆ†ç±»ç²¾å‡†ï¼ˆ90-95%å‡†ç¡®ç‡ï¼‰
- âœ… æ”¯æŒå¤šæ ‡ç­¾ï¼ˆä¸€ä¸ªchunkå¯å±äºå¤šä¸ªç±»åˆ«ï¼‰
- âœ… å¯è°ƒæ•´åˆ†ç±»ç­–ç•¥
- âœ… æœ‰ç¼“å­˜æœºåˆ¶

**ç¼ºç‚¹**:
- âŒ éœ€è¦é¢å¤–LLMè°ƒç”¨ï¼ˆä½†å¾ˆå¿«ï¼ŒGPT-4o-miniå¤„ç†50ä¸ªchunks < 5ç§’ï¼‰
- âŒ å¢åŠ ä¸€ç‚¹æˆæœ¬ï¼ˆä½†æ¯”å‡å°‘çš„Contextæˆæœ¬ä½å¾—å¤šï¼‰

**æ•ˆæœ**: Contextå‡å°‘60-85%

**æˆæœ¬åˆ†æ**:
```
åˆ†ç±»æˆæœ¬:
- 50ä¸ªchunks Ã— 1200å­— = 60,000å­— = çº¦15,000 tokens
- GPT-4o-miniè¾“å…¥: 15,000 tokens Ã— $0.15/1M = $0.00225
- GPT-4o-miniè¾“å‡º: 50ä¸ªç±»åˆ« Ã— 10 tokens = 500 tokens Ã— $0.6/1M = $0.0003
- æ€»è®¡: $0.0025 (çº¦2åˆ†é’±)

èŠ‚çœæˆæœ¬:
- Contextå‡å°‘70%: 48KB â†’ 14KB (æ¯ä¸ªStage)
- 4ä¸ªStageæ€»å…±èŠ‚çœ: 136KB tokens
- èŠ‚çœ: 136KB Ã— $0.01/1M â‰ˆ $0.14
- å‡€æ”¶ç›Š: $0.14 - $0.0025 = $0.1375 (æ¯æ¬¡æŠ½å–èŠ‚çœ1æ¯›4)

æ›´é‡è¦çš„æ˜¯æ—¶é—´æ”¶ç›Š:
- LLMå¤„ç†æ—¶é—´å‡å°‘50-70%
- 6-9åˆ†é’Ÿ â†’ 2-3åˆ†é’Ÿ âœ…âœ…
```

---

### æ–¹æ¡ˆC: åŸºäºEmbeddingçš„ç›¸ä¼¼åº¦åˆ†ç±»ï¼ˆâ­â­â­â­ å¿«é€Ÿç²¾å‡†ï¼‰

**åŸç†**: ä½¿ç”¨embeddingè®¡ç®—chunkä¸å„ç±»åˆ«çš„ç›¸ä¼¼åº¦

```python
# é¢„å®šä¹‰ç±»åˆ«çš„ä»£è¡¨æ€§æè¿°
category_descriptions = {
    "base": "é¡¹ç›®åŸºæœ¬ä¿¡æ¯ é¡¹ç›®åç§° æ‹›æ ‡äºº é‡‡è´­äºº æŠ•æ ‡æˆªæ­¢æ—¶é—´ å¼€æ ‡æ—¶é—´ é¢„ç®—é‡‘é¢ ä¿è¯é‡‘",
    "technical": "æŠ€æœ¯è¦æ±‚ æŠ€æœ¯è§„èŒƒ æŠ€æœ¯å‚æ•° è®¾å¤‡è§„æ ¼ æ€§èƒ½æŒ‡æ ‡ åŠŸèƒ½è¦æ±‚ å‹å·",
    "business": "å•†åŠ¡æ¡æ¬¾ ä»˜æ¬¾æ–¹å¼ äº¤ä»˜æ—¶é—´ è´¨ä¿æœŸ éªŒæ”¶æ ‡å‡† åˆåŒæ¡æ¬¾",
    "scoring": "è¯„åˆ†æ ‡å‡† è¯„æ ‡åŠæ³• è¯„å®¡ç»†åˆ™ åˆ†å€¼åˆ†é… è¯„åˆ†æƒé‡",
}

# è®¡ç®—ç±»åˆ«embeddingsï¼ˆåªéœ€è®¡ç®—ä¸€æ¬¡ï¼‰
category_embeddings = {
    cat: embed_text(desc) 
    for cat, desc in category_descriptions.items()
}

# åˆ†ç±»chunks
def classify_chunk_by_embedding(chunk: Chunk) -> str:
    chunk_embedding = embed_text(chunk.text)
    
    # è®¡ç®—ä¸å„ç±»åˆ«çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarities = {
        cat: cosine_similarity(chunk_embedding, cat_emb)
        for cat, cat_emb in category_embeddings.items()
    }
    
    # è¿”å›æœ€ç›¸ä¼¼çš„ç±»åˆ«
    best_category = max(similarities, key=similarities.get)
    
    # å¦‚æœç›¸ä¼¼åº¦å¤ªä½ï¼Œæ ‡è®°ä¸ºirrelevant
    if similarities[best_category] < 0.5:
        return "irrelevant"
    
    return best_category

# æ‰¹é‡åˆ†ç±»ï¼ˆéå¸¸å¿«ï¼ï¼‰
classifications = {
    chunk.chunk_id: classify_chunk_by_embedding(chunk)
    for chunk in all_chunks
}
```

**ä¼˜ç‚¹**:
- âœ… é€Ÿåº¦æå¿«ï¼ˆ50ä¸ªchunks < 1ç§’ï¼‰
- âœ… é›¶LLMè°ƒç”¨æˆæœ¬ï¼ˆåªç”¨embeddingï¼‰
- âœ… å¯ç¦»çº¿è®¡ç®—
- âœ… å‡†ç¡®ç‡ä¸­ç­‰ï¼ˆ75-85%ï¼‰

**ç¼ºç‚¹**:
- âŒ å‡†ç¡®ç‡ç•¥ä½äºLLMåˆ†ç±»
- âŒ ä¸æ”¯æŒå¤æ‚è¯­ä¹‰ç†è§£
- âŒ éœ€è¦è°ƒä¼˜ç›¸ä¼¼åº¦é˜ˆå€¼

**æ•ˆæœ**: Contextå‡å°‘50-70%

---

### æ–¹æ¡ˆD: åŸºäºè§„åˆ™çš„å¯å‘å¼åˆ†ç±»ï¼ˆâ­â­ å¿«ä½†ç²—ç³™ï¼‰

**åŸç†**: ä½¿ç”¨å…³é”®è¯å’Œè§„åˆ™è¿›è¡Œåˆ†ç±»

```python
CLASSIFICATION_RULES = {
    "base": {
        "keywords": ["é¡¹ç›®åç§°", "æ‹›æ ‡äºº", "é‡‡è´­äºº", "æŠ•æ ‡æˆªæ­¢", "å¼€æ ‡æ—¶é—´", 
                     "é¢„ç®—", "æœ€é«˜é™ä»·", "ä¿è¯é‡‘", "è”ç³»äºº"],
        "patterns": [
            r"é¡¹ç›®ç¼–å·[:ï¼š]\s*\S+",
            r"\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*æˆªæ­¢",
            r"æ‹›æ ‡äºº[:ï¼š].*æœ‰é™å…¬å¸",
        ]
    },
    "technical": {
        "keywords": ["æŠ€æœ¯è¦æ±‚", "æŠ€æœ¯è§„èŒƒ", "å‚æ•°", "è§„æ ¼", "å‹å·", 
                     "æ€§èƒ½", "åŠŸèƒ½", "é…ç½®", "CPU", "å†…å­˜"],
        "patterns": [
            r"â‰¥|â‰¤|ä¸ä½äº|ä¸å°äº",
            r"\d+\s*(GB|TB|MHz|GHz)",
            r"æŠ€æœ¯å‚æ•°è¡¨",
        ]
    },
    "business": {
        "keywords": ["ä»˜æ¬¾", "äº¤ä»˜", "è´¨ä¿", "éªŒæ”¶", "è¿çº¦", "åˆåŒ", 
                     "å‘ç¥¨", "ç¨è´¹", "è¿è¾“"],
        "patterns": [
            r"\d+%.*ä»˜æ¬¾",
            r"è´¨ä¿æœŸ[:ï¼š]\s*\d+",
            r"éªŒæ”¶.*å¤©å†…",
        ]
    },
    "scoring": {
        "keywords": ["è¯„åˆ†", "è¯„æ ‡", "è¯„å®¡", "åˆ†å€¼", "æƒé‡", "åŠ åˆ†", 
                     "æ‰£åˆ†", "å¦å†³"],
        "patterns": [
            r"\d+åˆ†",
            r"è¯„åˆ†æ ‡å‡†",
            r"ç»¼åˆè¯„åˆ†æ³•",
        ]
    }
}

def classify_chunk_by_rules(chunk: Chunk) -> str:
    """åŸºäºè§„åˆ™åˆ†ç±»chunk"""
    scores = {}
    
    for category, rules in CLASSIFICATION_RULES.items():
        score = 0
        
        # å…³é”®è¯åŒ¹é…
        for keyword in rules["keywords"]:
            if keyword in chunk.text:
                score += 1
        
        # æ­£åˆ™åŒ¹é…
        for pattern in rules["patterns"]:
            if re.search(pattern, chunk.text):
                score += 2  # æ¨¡å¼åŒ¹é…æƒé‡æ›´é«˜
        
        scores[category] = score
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„ç±»åˆ«
    if max(scores.values()) == 0:
        return "irrelevant"
    
    return max(scores, key=scores.get)
```

**ä¼˜ç‚¹**:
- âœ… é€Ÿåº¦æå¿«ï¼ˆ< 0.1ç§’ï¼‰
- âœ… é›¶æˆæœ¬
- âœ… å¯è§£é‡Šæ€§å¼º

**ç¼ºç‚¹**:
- âŒ å‡†ç¡®ç‡ä½ï¼ˆ60-70%ï¼‰
- âŒ ç»´æŠ¤æˆæœ¬é«˜ï¼ˆéœ€è¦ä¸æ–­è°ƒæ•´è§„åˆ™ï¼‰
- âŒ æ³›åŒ–èƒ½åŠ›å·®

**æ•ˆæœ**: Contextå‡å°‘40-60%

---

### æ–¹æ¡ˆE: æ··åˆæ–¹æ¡ˆï¼ˆâ­â­â­â­â­ æœ€ä¼˜ï¼Œæ¨èï¼‰

**åŸç†**: ç»“åˆå¤šç§æ–¹æ³•çš„ä¼˜åŠ¿

```python
async def classify_chunks_hybrid(chunks: List[Chunk]) -> Dict[str, str]:
    """
    æ··åˆåˆ†ç±»ç­–ç•¥ï¼š
    1. å…ˆç”¨è§„åˆ™å¿«é€Ÿç­›é€‰æ˜æ˜¾çš„cases
    2. æ¨¡ç³Šçš„casesç”¨embeddingåˆ†ç±»
    3. ä»ç„¶æ¨¡ç³Šçš„ç”¨å°æ¨¡å‹ç²¾ç¡®åˆ†ç±»
    """
    results = {}
    uncertain_chunks = []
    
    # Stage 1: è§„åˆ™åˆ†ç±»ï¼ˆå¿«é€Ÿå¤„ç†æ˜æ˜¾casesï¼‰
    for chunk in chunks:
        category, confidence = classify_by_rules_with_confidence(chunk)
        if confidence > 0.8:  # é«˜ç½®ä¿¡åº¦
            results[chunk.chunk_id] = category
        else:
            uncertain_chunks.append(chunk)
    
    logger.info(f"Rule-based classified {len(results)}/{len(chunks)} chunks")
    
    if not uncertain_chunks:
        return results
    
    # Stage 2: Embeddingåˆ†ç±»ï¼ˆå¤„ç†ä¸­ç­‰æ¨¡ç³Šcasesï¼‰
    still_uncertain = []
    for chunk in uncertain_chunks:
        category, confidence = classify_by_embedding_with_confidence(chunk)
        if confidence > 0.7:  # ä¸­ç­‰ç½®ä¿¡åº¦
            results[chunk.chunk_id] = category
        else:
            still_uncertain.append(chunk)
    
    logger.info(f"Embedding classified {len(uncertain_chunks)-len(still_uncertain)} chunks")
    
    if not still_uncertain:
        return results
    
    # Stage 3: LLMåˆ†ç±»ï¼ˆå¤„ç†éš¾casesï¼‰
    llm_results = await classify_by_llm(still_uncertain)
    results.update(llm_results)
    
    logger.info(f"LLM classified {len(still_uncertain)} chunks")
    
    return results
```

**ä¼˜ç‚¹**:
- âœ… å‡†ç¡®ç‡é«˜ï¼ˆ90-95%ï¼‰
- âœ… é€Ÿåº¦å¿«ï¼ˆå¤§éƒ¨åˆ†èµ°å¿«é€Ÿè·¯å¾„ï¼‰
- âœ… æˆæœ¬ä½ï¼ˆåªæœ‰å°‘é‡chunkséœ€è¦LLMï¼‰
- âœ… å¯é æ€§é«˜ï¼ˆå¤šé‡ä¿éšœï¼‰

**ç¼ºç‚¹**:
- âŒ å®ç°å¤æ‚åº¦é«˜

**æ•ˆæœ**: Contextå‡å°‘60-85%ï¼Œé€Ÿåº¦æœ€å¿«

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | å‡†ç¡®ç‡ | é€Ÿåº¦ | æˆæœ¬ | å®ç°éš¾åº¦ | Contextå‡å°‘ | æ¨èåº¦ |
|------|--------|------|------|----------|-------------|--------|
| **A: Queryåˆ†ç±»** | 60-70% | â­â­â­â­â­ | $0 | â­ ä½ | 30-50% | â­â­â­ |
| **B: LLMåˆ†ç±»** | 90-95% | â­â­â­ | $0.0025 | â­â­ ä¸­ | 60-85% | â­â­â­â­â­ |
| **C: Embeddingåˆ†ç±»** | 75-85% | â­â­â­â­â­ | $0 | â­â­ ä¸­ | 50-70% | â­â­â­â­ |
| **D: è§„åˆ™åˆ†ç±»** | 60-70% | â­â­â­â­â­ | $0 | â­ ä½ | 40-60% | â­â­ |
| **E: æ··åˆæ–¹æ¡ˆ** | 90-95% | â­â­â­â­ | $0.001 | â­â­â­ é«˜ | 60-85% | â­â­â­â­â­ |

---

## ğŸ¯ æ¨èå®æ–½æ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µ: Queryåˆ†ç±»ï¼ˆç«‹å³å¯å®æ–½ï¼‰

**å®æ–½æ­¥éª¤**:
1. ä¿®æ”¹ `extract_v2_service.py`ï¼Œæ¯ä¸ªStageä½¿ç”¨ä¸åŒæŸ¥è¯¢
2. æµ‹è¯•éªŒè¯æ•ˆæœ

**é¢„æœŸæ•ˆæœ**:
- Contextå‡å°‘30-50%
- å¼€å‘æ—¶é—´: 2å°æ—¶
- é€Ÿåº¦æå‡: 20-30%

---

### ç¬¬äºŒé˜¶æ®µ: æ·»åŠ LLMåˆ†ç±»ï¼ˆ1å‘¨å†…ï¼‰

**å®æ–½æ­¥éª¤**:
1. å®ç° `ChunkClassifier` ç±»
2. åœ¨æ£€ç´¢åæ·»åŠ åˆ†ç±»æ­¥éª¤
3. æ¯ä¸ªStageåªä½¿ç”¨å¯¹åº”ç±»åˆ«çš„chunks
4. æ·»åŠ ç¼“å­˜æœºåˆ¶

**ä»£ç ç¤ºä¾‹**:

```python
# backend/app/platform/extraction/chunk_classifier.py
class ChunkClassifier:
    """æ™ºèƒ½chunkåˆ†ç±»å™¨"""
    
    async def classify_batch(
        self, 
        chunks: List[Chunk]
    ) -> Dict[str, List[Chunk]]:
        """
        å°†chunksåˆ†ç±»åˆ°å„ä¸ªæ¨¡å—
        
        Returns:
            {
                "base": [chunk1, chunk2, ...],
                "technical": [chunk3, chunk4, ...],
                "business": [chunk5, chunk6, ...],
                "scoring": [chunk7, chunk8, ...]
            }
        """
        # å®ç°åˆ†ç±»é€»è¾‘
        ...

# backend/app/works/tender/extract_v2_service.py
async def _extract_project_info_staged_with_routing(
    self,
    project_id: str,
    ...
):
    # 1. å…¨å±€æ£€ç´¢
    all_chunks = await self.retriever.retrieve_all(...)
    
    # 2. æ™ºèƒ½åˆ†ç±»
    classifier = ChunkClassifier()
    chunks_by_category = await classifier.classify_batch(all_chunks)
    
    # 3. å„Stageä½¿ç”¨ä¸“å±chunks
    for stage_info in stages:
        stage_key = stage_info["key"]
        stage_chunks = chunks_by_category.get(stage_key, [])
        
        # æ„å»ºContextï¼ˆåªç”¨ç›¸å…³chunksï¼‰
        context = build_context(stage_chunks)
        
        # è°ƒç”¨LLM
        result = await self.engine.run(
            spec=spec,
            context=context,  # å°å¾—å¤šçš„context
            ...
        )
```

**é¢„æœŸæ•ˆæœ**:
- Contextå‡å°‘60-85%
- å¼€å‘æ—¶é—´: 8-12å°æ—¶
- é€Ÿåº¦æå‡: 50-70%
- å‡†ç¡®åº¦æå‡: 10-15%

---

### ç¬¬ä¸‰é˜¶æ®µ: ä¼˜åŒ–ä¸ºæ··åˆæ–¹æ¡ˆï¼ˆ2å‘¨å†…ï¼‰

**å®æ–½æ­¥éª¤**:
1. æ·»åŠ è§„åˆ™åˆ†ç±»å™¨ï¼ˆå¿«é€Ÿè·¯å¾„ï¼‰
2. æ·»åŠ embeddingåˆ†ç±»å™¨ï¼ˆä¸­é€Ÿè·¯å¾„ï¼‰
3. LLMåˆ†ç±»ä½œä¸ºå…œåº•ï¼ˆæ…¢é€Ÿä½†ç²¾ç¡®ï¼‰
4. å®ç°è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©

**é¢„æœŸæ•ˆæœ**:
- Contextå‡å°‘60-85%
- åˆ†ç±»é€Ÿåº¦: < 2ç§’ï¼ˆæ¯”çº¯LLMå¿«3-5å€ï¼‰
- æˆæœ¬é™ä½: 70%
- å‡†ç¡®ç‡: 90-95%

---

## ğŸ’° ROIåˆ†æ

### æ—¶é—´æ”¶ç›Š

```
å½“å‰ï¼ˆæ— åˆ†ç±»ï¼‰:
Stage 1: 2åˆ†é’Ÿ (48KB context)
Stage 2: 3åˆ†é’Ÿ (48KB context)
Stage 3: 2åˆ†é’Ÿ (48KB context)
Stage 4: 1.5åˆ†é’Ÿ (48KB context)
æ€»è®¡: 8.5åˆ†é’Ÿ

ä¼˜åŒ–åï¼ˆæ™ºèƒ½åˆ†ç±»ï¼‰:
Stage 1: 1åˆ†é’Ÿ (7KB context) â†“ 50%
Stage 2: 1.5åˆ†é’Ÿ (18KB context) â†“ 50%
Stage 3: 1åˆ†é’Ÿ (14KB context) â†“ 50%
Stage 4: 0.5åˆ†é’Ÿ (7KB context) â†“ 67%
åˆ†ç±»: 0.5åˆ†é’Ÿ
æ€»è®¡: 4.5åˆ†é’Ÿ â†“ 47% âœ…âœ…
```

### æˆæœ¬æ”¶ç›Š

```
æ¯æ¬¡æŠ½å–çš„tokenæˆæœ¬:
- Context: 4 Ã— 48KB = 192KB â‰ˆ 48,000 tokens
- è¾“å‡º: 4 Ã— 2KB = 8KB â‰ˆ 2,000 tokens
- æ€»è®¡: 50,000 tokens Ã— $0.01/1K = $0.50

ä¼˜åŒ–å:
- Context: 7+18+14+7 = 46KB â‰ˆ 11,500 tokens â†“ 76%
- è¾“å‡º: 2,000 tokens (ä¸å˜)
- åˆ†ç±»: 500 tokens (GPT-4o-mini)
- æ€»è®¡: 14,000 tokens Ã— $0.01/1K = $0.14 â†“ 72%

æ¯æ¬¡èŠ‚çœ: $0.36 (çº¦3æ¯›6)
æ¯å¤©100æ¬¡æŠ½å–: èŠ‚çœ$36
æ¯æœˆ: èŠ‚çœ$1080 âœ…
```

### å‡†ç¡®åº¦æ”¶ç›Š

```
å™ªéŸ³å‡å°‘:
- å½“å‰: æ¯ä¸ªStageæœ‰60-70%çš„æ— å…³chunks
- ä¼˜åŒ–å: æ¯ä¸ªStageåªæœ‰5-10%çš„æ— å…³chunks

å‡†ç¡®ç‡æå‡:
- åŸºæœ¬ä¿¡æ¯: 95% â†’ 98%
- æŠ€æœ¯å‚æ•°: 85% â†’ 92%
- å•†åŠ¡æ¡æ¬¾: 88% â†’ 95%
- è¯„åˆ†è§„åˆ™: 90% â†’ 96%

åŸå› : LLMä¸å†è¢«å¤§é‡æ— å…³ä¿¡æ¯å¹²æ‰°
```

---

## âœ¨ æœ€ç»ˆæ•ˆæœé¢„æµ‹

### æ€§èƒ½æå‡

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| **æ€»è€—æ—¶** | 8-9åˆ†é’Ÿ | 4-5åˆ†é’Ÿ | â†“ 50% |
| **Contextå¤§å°** | 192KB | 46KB | â†“ 76% |
| **Tokenæˆæœ¬** | $0.50 | $0.14 | â†“ 72% |
| **å‡†ç¡®ç‡** | 89% | 95% | â†‘ 6% |

### ä¸å…¶ä»–ä¼˜åŒ–çš„ç»„åˆæ•ˆæœ

```
åŸºçº¿: 10-15åˆ†é’Ÿ
  â†“ (P0: å‡å°‘æ£€ç´¢é‡)
6-9åˆ†é’Ÿ â†“ 40%
  â†“ (P1: æ™ºèƒ½åˆ†ç±»è·¯ç”±) âœ¨
3-4.5åˆ†é’Ÿ â†“ 50%
  â†“ (P1: åˆå¹¶Stage)
2-3åˆ†é’Ÿ â†“ 33%
  â†“ (P2: æ›´å¿«æ¨¡å‹)
1-1.5åˆ†é’Ÿ â†“ 50%

æœ€ç»ˆ: 1-1.5åˆ†é’Ÿ âœ…âœ…âœ…âœ…
æ¯”åŸæ¥å¿« 10å€ï¼
```

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨

### å¿«é€ŸéªŒè¯ï¼ˆä»Šå¤©ï¼‰

```bash
# 1. ä¿®æ”¹æŸ¥è¯¢é…ç½®ï¼Œè®©æ¯ä¸ªStageç”¨ä¸åŒæŸ¥è¯¢
# backend/app/works/tender/extract_v2_service.py

# 2. æµ‹è¯•æ•ˆæœ
# é€‰æ‹©ä¸€ä¸ªé¡¹ç›®ï¼Œè®°å½•å„Stageçš„è€—æ—¶å’Œå‡†ç¡®åº¦

# 3. å¯¹æ¯”æ•°æ®
# Before: Stage 1: 2min, Stage 2: 3min, ...
# After: Stage 1: 1.5min, Stage 2: 2.5min, ...
```

### æ­£å¼å®æ–½ï¼ˆæœ¬å‘¨ï¼‰

1. âœ… å®ç° `ChunkClassifier` ç±»
2. âœ… é›†æˆåˆ° `extract_v2_service.py`
3. âœ… æ·»åŠ ç¼“å­˜æœºåˆ¶
4. âœ… A/Bæµ‹è¯•éªŒè¯

---

**æ–‡æ¡£å®Œæˆæ—¶é—´**: 2025-12-25  
**æ¨èä¼˜å…ˆçº§**: P1ï¼ˆé«˜ä¼˜å…ˆçº§ï¼Œ1å‘¨å†…å®æ–½ï¼‰  
**é¢„æœŸæ”¶ç›Š**: é€Ÿåº¦æå‡50%ï¼Œæˆæœ¬é™ä½72%ï¼Œå‡†ç¡®åº¦æå‡6%
