"""
çŸ¥è¯†åº“æ–‡æ¡£è¿ç§»è„šæœ¬

åŠŸèƒ½ï¼š
1. ä»æ—§çš„ kb_documents è¡¨è¯»å–æ–‡æ¡£
2. è¿ç§»åˆ°æ–°çš„ documents/document_versions/doc_segments è¡¨
3. ä¿ç•™åŸæœ‰çš„å‘é‡æ•°æ®ï¼ˆMilvusï¼‰
4. è¿ç§»å®Œæˆåå¯é€‰æ‹©æ¸…ç†æ—§æ•°æ®

ä½¿ç”¨æ–¹æ³•ï¼š
    python backend/scripts/migrate_kb_documents_to_docstore.py [--delete-old]
    
å‚æ•°ï¼š
    --delete-old: è¿ç§»å®Œæˆååˆ é™¤æ—§çš„ kb_documents è¡¨ä¸­çš„æ•°æ®
    --dry-run: åªæ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®åº“
    --kb-id: åªè¿ç§»æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£
"""
import sys
import os
import asyncio
import hashlib
from datetime import datetime
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from app.services.db.postgres import _get_pool, init_db
from psycopg.types.json import Json


class KBDocumentMigrator:
    """çŸ¥è¯†åº“æ–‡æ¡£è¿ç§»å™¨"""
    
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.pool = _get_pool()
        self.stats = {
            'total': 0,
            'migrated': 0,
            'skipped': 0,
            'failed': 0,
            'errors': []
        }
    
    def get_legacy_documents(self, kb_id: Optional[str] = None) -> List[Dict[str, Any]]:
        """è·å–æ—§è¡¨ä¸­çš„æ–‡æ¡£"""
        with self.pool.connection() as conn:
            with conn.cursor() as cur:
                if kb_id:
                    query = """
                        SELECT id, kb_id, filename, source, status, created_at, updated_at, 
                               meta_json, kb_category
                        FROM kb_documents
                        WHERE kb_id = %s
                        ORDER BY created_at
                    """
                    cur.execute(query, (kb_id,))
                else:
                    query = """
                        SELECT id, kb_id, filename, source, status, created_at, updated_at, 
                               meta_json, kb_category
                        FROM kb_documents
                        ORDER BY created_at
                    """
                    cur.execute(query)
                
                return [dict(row) for row in cur.fetchall()]
    
    def get_doc_segments_by_doc_id(self, old_doc_id: str) -> List[Dict[str, Any]]:
        """è·å–æ–‡æ¡£å¯¹åº”çš„segmentsï¼ˆä»doc_segmentsè¡¨ï¼Œé€šè¿‡chunkæ˜ å°„ï¼‰"""
        with self.pool.connection() as conn:
            with conn.cursor() as cur:
                # é€šè¿‡ kb_chunks è¡¨æ‰¾åˆ°å¯¹åº”çš„ chunk_id
                cur.execute("""
                    SELECT chunk_id, content, position
                    FROM kb_chunks
                    WHERE doc_id = %s
                    ORDER BY position
                """, (old_doc_id,))
                
                chunks = cur.fetchall()
                return [dict(chunk) for chunk in chunks]
    
    def calculate_file_hash(self, content: str) -> str:
        """è®¡ç®—æ–‡ä»¶å†…å®¹çš„SHA256å“ˆå¸Œ"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def migrate_document(self, legacy_doc: Dict[str, Any]) -> bool:
        """è¿ç§»å•ä¸ªæ–‡æ¡£"""
        old_doc_id = legacy_doc['id']
        kb_id = legacy_doc['kb_id']
        filename = legacy_doc['filename']
        kb_category = legacy_doc.get('kb_category', 'general_doc')
        
        print(f"\nğŸ“„ è¿ç§»æ–‡æ¡£: {filename} (id={old_doc_id}, kb_id={kb_id})")
        
        try:
            # 1. è·å–æ–‡æ¡£çš„æ‰€æœ‰chunksï¼ˆç”¨äºè®¡ç®—content hashï¼‰
            chunks = self.get_doc_segments_by_doc_id(old_doc_id)
            
            if not chunks:
                print(f"  âš ï¸  æ–‡æ¡£æ²¡æœ‰chunksï¼Œå¯èƒ½å·²ç»è¢«åˆ é™¤æˆ–ä»æœªå…¥åº“ï¼Œè·³è¿‡")
                self.stats['skipped'] += 1
                return False
            
            # 2. åˆå¹¶æ‰€æœ‰chunkå†…å®¹ä½œä¸ºæ–‡æ¡£å†…å®¹ï¼ˆç”¨äºè®¡ç®—hashï¼‰
            full_content = "\n".join([chunk['content'] for chunk in chunks])
            content_hash = self.calculate_file_hash(full_content)
            
            if self.dry_run:
                print(f"  [DRY RUN] å°†åˆ›å»º document + {len(chunks)} segments")
                self.stats['migrated'] += 1
                return True
            
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    # 3. åˆ›å»º document è®°å½•
                    doc_id = old_doc_id  # ä¿æŒç›¸åŒçš„IDï¼Œé¿å…å¤–é”®å¼•ç”¨é—®é¢˜
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    cur.execute("SELECT id FROM documents WHERE id = %s", (doc_id,))
                    if cur.fetchone():
                        print(f"  â­ï¸  æ–‡æ¡£å·²å­˜åœ¨äºæ–°è¡¨ï¼Œè·³è¿‡")
                        self.stats['skipped'] += 1
                        return False
                    
                    # åˆ›å»º document
                    cur.execute("""
                        INSERT INTO documents (id, namespace, doc_type, owner_id, created_at, meta_json)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        doc_id,
                        'kb',  # namespace
                        kb_category,  # doc_type
                        None,  # owner_id (çŸ¥è¯†åº“æ–‡æ¡£æ²¡æœ‰owner)
                        legacy_doc['created_at'],
                        Json({
                            'kb_id': kb_id,
                            'kb_category': kb_category,
                            'source': legacy_doc.get('source', 'upload'),
                            'legacy_migration': True,
                            'migrated_at': datetime.now().isoformat()
                        })
                    ))
                    print(f"  âœ… åˆ›å»º document: {doc_id}")
                    
                    # 4. åˆ›å»º document_version è®°å½•
                    doc_version_id = f"{doc_id}_v1"
                    cur.execute("""
                        INSERT INTO document_versions (id, document_id, sha256, filename, storage_path, created_at)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        doc_version_id,
                        doc_id,
                        content_hash,
                        filename,
                        None,  # storage_path (æ—§æ•°æ®æ²¡æœ‰å­˜å‚¨è·¯å¾„)
                        legacy_doc['created_at']
                    ))
                    print(f"  âœ… åˆ›å»º document_version: {doc_version_id}")
                    
                    # 5. åˆ›å»º doc_segments è®°å½•
                    for idx, chunk in enumerate(chunks):
                        segment_id = f"{doc_version_id}_seg{idx}"
                        cur.execute("""
                            INSERT INTO doc_segments (id, doc_version_id, segment_no, content_text, 
                                                     meta_json, created_at)
                            VALUES (%s, %s, %s, %s, %s, %s)
                        """, (
                            segment_id,
                            doc_version_id,
                            idx,
                            chunk['content'],
                            Json({
                                'chunk_id': chunk['chunk_id'],  # ä¿ç•™åŸchunk_idä»¥ä¾¿è¿½æº¯
                                'position': chunk['position']
                            }),
                            legacy_doc['created_at']
                        ))
                    
                    print(f"  âœ… åˆ›å»º {len(chunks)} ä¸ª doc_segments")
                    
                    conn.commit()
                    self.stats['migrated'] += 1
                    return True
        
        except Exception as e:
            error_msg = f"è¿ç§»å¤±è´¥ {filename}: {str(e)}"
            print(f"  âŒ {error_msg}")
            self.stats['failed'] += 1
            self.stats['errors'].append(error_msg)
            return False
    
    def delete_legacy_document(self, doc_id: str):
        """åˆ é™¤æ—§è¡¨ä¸­çš„æ–‡æ¡£è®°å½•"""
        if self.dry_run:
            print(f"  [DRY RUN] å°†åˆ é™¤æ—§è¡¨è®°å½•: {doc_id}")
            return
        
        with self.pool.connection() as conn:
            with conn.cursor() as cur:
                # kb_documents è¡¨çš„å¤–é”®ä¼šçº§è”åˆ é™¤ kb_chunks
                cur.execute("DELETE FROM kb_documents WHERE id = %s", (doc_id,))
                conn.commit()
                print(f"  ğŸ—‘ï¸  å·²åˆ é™¤æ—§è¡¨è®°å½•: {doc_id}")
    
    def run(self, kb_id: Optional[str] = None, delete_old: bool = False):
        """è¿è¡Œè¿ç§»"""
        print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘  ğŸ“¦ çŸ¥è¯†åº“æ–‡æ¡£è¿ç§»å·¥å…·                                                              â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        
        if self.dry_run:
            print("\nâš ï¸  DRY RUN æ¨¡å¼ï¼šåªæ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®åº“\n")
        
        # 1. è·å–å¾…è¿ç§»çš„æ–‡æ¡£
        print("\nğŸ“‹ æ­£åœ¨æ‰«ææ—§è¡¨...")
        legacy_docs = self.get_legacy_documents(kb_id)
        self.stats['total'] = len(legacy_docs)
        
        if not legacy_docs:
            print("âœ… æ²¡æœ‰éœ€è¦è¿ç§»çš„æ–‡æ¡£")
            return
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(legacy_docs)} ä¸ªå¾…è¿ç§»æ–‡æ¡£")
        
        if kb_id:
            print(f"ğŸ¯ åªè¿ç§»çŸ¥è¯†åº“: {kb_id}")
        
        # 2. é€ä¸ªè¿ç§»
        print("\nğŸš€ å¼€å§‹è¿ç§»...\n")
        for doc in legacy_docs:
            success = self.migrate_document(doc)
            
            # å¦‚æœè¿ç§»æˆåŠŸä¸”éœ€è¦åˆ é™¤æ—§æ•°æ®
            if success and delete_old:
                self.delete_legacy_document(doc['id'])
        
        # 3. è¾“å‡ºç»Ÿè®¡
        print("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        print("â•‘  ğŸ“Š è¿ç§»ç»Ÿè®¡                                                                         â•‘")
        print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        print(f"æ€»è®¡:   {self.stats['total']}")
        print(f"æˆåŠŸ:   {self.stats['migrated']} âœ…")
        print(f"è·³è¿‡:   {self.stats['skipped']} â­ï¸")
        print(f"å¤±è´¥:   {self.stats['failed']} âŒ")
        
        if self.stats['errors']:
            print("\nâŒ é”™è¯¯åˆ—è¡¨:")
            for error in self.stats['errors'][:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªé”™è¯¯
                print(f"  - {error}")
            if len(self.stats['errors']) > 10:
                print(f"  ... è¿˜æœ‰ {len(self.stats['errors']) - 10} ä¸ªé”™è¯¯")
        
        if delete_old and not self.dry_run:
            print(f"\nğŸ—‘ï¸  å·²åˆ é™¤ {self.stats['migrated']} ä¸ªæ—§è¡¨è®°å½•")
        
        print("\nâœ… è¿ç§»å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='çŸ¥è¯†åº“æ–‡æ¡£è¿ç§»å·¥å…·')
    parser.add_argument('--delete-old', action='store_true', 
                       help='è¿ç§»å®Œæˆååˆ é™¤æ—§çš„ kb_documents è¡¨ä¸­çš„æ•°æ®')
    parser.add_argument('--dry-run', action='store_true',
                       help='åªæ¨¡æ‹Ÿè¿è¡Œï¼Œä¸å®é™…ä¿®æ”¹æ•°æ®åº“')
    parser.add_argument('--kb-id', type=str,
                       help='åªè¿ç§»æŒ‡å®šçŸ¥è¯†åº“çš„æ–‡æ¡£')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®åº“
    init_db()
    
    # åˆ›å»ºè¿ç§»å™¨å¹¶è¿è¡Œ
    migrator = KBDocumentMigrator(dry_run=args.dry_run)
    migrator.run(kb_id=args.kb_id, delete_old=args.delete_old)


if __name__ == '__main__':
    main()

