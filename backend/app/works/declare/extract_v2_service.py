"""
ç”³æŠ¥ä¹¦ V2 æŠ½å–æœåŠ¡
åŸºäºå¹³å° ExtractionEngine
"""
import logging
from typing import Any, Dict, Optional

from psycopg_pool import ConnectionPool

from app.platform.extraction.engine import ExtractionEngine
from app.platform.retrieval.facade import RetrievalFacade
from app.services.embedding_provider_store import get_embedding_store
from app.works.declare.extraction_specs.requirements_v2 import build_requirements_spec
from app.works.declare.extraction_specs.directory_v2 import build_directory_spec

logger = logging.getLogger(__name__)


class DeclareExtractV2Service:
    """ç”³æŠ¥ä¹¦ V2 æŠ½å–æœåŠ¡"""
    
    def __init__(self, pool: ConnectionPool, llm_orchestrator: Any = None):
        self.pool = pool
        self.retriever = RetrievalFacade(pool)
        self.llm = llm_orchestrator
        self.engine = ExtractionEngine()
    
    async def extract_requirements(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æŠ½å–ç”³æŠ¥è¦æ±‚
        
        Returns:
            {
                "data": {"eligibility_conditions": [...], "materials_required": [...], ...},
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "retrieval_trace": {...}
            }
        """
        logger.info(f"DeclareExtractV2: extract_requirements start project_id={project_id}")
        
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        spec = build_requirements_spec()
        
        result = await self.engine.run(
            spec=spec,
            retriever=self.retriever,
            llm=self.llm,
            project_id=project_id,
            model_id=model_id,
            run_id=run_id,
            embedding_provider=embedding_provider,
        )
        
        logger.info(
            f"DeclareExtractV2: extract_requirements done "
            f"eligibility_count={len(result.data.get('eligibility_conditions', [])) if isinstance(result.data, dict) else 0} "
            f"evidence={len(result.evidence_chunk_ids)}"
        )
        
        return {
            "data": result.data,
            "evidence_chunk_ids": result.evidence_chunk_ids,
            "evidence_spans": result.evidence_spans,
            "retrieval_trace": result.retrieval_trace.__dict__ if result.retrieval_trace else {}
        }
    
    async def generate_directory(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç”³æŠ¥ä¹¦ç›®å½•
        
        Returns:
            {
                "data": {"nodes": [...]},
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "retrieval_trace": {...}
            }
        """
        logger.info(f"DeclareExtractV2: generate_directory start project_id={project_id}")
        
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        spec = build_directory_spec()
        
        result = await self.engine.run(
            spec=spec,
            retriever=self.retriever,
            llm=self.llm,
            project_id=project_id,
            model_id=model_id,
            run_id=run_id,
            embedding_provider=embedding_provider,
        )
        
        logger.info(
            f"DeclareExtractV2: generate_directory done "
            f"nodes_count={len(result.data.get('nodes', [])) if isinstance(result.data, dict) else 0} "
            f"evidence={len(result.evidence_chunk_ids)}"
        )
        
        return {
            "data": result.data,
            "evidence_chunk_ids": result.evidence_chunk_ids,
            "evidence_spans": result.evidence_spans,
            "retrieval_trace": result.retrieval_trace.__dict__ if result.retrieval_trace else {}
        }
    
    async def autofill_section(
        self,
        project_id: str,
        model_id: Optional[str],
        node_title: str,
        requirements_summary: str = "",
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è‡ªåŠ¨å¡«å……å•ä¸ªç« èŠ‚ï¼ˆè¿ç§»åˆ°ç»Ÿä¸€æ¡†æ¶ï¼‰
        
        Args:
            node_title: ç« èŠ‚æ ‡é¢˜
            requirements_summary: ç”³æŠ¥è¦æ±‚æ‘˜è¦
        
        Returns:
            {
                "data": {"content_md": "...", "summary": "...", ...},
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "retrieval_trace": {...}
            }
        """
        logger.info(f"DeclareExtractV2: autofill_section (unified) start project_id={project_id} node_title={node_title}")
        
        # å§”æ‰˜ç»™ç»Ÿä¸€æ¡†æ¶çš„å®ç°ï¼ˆé»˜è®¤level=3ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        result = await self.autofill_section_unified(
            project_id=project_id,
            model_id=model_id,
            node_title=node_title,
            node_level=3,  # é»˜è®¤å±‚çº§
            requirements_summary=requirements_summary,
            run_id=run_id,
        )
        
        # è½¬æ¢è¿”å›æ ¼å¼ä»¥ä¿æŒå‘åå…¼å®¹
        return {
            "data": result["data"],
            "evidence_chunk_ids": result.get("evidence_chunk_ids", []),
            "evidence_spans": [],  # ç»Ÿä¸€æ¡†æ¶ä¸ä½¿ç”¨evidence_spans
            "retrieval_trace": result.get("retrieval_trace", {})
        }
    
    async def autofill_section_unified(
        self,
        project_id: str,
        model_id: Optional[str],
        node_title: str,
        node_level: int,
        requirements_summary: str = "",
        requirements_dict: Optional[Dict[str, Any]] = None,  # âœ… æ–°å¢ï¼šç»“æ„åŒ–requirements
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        è‡ªåŠ¨å¡«å……å•ä¸ªç« èŠ‚ï¼ˆä½¿ç”¨ç»Ÿä¸€ç»„ä»¶ï¼‰
        
        Args:
            project_id: é¡¹ç›®ID
            model_id: æ¨¡å‹ID
            node_title: ç« èŠ‚æ ‡é¢˜
            node_level: ç« èŠ‚å±‚çº§
            requirements_summary: ç”³æŠ¥è¦æ±‚æ‘˜è¦
            run_id: è¿è¡Œè®°å½•ID
        
        Returns:
            {
                "data": {"content_md": "...", "confidence": "...", ...},
                "evidence_chunk_ids": [...],
                "quality_metrics": {...}
            }
        """
        from app.services.generation import (
            DocumentRetriever,
            RetrievalContext,
            PromptBuilder,
            PromptContext,
            ContentGenerator,
            GenerationContext,
            QualityAssessor
        )
        from app.services.dao.declare_dao import DeclareDAO
        
        logger.info(f"DeclareExtractV2: autofill_section_unified start project_id={project_id} node_title={node_title}")
        
        # Step 1: è·å–é¡¹ç›®ä¿¡æ¯
        dao = DeclareDAO(self.pool)
        proj = dao.get_project(project_id)
        if not proj:
            raise ValueError(f"é¡¹ç›®ä¸å­˜åœ¨: {project_id}")
        
        kb_id = proj.get("kb_id")
        if not kb_id:
            raise ValueError(f"é¡¹ç›®æœªç»‘å®šçŸ¥è¯†åº“: {project_id}")
        
        # âœ… ä¼˜å…ˆä½¿ç”¨ç»“æ„åŒ–çš„requirements_dictï¼Œå¦åˆ™ä½¿ç”¨æ—§çš„requirements_summary
        if requirements_dict is None:
            requirements_dict = {}
        if not requirements_dict and requirements_summary:
            requirements_dict = {"summary": requirements_summary}
        
        # è·å–èŠ‚ç‚¹å…ƒæ•°æ®ï¼ˆåŒ…å«notesç­‰ä¿¡æ¯ï¼‰
        section_metadata = {}
        try:
            # ä»æ´»è·ƒç›®å½•ä¸­æŸ¥æ‰¾è¯¥èŠ‚ç‚¹
            nodes = dao.get_active_directory_nodes(project_id)
            for node in nodes:
                if node.get("title") == node_title and node.get("level") == node_level:
                    meta_json = node.get("meta_json", {})
                    if isinstance(meta_json, dict):
                        section_metadata = meta_json
                    break
            logger.info(f"DeclareExtractV2: section_metadata={section_metadata}")
        except Exception as e:
            logger.warning(f"DeclareExtractV2: Failed to get section metadata: {e}")
        
        # Step 2: æ£€ç´¢ç›¸å…³èµ„æ–™ï¼ˆä½¿ç”¨ç»Ÿä¸€ç»„ä»¶ï¼‰
        retriever = DocumentRetriever(self.pool)
        retrieval_context = RetrievalContext(
            kb_id=kb_id,
            section_title=node_title,
            section_level=node_level,
            document_type="declare",
            requirements=requirements_dict
        )
        retrieval_result = await retriever.retrieve(retrieval_context, top_k=10)
        
        # ğŸ” DEBUG: æ£€æŸ¥æ£€ç´¢ç»“æœ
        logger.info(f"[DEBUG] æ£€ç´¢ç»“æœ: {len(retrieval_result.chunks)} ä¸ªchunk")
        if retrieval_result.chunks:
            first_chunk = retrieval_result.chunks[0]
            logger.info(f"[DEBUG] ç¬¬ä¸€ä¸ªchunkå†…å®¹: {first_chunk.get('text', '')[:200]}")
            logger.info(f"[DEBUG] has_relevant: {retrieval_result.has_relevant}")
        else:
            logger.warning(f"[DEBUG] âš ï¸  æ£€ç´¢ç»“æœä¸ºç©ºï¼")
        
        # Step 3: æ„å»ºPromptï¼ˆä½¿ç”¨ç»Ÿä¸€ç»„ä»¶ï¼‰
        prompt_builder = PromptBuilder()
        prompt_context = PromptContext(
            document_type="declare",
            section_title=node_title,
            section_level=node_level,
            project_info={},  # Declareä¸€èˆ¬ä¸éœ€è¦project_info
            requirements=requirements_dict,
            retrieval_result=retrieval_result,
            section_metadata=section_metadata  # ä¼ é€’ç« èŠ‚å…ƒæ•°æ®ï¼ˆåŒ…å«notesï¼‰
        )
        prompt = prompt_builder.build(prompt_context)
        
        # Step 4: ç”Ÿæˆå†…å®¹ï¼ˆä½¿ç”¨ç»Ÿä¸€ç»„ä»¶ï¼‰
        generator = ContentGenerator(self.llm)
        gen_context = GenerationContext(
            document_type="declare",
            section_title=node_title,
            prompt=prompt,
            model_id=model_id
        )
        generation_result = await generator.generate(gen_context)
        
        # Step 5: è¯„ä¼°è´¨é‡ï¼ˆä½¿ç”¨ç»Ÿä¸€ç»„ä»¶ï¼‰
        assessor = QualityAssessor()
        quality_metrics = assessor.assess(
            generation_result,
            retrieval_result,
            node_level
        )
        
        # Step 6: è®°å½•è´¨é‡æŒ‡æ ‡
        logger.info(
            f"DeclareExtractV2: autofill_section_unified done "
            f"content_length={generation_result.word_count} "
            f"evidence={len(retrieval_result.chunks)} "
            f"quality={quality_metrics.overall_score:.2f}"
        )
        
        return {
            "data": {
                "content_md": generation_result.content,
                "confidence": generation_result.confidence,
                "word_count": generation_result.word_count
            },
            "evidence_chunk_ids": retrieval_result.get_chunk_ids(),
            "quality_metrics": quality_metrics.to_dict(),
            "retrieval_trace": {
                "query_strategy": retrieval_result.retrieval_strategy,
                "chunk_count": len(retrieval_result.chunks),
                "quality_score": retrieval_result.quality_score
            }
        }

