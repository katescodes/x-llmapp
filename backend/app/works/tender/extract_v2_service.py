"""
æ–°æŠ½å–æœåŠ¡ (v2) - Step 3
åŸºäºå¹³å° ExtractionEngine çš„é¡¹ç›®ä¿¡æ¯/é£é™©æŠ½å–/ç›®å½•ç”Ÿæˆ
"""
import logging
from typing import Any, Dict, List, Optional

from psycopg_pool import ConnectionPool

from app.platform.extraction.engine import ExtractionEngine
from app.platform.retrieval.facade import RetrievalFacade
from app.services.embedding_provider_store import get_embedding_store
from .schemas.directory_v2 import build_directory_spec_async

logger = logging.getLogger(__name__)


class ExtractV2Service:
    """æ–°æŠ½å–æœåŠ¡ - ä½¿ç”¨å¹³å° ExtractionEngine"""
    
    def __init__(self, pool: ConnectionPool, llm_orchestrator: Any = None):
        self.pool = pool
        self.retriever = RetrievalFacade(pool)
        self.llm = llm_orchestrator
        self.engine = ExtractionEngine()
        # åˆ›å»ºDAOç”¨äºæ›´æ–°runçŠ¶æ€å’Œä¿å­˜æ•°æ®
        from app.services.dao.tender_dao import TenderDAO
        self.dao = TenderDAO(pool)
    
    async def extract_project_info_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æŠ½å–é¡¹ç›®ä¿¡æ¯ (V3) - å…­é˜¶æ®µé¡ºåº/å¹¶è¡ŒæŠ½å–
        
        ä½¿ç”¨åŸºäºChecklistçš„P0+P1ä¸¤é˜¶æ®µæå–æ¡†æ¶
        
        Args:
            project_id: é¡¹ç›®ID
            model_id: æ¨¡å‹ID
            run_id: è¿è¡ŒID
        
        Returns:
            {
                "schema_version": "tender_info_v3",
                "project_overview": {...},
                "bidder_qualification": {...},
                "evaluation_and_scoring": {...},
                "business_terms": {...},
                "technical_requirements": {...},
                "document_preparation": {...},
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "retrieval_trace": {...}
            }
        """
        logger.info(f"ExtractV2: extract_project_info_v2 start project_id={project_id}")
        
        # è·å– embedding provider
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–æ˜¯å¦å¯ç”¨å¹¶è¡Œ
        import os
        parallel_enabled = os.getenv("EXTRACT_PROJECT_INFO_PARALLEL", "false").lower() in ("true", "1", "yes")
        
        return await self._extract_project_info_staged(
            project_id=project_id,
            model_id=model_id,
            run_id=run_id,
            embedding_provider=embedding_provider,
            parallel=parallel_enabled,
        )
    
    async def _extract_project_info_staged(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str],
        embedding_provider: str,
        parallel: bool = False,
    ) -> Dict[str, Any]:
        """
        å…­é˜¶æ®µé¡ºåºæŠ½å–é¡¹ç›®ä¿¡æ¯ï¼ˆV3ç‰ˆæœ¬ - Checklist-basedï¼‰
        
        âœ¨ æ–°æ–¹æ³•ï¼šåŸºäºChecklistçš„P0+P1ä¸¤é˜¶æ®µæå–æ¡†æ¶
        
        Stage 1: project_overview (é¡¹ç›®æ¦‚è§ˆ - å«èŒƒå›´ã€è¿›åº¦ã€ä¿è¯é‡‘)
        Stage 2: bidder_qualification (æŠ•æ ‡äººèµ„æ ¼)
        Stage 3: evaluation_and_scoring (è¯„å®¡ä¸è¯„åˆ†)
        Stage 4: business_terms (å•†åŠ¡æ¡æ¬¾)
        Stage 5: technical_requirements (æŠ€æœ¯è¦æ±‚)
        Stage 6: document_preparation (æ–‡ä»¶ç¼–åˆ¶)
        
        æ¯ä¸ªstageé‡‡ç”¨ï¼š
        - P0é˜¶æ®µï¼šåŸºäºchecklistçš„ç»“æ„åŒ–æå–
        - P1é˜¶æ®µï¼šè¡¥å……æ‰«æé—æ¼ä¿¡æ¯
        - éªŒè¯ï¼šæ£€æŸ¥å¿…å¡«å­—æ®µå’Œè¯æ®
        
        Args:
            parallel: æ˜¯å¦å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰Stageï¼ˆæ”¯æŒæœ€å¤š6ä¸ªå¹¶è¡Œï¼‰
        """
        import os
        import asyncio
        from app.works.tender.project_info_extractor import ProjectInfoExtractor
        from app.works.tender.tender_context_retriever import TenderContextRetriever
        
        logger.info(
            f"ExtractV2: Starting CHECKLIST-BASED extraction (V3 - 6 stages) "
            f"for project={project_id} parallel={parallel}"
        )
        
        # å®šä¹‰å…­ä¸ªé˜¶æ®µå…ƒæ•°æ®ï¼ˆç”¨äºè¿›åº¦å±•ç¤ºï¼‰
        stages_meta = [
            {"stage": 1, "name": "é¡¹ç›®æ¦‚è§ˆ", "key": "project_overview"},
            {"stage": 2, "name": "æŠ•æ ‡äººèµ„æ ¼", "key": "bidder_qualification"},
            {"stage": 3, "name": "è¯„å®¡ä¸è¯„åˆ†", "key": "evaluation_and_scoring"},
            {"stage": 4, "name": "å•†åŠ¡æ¡æ¬¾", "key": "business_terms"},
            {"stage": 5, "name": "æŠ€æœ¯è¦æ±‚", "key": "technical_requirements"},
            {"stage": 6, "name": "æ–‡ä»¶ç¼–åˆ¶", "key": "document_preparation"},
        ]
        
        try:
            # ===== æ­¥éª¤1ï¼šç»Ÿä¸€æ£€ç´¢æ‹›æ ‡æ–‡æ¡£ä¸Šä¸‹æ–‡ =====
            logger.info("Step 1: æ£€ç´¢æ‹›æ ‡æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼ˆä¸€æ¬¡æ€§æ£€ç´¢ï¼‰")
            
            if run_id:
                self.dao.update_run(
                    run_id, "running", progress=0.05, 
                    message="æ­£åœ¨æ£€ç´¢æ‹›æ ‡æ–‡æ¡£..."
                )
            
            context_retriever = TenderContextRetriever(self.retriever)
            context_data = await context_retriever.retrieve_tender_context(
                project_id=project_id,
                top_k=150,  # è·å–è¶³å¤Ÿå¤šçš„ä¸Šä¸‹æ–‡
                max_context_chunks=100,
                sort_by_position=True,
                filter_contract_clauses=True,  # âœ¨ å¯ç”¨åˆåŒæ¡æ¬¾è¿‡æ»¤
            )
            
            if context_data.used_chunks == 0:
                raise ValueError(f"æœªæ£€ç´¢åˆ°æ‹›æ ‡æ–‡æ¡£å†…å®¹ï¼Œproject_id={project_id}")
            
            logger.info(
                f"ä¸Šä¸‹æ–‡æ£€ç´¢å®Œæˆ: total={context_data.total_chunks}, "
                f"used={context_data.used_chunks}, "
                f"context_length={len(context_data.context_text)}"
            )
            
            # ===== æ­¥éª¤2ï¼šåˆ›å»ºChecklistæå–å™¨ =====
            logger.info("Step 2: åˆå§‹åŒ–Checklistæå–å™¨")
            
            extractor = ProjectInfoExtractor(llm=self.llm)
            
            # ===== æ­¥éª¤3ï¼šæ‰§è¡Œ6ä¸ªstageï¼ˆæ”¯æŒå¹¶è¡Œæˆ–é¡ºåºï¼‰ =====
            logger.info(f"Step 3: å¼€å§‹6é˜¶æ®µchecklistæå– (parallel={parallel})")
            
            all_stage_results = {}
            all_evidence_ids = set()
            
            if parallel:
                # ===== å¹¶è¡Œæ¨¡å¼ï¼šæ‰€æœ‰stageåŒæ—¶æ‰§è¡Œ =====
                logger.info("ä½¿ç”¨å¹¶è¡Œæ¨¡å¼ï¼Œæ‰€æœ‰6ä¸ªstageåŒæ—¶æ‰§è¡Œ")
                
                # è·å–æœ€å¤§å¹¶å‘æ•°
                max_concurrent = int(os.getenv("EXTRACT_MAX_CONCURRENT", "6"))
                logger.info(f"æœ€å¤§å¹¶å‘æ•°: {max_concurrent}")
                
                # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘
                semaphore = asyncio.Semaphore(max_concurrent)
                
                async def extract_stage_with_semaphore(stage_meta):
                    async with semaphore:
                        stage_num = stage_meta["stage"]
                        stage_name = stage_meta["name"]
                        stage_key = stage_meta["key"]
                        
                        logger.info(f"=== å¼€å§‹å¹¶è¡Œæå– Stage {stage_num}/6: {stage_name} ===")
                        
                        try:
                            stage_result = await extractor.extract_stage(
                                stage=stage_num,
                                context_text=context_data.context_text,
                                segment_id_map=context_data.segment_id_map,
                                model_id=model_id,
                                context_info=None,  # å¹¶è¡Œæ¨¡å¼ä¸‹ä¸ä¼ é€’context
                                enable_p1=True
                            )
                            
                            logger.info(
                                f"Stage {stage_num} å®Œæˆ: "
                                f"fields={len(stage_result['data'])}, "
                                f"evidence={len(stage_result['evidence_segment_ids'])}"
                            )
                            
                            return stage_key, stage_result
                            
                        except Exception as e:
                            logger.error(f"Stage {stage_num} å¤±è´¥: {e}", exc_info=True)
                            return stage_key, {
                                "data": {},
                                "evidence_segment_ids": [],
                                "p1_supplements_count": 0
                            }
                
                # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰stage
                if run_id:
                    self.dao.update_run(
                        run_id, "running", progress=0.10,
                        message="æ­£åœ¨å¹¶è¡Œæå–æ‰€æœ‰é˜¶æ®µ..."
                    )
                
                tasks = [extract_stage_with_semaphore(meta) for meta in stages_meta]
                results = await asyncio.gather(*tasks)
                
                # æ”¶é›†ç»“æœ
                for stage_key, stage_result in results:
                    all_stage_results[stage_key] = stage_result["data"]
                    all_evidence_ids.update(stage_result["evidence_segment_ids"])
                
                # æ›´æ–°è¿›åº¦
                if run_id:
                    self.dao.update_run(
                        run_id, "running", progress=0.90,
                        message="æ‰€æœ‰é˜¶æ®µæå–å®Œæˆï¼Œæ­£åœ¨ä¿å­˜..."
                    )
                
            else:
                # ===== é¡ºåºæ¨¡å¼ï¼šä¸€ä¸ªæ¥ä¸€ä¸ªæ‰§è¡Œï¼ˆæ”¯æŒcontextä¼ é€’ï¼‰ =====
                logger.info("ä½¿ç”¨é¡ºåºæ¨¡å¼ï¼Œstageä¾æ¬¡æ‰§è¡Œ")
                context_info = None  # ç”¨äºä¼ é€’å‰åºstageçš„ç»“æœ
                
                for stage_meta in stages_meta:
                    stage_num = stage_meta["stage"]
                    stage_name = stage_meta["name"]
                    stage_key = stage_meta["key"]
                    
                    logger.info(f"=== Extracting Stage {stage_num}/6: {stage_name} ===")
                    
                    # æ›´æ–°è¿›åº¦
                    if run_id:
                        progress = 0.05 + (stage_num - 1) * 0.15  # 0.05, 0.20, 0.35, 0.50, 0.65, 0.80
                        self.dao.update_run(
                            run_id, "running", progress=progress,
                            message=f"æ­£åœ¨æŠ½å–ï¼š{stage_name} (P0+P1)..."
                        )
                    
                    try:
                        # è°ƒç”¨extractoræå–å•ä¸ªstage
                        stage_result = await extractor.extract_stage(
                            stage=stage_num,
                            context_text=context_data.context_text,
                            segment_id_map=context_data.segment_id_map,
                            model_id=model_id,
                            context_info=context_info,  # ä¼ é€’å‰åºstageçš„ç»“æœ
                            enable_p1=True  # å¯ç”¨P1è¡¥å……æ‰«æ
                        )
                        
                        # ä¿å­˜ç»“æœ
                        all_stage_results[stage_key] = stage_result["data"]
                        all_evidence_ids.update(stage_result["evidence_segment_ids"])
                        
                        # ä¼ é€’contextç»™ä¸‹ä¸€ä¸ªstage
                        if context_info is None:
                            context_info = {}
                        context_info[stage_key] = stage_result["data"]
                        
                        logger.info(
                            f"Stage {stage_num} complete: "
                            f"fields={len(stage_result['data'])}, "
                            f"evidence={len(stage_result['evidence_segment_ids'])}, "
                            f"p1_supplements={stage_result['p1_supplements_count']}"
                        )
                        
                        # âœ… å¢é‡ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆé¡ºåºæ¨¡å¼ï¼‰
                        incremental_data = {
                            "schema_version": "tender_info_v3",
                            **{k: all_stage_results.get(k, {}) for k in [s["key"] for s in stages_meta]}
                        }
                        self.dao.upsert_project_info(
                            project_id,
                            data_json=incremental_data,
                            evidence_chunk_ids=list(all_evidence_ids)
                        )
                        
                        # æ›´æ–°å®Œæˆè¿›åº¦
                        if run_id:
                            progress = 0.05 + stage_num * 0.15  # 0.20, 0.35, 0.50, 0.65, 0.80, 0.95
                            self.dao.update_run(
                                run_id, "running", progress=progress,
                                message=f"{stage_name}å·²å®Œæˆ"
                            )
                        
                    except Exception as e:
                        logger.error(f"Stage {stage_num} failed: {e}", exc_info=True)
                        # å¤±è´¥æ—¶è®¾ç½®ç©ºæ•°æ®ï¼Œç»§ç»­æ‰§è¡Œåç»­stage
                        all_stage_results[stage_key] = {}
                        
                        # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­æµç¨‹
                        if run_id:
                            self.dao.update_run(
                                run_id, "running",
                                message=f"{stage_name}æå–å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€é˜¶æ®µ"
                            )
            
            # ===== æ­¥éª¤4ï¼šæœ€ç»ˆä¿å­˜ï¼ˆå¹¶è¡Œæ¨¡å¼éœ€è¦åœ¨è¿™é‡Œä¿å­˜ï¼‰ =====
            if parallel:
                incremental_data = {
                    "schema_version": "tender_info_v3",
                    **{k: all_stage_results.get(k, {}) for k in [s["key"] for s in stages_meta]}
                }
                self.dao.upsert_project_info(
                    project_id,
                    data_json=incremental_data,
                    evidence_chunk_ids=list(all_evidence_ids)
                )
            
            # ===== æ­¥éª¤5ï¼šéªŒè¯æå–ç»“æœ =====
            logger.info("Step 5: éªŒè¯æå–ç»“æœ")
            
            final_result = {
                "schema_version": "tender_info_v3",
                **all_stage_results,
                "evidence_chunk_ids": list(all_evidence_ids)
            }
            
            validation_report = extractor.validate_result(final_result)
            
            if not validation_report["is_valid"]:
                logger.warning(
                    f"Validation failed: errors={validation_report['errors']}"
                )
            
            if validation_report["warnings"]:
                logger.warning(
                    f"Validation warnings: {validation_report['warnings']}"
                )
            
            # ===== æ­¥éª¤6ï¼šæ„å»ºæœ€ç»ˆè¿”å›ç»“æœ =====
            logger.info(
                f"ExtractV2: CHECKLIST-BASED extraction complete - "
                f"mode={'parallel' if parallel else 'sequential'}, "
                f"stages_completed={len([r for r in all_stage_results.values() if r])}/6, "
                f"evidence_segments={len(all_evidence_ids)}"
            )
            
            # æ„å»ºevidence_spansï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
            evidence_spans = []
            for seg_id in list(all_evidence_ids)[:50]:  # é™åˆ¶æ•°é‡
                chunk = context_data.segment_id_map.get(seg_id)
                if chunk:
                    meta = chunk.meta or {}
                    evidence_spans.append({
                        "source": meta.get("doc_version_id", ""),
                        "page_no": meta.get("page_no", 0),
                        "snippet": chunk.text[:200]
                    })
            
            # ===== æ­¥éª¤6ï¼šæ„å»ºå®Œæ•´çš„ç»“æœï¼ˆç¡®ä¿åŒ…å«æ‰€æœ‰6ä¸ªstageï¼‰ =====
            final_result = {
                "schema_version": "tender_info_v3",
                # æ˜ç¡®åˆ—å‡ºæ‰€æœ‰6ä¸ªstageï¼Œå³ä½¿æŸäº›ä¸ºç©º
                "project_overview": all_stage_results.get("project_overview", {}),
                "bidder_qualification": all_stage_results.get("bidder_qualification", {}),
                "evaluation_and_scoring": all_stage_results.get("evaluation_and_scoring", {}),
                "business_terms": all_stage_results.get("business_terms", {}),
                "technical_requirements": all_stage_results.get("technical_requirements", {}),
                "document_preparation": all_stage_results.get("document_preparation", {}),
                "evidence_chunk_ids": list(all_evidence_ids),
                "evidence_spans": evidence_spans,
                "retrieval_trace": {
                    "mode": "checklist_based_v3",
                    "method": "P0+P1",
                    "stages": len(all_stage_results),
                    "validation": validation_report
                }
            }
            
            # ===== æ­¥éª¤7ï¼šæœ€ç»ˆç¡®è®¤ä¿å­˜ï¼ˆç¡®ä¿æ•°æ®å®Œæ•´ï¼‰ =====
            # æœ€åå†ä¿å­˜ä¸€æ¬¡ï¼Œç¡®ä¿æ‰€æœ‰stageçš„æ•°æ®éƒ½å·²ä¿å­˜
            logger.info("æœ€ç»ˆä¿å­˜é¡¹ç›®ä¿¡æ¯åˆ°æ•°æ®åº“...")
            data_to_save_final = {
                "schema_version": "tender_info_v3",
                "project_overview": all_stage_results.get("project_overview", {}),
                "bidder_qualification": all_stage_results.get("bidder_qualification", {}),
                "evaluation_and_scoring": all_stage_results.get("evaluation_and_scoring", {}),
                "business_terms": all_stage_results.get("business_terms", {}),
                "technical_requirements": all_stage_results.get("technical_requirements", {}),
                "document_preparation": all_stage_results.get("document_preparation", {}),
            }
            self.dao.upsert_project_info(
                project_id,
                data_json=data_to_save_final,
                evidence_chunk_ids=list(all_evidence_ids)
            )
            logger.info("é¡¹ç›®ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")
            
            # ===== æ­¥éª¤8ï¼šæ›´æ–°runè¿›åº¦ä¸ºæ¥è¿‘å®Œæˆ =====
            if run_id:
                logger.info(f"æ›´æ–°runè¿›åº¦: run_id={run_id}")
                self.dao.update_run(
                    run_id, 
                    "running",  # ä¿æŒrunningçŠ¶æ€ï¼Œç”±TenderServiceæœ€ç»ˆæ›´æ–°ä¸ºsuccess
                    progress=0.98,
                    message="é¡¹ç›®ä¿¡æ¯æå–å®Œæˆï¼Œæ­£åœ¨ä¿å­˜..."
                )
            
            return final_result
            
        except Exception as e:
            logger.error(f"Checklist-based extraction failed: {e}", exc_info=True)
            
            if run_id:
                self.dao.update_run(run_id, "failed", message=f"æå–å¤±è´¥: {str(e)}")
            
            raise
    
    # extract_risks_v2 å·²åˆ é™¤ï¼Œè¯·ä½¿ç”¨ extract_requirements_v1
    # risksæ¨¡å—å·²åºŸå¼ƒï¼Œç»Ÿä¸€ä½¿ç”¨requirementsæ¨¡å—æå–æ‹›æ ‡è¦æ±‚
    
    async def extract_requirements_v1(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        âŒ å·²åºŸå¼ƒï¼šV1æ‹›æ ‡è¦æ±‚æå–å·²åºŸå¼ƒ
        
        è¯·ä½¿ç”¨ extract_requirements_v2ï¼ˆæ ‡å‡†æ¸…å•æ–¹å¼ï¼‰
        
        åºŸå¼ƒæ—¶é—´ï¼š2025-12-29
        åºŸå¼ƒåŸå› ï¼šV2æ ‡å‡†æ¸…å•æ–¹å¼æä¾›æ›´é«˜è´¨é‡çš„æ•°æ®ï¼ˆ100% norm_keyè¦†ç›–ï¼‰
        """
        raise NotImplementedError(
            "âŒ V1æ‹›æ ‡è¦æ±‚æå–å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ extract_requirements_v2ï¼ˆæ ‡å‡†æ¸…å•æ–¹å¼ï¼‰"
        )
    
    def _infer_routing_fields(self, req: Dict[str, Any]) -> tuple:
        """
        æ¨æ–­è·¯ç”±å­—æ®µï¼ˆStep 2ï¼‰
        
        æ ¹æ® requirement çš„ç‰¹å¾æ¨æ–­ eval_methodã€must_reject ç­‰å­—æ®µ
        
        Returns:
            (eval_method, must_reject, expected_evidence_json, rubric_json, weight)
        """
        dimension = req.get("dimension", "")
        req_type = req.get("req_type", "")
        requirement_text = req.get("requirement_text", "")
        is_hard = req.get("is_hard", False)
        value_schema = req.get("value_schema_json", {})
        
        # é»˜è®¤å€¼
        eval_method = "PRESENCE"  # é»˜è®¤ä¸ºå­˜åœ¨æ€§æ£€æŸ¥
        must_reject = False
        expected_evidence = None
        rubric = None
        weight = 1.0
        
        # 1. æ ¹æ® dimension å’Œ req_type æ¨æ–­ eval_method
        if dimension == "qualification":
            # èµ„æ ¼ç±»å¤šä¸ºå­˜åœ¨æ€§/æœ‰æ•ˆæ€§æ£€æŸ¥
            if "è¥ä¸šæ‰§ç…§" in requirement_text or "èµ„è´¨è¯ä¹¦" in requirement_text or "è®¸å¯è¯" in requirement_text:
                eval_method = "VALIDITY"
                expected_evidence = {"doc_types": ["license", "certificate"], "fields": ["expire_date", "scope"]}
            elif "ä¸šç»©" in requirement_text or "é¡¹ç›®ç»éªŒ" in requirement_text:
                eval_method = "VALIDITY"
                expected_evidence = {"doc_types": ["performance"], "fields": ["project_name", "contract_amount", "completion_date"]}
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "price":
            # ä»·æ ¼ç±»å¤šä¸ºæ•°å€¼æ¯”è¾ƒ
            eval_method = "NUMERIC"
            if value_schema and isinstance(value_schema, dict):
                expected_evidence = {
                    "type": "numeric",
                    "constraints": value_schema
                }
        
        elif dimension == "technical":
            # æŠ€æœ¯ç±»ï¼šå‚æ•°è¡¨ä¸º TABLE_COMPAREï¼Œè¯„åˆ†ç‚¹ä¸º SEMANTIC
            if req_type == "scoring":
                eval_method = "SEMANTIC"
                # æå–è¯„åˆ†ç»†åˆ™ä½œä¸º rubric
                score = self._extract_score(requirement_text)
                rubric = {
                    "criteria": requirement_text,
                    "scoring_method": "LLM",
                    "max_points": score
                }
            elif "å‚æ•°è¡¨" in requirement_text or "è§„æ ¼è¡¨" in requirement_text:
                eval_method = "TABLE_COMPARE"
            elif ("å‚æ•°" in requirement_text or "è§„æ ¼" in requirement_text or "æŒ‡æ ‡" in requirement_text or
                  "ä¸ä½äº" in requirement_text or "ä¸è¶…è¿‡" in requirement_text or "â‰¥" in requirement_text or "â‰¤" in requirement_text):
                # åŒ…å«æ•°å€¼æ¯”è¾ƒå…³é”®è¯ï¼Œä½¿ç”¨ NUMERIC
                eval_method = "NUMERIC"
            elif "åç¦»" in requirement_text and "ä¸å…è®¸" in requirement_text:
                eval_method = "EXACT_MATCH"
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "business":
            # å•†åŠ¡ç±»ï¼šå·¥æœŸ/è´¨ä¿/ä»˜æ¬¾ç­‰ä¸ºæ•°å€¼æ£€æŸ¥ï¼Œè¯„åˆ†ç‚¹ä¸ºè¯­ä¹‰
            if req_type == "scoring":
                eval_method = "SEMANTIC"
                score = self._extract_score(requirement_text)
                rubric = {
                    "criteria": requirement_text,
                    "scoring_method": "LLM",
                    "max_points": score
                }
            elif "å·¥æœŸ" in requirement_text or "è´¨ä¿" in requirement_text or "ä»˜æ¬¾" in requirement_text:
                eval_method = "NUMERIC"
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "doc_structure":
            # æ–‡æ¡£ç»“æ„ç±»å¤šä¸ºå­˜åœ¨æ€§æ£€æŸ¥
            eval_method = "PRESENCE"
        
        # 2. æ ¹æ® is_hard å’Œ req_type æ¨æ–­ must_reject
        if is_hard and req_type in ("threshold", "must_provide", "must_not_deviate"):
            must_reject = True
        
        # 3. æƒé‡ï¼šè¯„åˆ†é¡¹æƒé‡è¾ƒé«˜
        if req_type == "scoring":
            score = self._extract_score(requirement_text)
            weight = score if score else 5.0
        elif must_reject:
            weight = 10.0  # å¿…é¡»é¡¹æƒé‡æœ€é«˜
        
        return eval_method, must_reject, expected_evidence, rubric, weight
    
    def _extract_score(self, text: str) -> Optional[float]:
        """ä»æ–‡æœ¬ä¸­æå–åˆ†å€¼"""
        import re
        # åŒ¹é…"XXåˆ†"ã€"X-XXåˆ†"ã€"æœ€å¤šXXåˆ†"ã€"ä¸è¶…è¿‡XXåˆ†"ç­‰æ¨¡å¼
        patterns = [
            r'æœ€å¤š\s*(\d+(?:\.\d+)?)\s*åˆ†',
            r'ä¸è¶…è¿‡\s*(\d+(?:\.\d+)?)\s*åˆ†',
            r'(?:å¾—|ä¸º)\s*(\d+(?:\.\d+)?)\s*åˆ†',
            r'(\d+(?:\.\d+)?)\s*åˆ†',
            r'(\d+(?:\.\d+)?)\s*points?',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return float(match.group(1))
                except:
                    pass
        return None
    
    async def generate_directory_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
        use_fast_mode: bool = True,
        enable_refinement: bool = False,  # âŒ ç¦ç”¨è§„åˆ™ç»†åŒ–ï¼ˆä¼šè‡ªè¡Œåˆ›é€ åˆ†å†Œï¼‰
        enable_bracket_parsing: bool = False,  # âŒ ç¦ç”¨æ‹¬å·è§£æï¼ˆé¿å…é¢å¤–å±‚çº§ï¼‰
        enable_template_matching: bool = True,  # âœ¨ é˜¶æ®µ5ï¼šæ ¼å¼èŒƒæœ¬è‡ªåŠ¨å¡«å……
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆç›®å½• (v2) - å¤šé˜¶æ®µç”Ÿæˆ
        
        é˜¶æ®µ1ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰ï¼šå¦‚æœæœ‰é¡¹ç›®ä¿¡æ¯ï¼Œç›´æ¥æ„å»ºéª¨æ¶
        é˜¶æ®µ2ï¼ˆLLMè¡¥å……ï¼‰ï¼šæ£€ç´¢è¡¥å…¨ç»†èŠ‚æˆ–å…¨æ–°ç”Ÿæˆ
        é˜¶æ®µ3ï¼ˆç›®å½•å¢å¼ºï¼‰ï¼šè¡¥å……é—æ¼çš„å¿…å¡«èŠ‚ç‚¹
        é˜¶æ®µ4-Aï¼ˆè§„åˆ™ç»†åŒ–ï¼‰ï¼šåŸºäºæ‹›æ ‡è¦æ±‚ç»†åŒ–è¯„åˆ†æ ‡å‡†ã€èµ„æ ¼å®¡æŸ¥ç­‰èŠ‚ç‚¹
        é˜¶æ®µ4-Bï¼ˆLLMæ‹¬å·è§£æï¼‰ï¼šè§£ææ‹¬å·è¯´æ˜ï¼Œç”ŸæˆL4ç»†åˆ†èŠ‚ç‚¹
        é˜¶æ®µ5ï¼ˆæ ¼å¼èŒƒæœ¬å¡«å……ï¼‰ï¼šè‡ªåŠ¨è¯†åˆ«å¹¶å¡«å……æ ¼å¼èŒƒæœ¬åˆ°èŠ‚ç‚¹æ­£æ–‡ âœ¨ æ–°å¢
        
        Args:
            use_fast_mode: æ˜¯å¦å¯ç”¨å¿«é€Ÿæ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            enable_refinement: æ˜¯å¦å¯ç”¨è§„åˆ™ç»†åŒ–ï¼ˆé»˜è®¤Trueï¼Œè®¾ä¸ºFalseå¯å›é€€ï¼‰
            enable_bracket_parsing: æ˜¯å¦å¯ç”¨LLMæ‹¬å·è§£æï¼ˆé»˜è®¤Trueï¼Œè®¾ä¸ºFalseå¯å›é€€ï¼‰
            enable_template_matching: æ˜¯å¦å¯ç”¨æ ¼å¼èŒƒæœ¬åŒ¹é…ï¼ˆé»˜è®¤Trueï¼Œè®¾ä¸ºFalseå¯å›é€€ï¼‰
        
        Returns:
            {
                "data": {
                    "nodes": [...]
                },
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "generation_mode": "fast" | "llm" | "hybrid",
                "refinement_stats": {...},
                "bracket_parsing_stats": {...},
                "template_matching_stats": {...}  # âœ¨ æ–°å¢ï¼šèŒƒæœ¬å¡«å……ç»Ÿè®¡
            }
        """
        logger.info(f"ExtractV2: generate_directory start project_id={project_id}, fast_mode={use_fast_mode}")
        
        # é˜¶æ®µ1ï¼šå°è¯•å¿«é€Ÿæ¨¡å¼
        # âŒ å·²ç¦ç”¨ï¼šä¸å†ä½¿ç”¨å›ºå®šçš„å•†åŠ¡/æŠ€æœ¯/ä»·æ ¼åˆ’åˆ†ï¼Œå®Œå…¨ä¾èµ–ä»æ‹›æ ‡ä¹¦æå–çš„å®é™…ç›®å½•ç»“æ„
        # ç°åœ¨ç›®å½•ç”Ÿæˆå®Œå…¨åŸºäºæ‹›æ ‡ä¹¦ä¸­çš„"æŠ•æ ‡æ–‡ä»¶æ ¼å¼"ç« èŠ‚
        fast_nodes = []
        fast_stats = {}
        generation_mode = "llm"  # é»˜è®¤å…¨LLM
        
        # if use_fast_mode:
        #     tender_info = self.dao.get_project_info(project_id)
        #     if tender_info and tender_info.get("schema_version") == "tender_info_v3":
        #         try:
        #             from app.works.tender.directory_fast_builder import build_directory_from_project_info
        #             
        #             fast_nodes, fast_stats = build_directory_from_project_info(
        #                 project_id=project_id,
        #                 pool=self.pool,
        #                 tender_info=tender_info
        #             )
        #             
        #             if fast_nodes and len(fast_nodes) >= 5:  # è‡³å°‘5ä¸ªèŠ‚ç‚¹æ‰è®¤ä¸ºæœ‰æ•ˆ
        #                 logger.info(
        #                     f"ExtractV2: Fast mode success - {len(fast_nodes)} nodes, "
        #                     f"skip LLM generation"
        #                 )
        #                 generation_mode = "fast"
        #                 
        #                 # å¿«é€Ÿæ¨¡å¼æˆåŠŸï¼Œç›´æ¥è¿”å›
        #                 return {
        #                     "data": {"nodes": fast_nodes},
        #                     "evidence_chunk_ids": [],
        #                     "evidence_spans": [],
        #                     "retrieval_trace": {},
        #                     "generation_mode": generation_mode,
        #                     "fast_stats": fast_stats
        #                 }
        #             else:
        #                 logger.info(f"ExtractV2: Fast mode insufficient ({len(fast_nodes)} nodes), fallback to LLM")
        #                 generation_mode = "hybrid"
        #                 
        #         except Exception as e:
        #             logger.warning(f"ExtractV2: Fast mode failed (non-fatal): {e}")
        #             generation_mode = "llm"
        #     else:
        #         logger.info("ExtractV2: No project_info available, using LLM mode")
        
        # âœ… æ–°ç­–ç•¥ï¼ˆ2026-01ï¼‰ï¼šä¼˜å…ˆä»æ‹›æ ‡ä¹¦åŸæ–‡æå–ç›®å½•ï¼Œç¦æ­¢LLMè‡ªè¡Œåˆ’åˆ†å¤§ç±»
        # 
        # æµç¨‹ï¼š
        # 1. å…ˆç”¨ augment ä»æ‹›æ ‡ä¹¦"æŠ•æ ‡æ–‡ä»¶æ ¼å¼"ç« èŠ‚æå–åŸæ–‡ç›®å½•ï¼ˆå†™å…¥æ•°æ®åº“ï¼‰
        # 2. å¦‚æœæå–æˆåŠŸï¼ˆ>= 5ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œç›´æ¥ä½¿ç”¨ï¼Œä¸è°ƒç”¨LLM
        # 3. å¦‚æœæå–å¤±è´¥æˆ–èŠ‚ç‚¹å¤ªå°‘ï¼Œæ‰å›é€€åˆ°LLMç”Ÿæˆ
        
        logger.info(f"ExtractV2: å¼€å§‹ç”Ÿæˆç›®å½•...")
        
        # ğŸ” DEBUG: å¼ºåˆ¶å†™å…¥æ—¥å¿—æ–‡ä»¶
        import sys
        debug_log = open("/tmp/extract_v2_debug.log", "a")
        debug_log.write(f"\n=== ExtractV2.generate_directory_v2 START ===\n")
        debug_log.write(f"project_id: {project_id}\n")
        debug_log.write(f"use_fast_mode: {use_fast_mode}\n")
        debug_log.flush()
        
        print(f"[ExtractV2-DEBUG] å¼€å§‹ç”Ÿæˆç›®å½•: project_id={project_id}", file=sys.stderr)
        
        # æ­¥éª¤0ï¼šæ¸…ç©ºç°æœ‰ç›®å½•èŠ‚ç‚¹ï¼ˆé¿å…ä½¿ç”¨æ—§æ•°æ®ï¼‰
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("""
                        DELETE FROM tender_directory_nodes WHERE project_id = %s
                    """, [project_id])
                    deleted_count = cur.rowcount
                    conn.commit()
                    if deleted_count > 0:
                        logger.info(f"ExtractV2: æ¸…ç©ºäº† {deleted_count} ä¸ªæ—§ç›®å½•èŠ‚ç‚¹")
                        debug_log.write(f"æ¸…ç©ºäº† {deleted_count} ä¸ªæ—§èŠ‚ç‚¹\n")
                        debug_log.flush()
        except Exception as e:
            logger.warning(f"ExtractV2: æ¸…ç©ºæ—§èŠ‚ç‚¹å¤±è´¥ï¼ˆéè‡´å‘½ï¼‰: {e}")
            debug_log.write(f"æ¸…ç©ºå¤±è´¥: {e}\n")
            debug_log.flush()
        
        # æ­¥éª¤1ï¼šç›´æ¥è·³è¿‡augmentï¼Œå¼ºåˆ¶ä½¿ç”¨LLMç”Ÿæˆ
        # åŸå› ï¼šaugmentå¯¹äº"æŠ•æ ‡æ–‡ä»¶ç»„æˆ"ç­‰æ‰å¹³åˆ—è¡¨çš„å¤„ç†ä¸ç¨³å®šï¼Œå®¹æ˜“äº§ç”Ÿé”™è¯¯çš„çˆ¶å­å…³ç³»
        # LLMç”Ÿæˆçš„ç›®å½•ç»“æ„æ›´å‡†ç¡®ã€å¯é 
        extracted_count = 0
        logger.info(f"ExtractV2: è·³è¿‡augmentï¼Œç›´æ¥ä½¿ç”¨LLMç”Ÿæˆç›®å½•ï¼ˆæ›´å‡†ç¡®ã€å¯é ï¼‰")
        debug_log.write(f"è·³è¿‡augmentï¼Œç›´æ¥ä½¿ç”¨LLMç”Ÿæˆ\n")
        debug_log.flush()
        
        # === LLM ç”Ÿæˆæ¨¡å¼ ===
        generation_mode = "llm"
        
        # 1. è·å– embedding provider
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        # 2. æ„å»º spec
        spec = await build_directory_spec_async(self.pool)
        
        # 3. è°ƒç”¨å¼•æ“
        result = await self.engine.run(
            spec=spec,
            retriever=self.retriever,
            llm=self.llm,
            project_id=project_id,
            model_id=model_id,
            run_id=run_id,
            embedding_provider=embedding_provider,
        )
        
        # 4. éªŒè¯ç»“æœ
        if not result.data or not isinstance(result.data, dict):
            logger.error(f"ExtractV2: directory data invalid, type={type(result.data)}")
            raise ValueError("Directory extraction returned invalid data")
        
        nodes = result.data.get("nodes", [])
        if not nodes:
            logger.warning(f"ExtractV2: no directory nodes extracted for project={project_id}")
        
        # 5. å¦‚æœæ˜¯æ··åˆæ¨¡å¼ï¼Œåˆå¹¶å¿«é€ŸèŠ‚ç‚¹å’ŒLLMèŠ‚ç‚¹
        # âŒ å·²ç¦ç”¨ï¼šä¸å†ä½¿ç”¨å›ºå®šåˆ’åˆ†
        # if generation_mode == "hybrid" and fast_nodes:
        #     logger.info(f"ExtractV2: Merging fast nodes ({len(fast_nodes)}) with LLM nodes ({len(nodes)})")
        #     # ç®€å•ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨å¿«é€ŸèŠ‚ç‚¹ï¼ŒLLMèŠ‚ç‚¹ä½œä¸ºè¡¥å……
        #     nodes = fast_nodes + nodes
        
        logger.info(f"ExtractV2: generate_directory done nodes={len(nodes)}, mode={generation_mode}")
        
        # 5. ç›®å½•å¢å¼º - å·²åœ¨é˜¶æ®µ1å®Œæˆï¼Œä¸å†é‡å¤æ‰§è¡Œ
        # âŒ å·²ç¦ç”¨ï¼šaugment å·²ç»åœ¨é˜¶æ®µ1æ‰§è¡Œè¿‡äº†
        # try:
        #     logger.info(f"ExtractV2: Attempting directory augmentation for project={project_id}")
        #     
        #     # è¯»å– tender_project_info
        #     tender_info = self.dao.get_project_info(project_id)
        #     if tender_info and tender_info.get("schema_version") == "tender_info_v3":
        #         from app.works.tender.directory_augment_v1 import augment_directory_from_tender_info_v3
        #         
        #         augment_result = augment_directory_from_tender_info_v3(
        #             project_id=project_id,
        #             pool=self.pool,
        #             tender_info=tender_info
        #         )
        #         
        #         logger.info(
        #             f"ExtractV2: Directory augmentation done - "
        #             f"added={augment_result['added_count']}, "
        #             f"titles={augment_result['enhanced_titles'][:5]}"
        #         )
        # except Exception as e:
        #     logger.warning(f"ExtractV2: Directory augmentation failed (non-fatal): {e}")
        
        # âœ¨ 6. è§„åˆ™ç»†åŒ– - åŸºäºæ‹›æ ‡è¦æ±‚ç»†åŒ–è¯„åˆ†æ ‡å‡†ã€èµ„æ ¼å®¡æŸ¥ç­‰èŠ‚ç‚¹ï¼ˆæ–°å¢é˜¶æ®µ4ï¼‰
        refinement_stats = {}
        try:
            if enable_refinement:
                logger.info(f"ExtractV2: Starting rule-based refinement for project={project_id}")
                
                from app.works.tender.directory_refinement_rule import refine_directory_from_requirements
                
                refinement_result = refine_directory_from_requirements(
                    project_id=project_id,
                    pool=self.pool,
                    nodes=nodes,
                    enable_refinement=True,
                )
                
                # æ›´æ–°èŠ‚ç‚¹åˆ—è¡¨
                nodes = refinement_result["refined_nodes"]
                refinement_stats = refinement_result["stats"]
                
                logger.info(
                    f"ExtractV2: Rule-based refinement done - "
                    f"added={refinement_result['added_count']} nodes, "
                    f"total={len(nodes)}, "
                    f"refined_parents={refinement_result['refined_parents']}"
                )
            else:
                logger.info(f"ExtractV2: Rule-based refinement disabled")
                refinement_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: Rule-based refinement failed (non-fatal): {e}")
            refinement_stats = {"error": str(e)}
        
        # âœ¨ 7. LLMæ‹¬å·è§£æ - è§£æL3èŠ‚ç‚¹çš„æ‹¬å·è¯´æ˜ï¼Œç”ŸæˆL4å­èŠ‚ç‚¹ï¼ˆæ–°å¢é˜¶æ®µ4-Bï¼‰
        bracket_parsing_stats = {}
        try:
            if enable_bracket_parsing:
                logger.info(f"ExtractV2: Starting LLM-based bracket parsing for project={project_id}")
                
                from app.works.tender.directory_bracket_parser import parse_brackets_with_llm
                
                bracket_result = await parse_brackets_with_llm(
                    nodes=nodes,
                    llm=self.llm,
                    model_id=model_id,
                    enable_parsing=True,
                )
                
                # æ›´æ–°èŠ‚ç‚¹åˆ—è¡¨
                nodes = bracket_result["enhanced_nodes"]
                bracket_parsing_stats = bracket_result["stats"]
                
                logger.info(
                    f"ExtractV2: LLM bracket parsing done - "
                    f"added={bracket_result['added_count']} L4 nodes, "
                    f"total={len(nodes)}, "
                    f"parsed_parents={bracket_result['parsed_parents']}"
                )
            else:
                logger.info(f"ExtractV2: LLM bracket parsing disabled")
                bracket_parsing_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: LLM bracket parsing failed (non-fatal): {e}")
            bracket_parsing_stats = {"error": str(e)}
        
        # âœ¨ 8. æ ¼å¼èŒƒæœ¬åŒ¹é…ä¸å¡«å…… - è‡ªåŠ¨è¯†åˆ«å¹¶å¡«å……æ ¼å¼èŒƒæœ¬åˆ°èŠ‚ç‚¹æ­£æ–‡ï¼ˆæ–°å¢é˜¶æ®µ5ï¼‰
        template_matching_stats = {}
        try:
            if enable_template_matching:
                logger.info(f"ExtractV2: Starting template matching and auto-fill for project={project_id}")
                
                from app.works.tender.template_matcher import match_templates_to_nodes, auto_fill_template_bodies
                
                # 8.1 åŒ¹é…èŒƒæœ¬åˆ°èŠ‚ç‚¹
                match_result = await match_templates_to_nodes(
                    nodes=nodes,
                    project_id=project_id,
                    pool=self.pool,
                    llm=self.llm,
                    model_id=model_id,
                    enable_matching=True,
                )
                
                matches = match_result.get("matches", [])
                match_stats = match_result.get("stats", {})
                
                # 8.2 è‡ªåŠ¨å¡«å……åŒ¹é…çš„èŒƒæœ¬
                fill_result = {}
                if matches:
                    fill_result = await auto_fill_template_bodies(
                        matches=matches,
                        project_id=project_id,
                        pool=self.pool,
                    )
                    
                    logger.info(
                        f"ExtractV2: Template auto-fill done - "
                        f"{fill_result.get('filled_count', 0)}/{len(matches)} nodes filled"
                    )
                
                template_matching_stats = {
                    "enabled": True,
                    **match_stats,
                    **fill_result,
                }
            else:
                logger.info(f"ExtractV2: Template matching disabled")
                template_matching_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: Template matching failed (non-fatal): {e}")
            template_matching_stats = {"error": str(e)}
        
        # 9. ä¿å­˜èŠ‚ç‚¹åˆ°æ•°æ®åº“ï¼ˆå¦‚æœæ˜¯LLMç”Ÿæˆæ¨¡å¼ï¼‰
        if generation_mode == "llm" and nodes:
            try:
                logger.info(f"ExtractV2: Saving {len(nodes)} LLM-generated nodes to database...")
                self._save_nodes_to_db(project_id, nodes)
                logger.info(f"ExtractV2: Successfully saved {len(nodes)} nodes")
            except Exception as e:
                logger.error(f"ExtractV2: Failed to save nodes to database: {e}")
                # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œç»§ç»­è¿”å›ç»“æœ
        
        # 10. è¿”å›ç»“æœ
        return {
            "data": {"nodes": nodes},
            "evidence_chunk_ids": result.evidence_chunk_ids,
            "evidence_spans": result.evidence_spans,
            "retrieval_trace": result.retrieval_trace.__dict__ if result.retrieval_trace else {},
            "generation_mode": generation_mode,
            "fast_stats": fast_stats if generation_mode in ["fast", "hybrid"] else {},
            "refinement_stats": refinement_stats,
            "bracket_parsing_stats": bracket_parsing_stats,
            "template_matching_stats": template_matching_stats,  # âœ¨ æ–°å¢ï¼šèŒƒæœ¬å¡«å……ç»Ÿè®¡
        }
    
    def _generate_evidence_spans(
        self,
        chunks: List,
        evidence_chunk_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        ç”Ÿæˆ evidence_spansï¼ˆåŸºäº meta.page_noï¼‰
        æ­¤æ–¹æ³•ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œä½†æ–°ä»£ç åº”ä½¿ç”¨ ExtractionEngine å†…ç½®æ–¹æ³•
        
        Returns:
            [
                {
                    "source": "asset_id or doc_version_id",
                    "page_no": 5,
                    "snippet": "è¯æ®ç‰‡æ®µ..."
                }
            ]
        """
        spans = []
        chunk_map = {c.chunk_id: c for c in chunks}
        
        for chunk_id in evidence_chunk_ids:
            chunk = chunk_map.get(chunk_id)
            if not chunk:
                continue
            
            meta = chunk.meta or {}
            page_no = meta.get("page_no") or meta.get("chunk_position", 0)
            doc_version_id = meta.get("doc_version_id", "")
            
            spans.append({
                "source": doc_version_id,
                "page_no": page_no,
                "snippet": chunk.text[:200]  # åªå–å‰ 200 å­—ç¬¦ä½œä¸ºç‰‡æ®µ
            })
        
        return spans
    
    async def _extract_stages_parallel(
        self,
        stages: List[Dict],
        spec: Any,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str],
        embedding_provider: str,
        stage_results: Dict,
        all_evidence_chunk_ids: set,
        all_evidence_spans: List,
        all_traces: List,
    ) -> Dict[str, Any]:
        """
        å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰Stageï¼ˆ6ä¸ªStageåŒæ—¶æ‰§è¡Œï¼‰
        
        å®æ—¶æ›´æ–°messageå­—æ®µï¼Œå±•ç¤ºæ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„Stage
        """
        import json
        import asyncio
        from app.platform.extraction.parallel import ParallelExtractor, ParallelExtractionTask
        import time
        
        parallel_start = time.time()
        logger.info(f"[PARALLEL_TIMING] ========== PARALLEL EXTRACTION START at {parallel_start:.3f} ==========")
        logger.info(f"ExtractV2: Starting PARALLEL extraction with {len(stages)} stages")
        
        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        tasks = []
        for stage_info in stages:
            task = ParallelExtractionTask(
                task_id=f"stage_{stage_info['stage']}",
                spec=spec,
                project_id=project_id,
                stage=stage_info['stage'],
                stage_name=stage_info['name'],
                module_name="project_info",
            )
            tasks.append(task)
        
        # è·Ÿè¸ªæ­£åœ¨æ‰§è¡Œçš„Stage
        active_stages = set()
        completed_stages = set()
        
        def update_progress_message():
            """æ›´æ–°è¿›åº¦æ¶ˆæ¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰æ´»è·ƒçš„Stage"""
            if run_id:
                active_names = [stages[s-1]['name'] for s in sorted(active_stages)]
                completed_count = len(completed_stages)
                total = len(stages)
                progress = completed_count / total
                
                if active_names:
                    # æ„å»ºæ¶ˆæ¯ï¼šæ­£åœ¨æŠ½å–ï¼šé¡¹ç›®æ¦‚è§ˆã€æŠ•æ ‡äººèµ„æ ¼ã€è¯„å®¡ä¸è¯„åˆ†...
                    msg = f"æ­£åœ¨æŠ½å–ï¼š{('ã€').join(active_names)}"
                    if completed_count > 0:
                        msg += f" ({completed_count}/{total}å·²å®Œæˆ)"
                else:
                    msg = f"å¹¶è¡ŒæŠ½å–å®Œæˆ ({completed_count}/{total})"
                
                self.dao.update_run(run_id, "running", progress=progress, message=msg)
                logger.info(f"[ParallelExtract] {msg}")
        
        def on_task_start(stage_num: int):
            """ä»»åŠ¡å¼€å§‹å›è°ƒ"""
            active_stages.add(stage_num)
            update_progress_message()
        
        def on_task_complete(stage_num: int):
            """ä»»åŠ¡å®Œæˆå›è°ƒ"""
            if stage_num in active_stages:
                active_stages.remove(stage_num)
            completed_stages.add(stage_num)
            update_progress_message()
        
        # åˆ›å»ºå¹¶è¡ŒæŠ½å–å™¨
        extractor = ParallelExtractor(max_concurrent=6)  # 6ä¸ªStageå¹¶å‘
        
        # åŒ…è£…æ‰§è¡Œå‡½æ•°ä»¥æ·»åŠ å›è°ƒ
        async def execute_with_callbacks(task: ParallelExtractionTask):
            """æ‰§è¡Œå•ä¸ªä»»åŠ¡å¹¶è§¦å‘å›è°ƒ"""
            import time
            stage_num = task.stage
            stage_name = task.stage_name
            
            # è®°å½•ç²¾ç¡®å¼€å§‹æ—¶é—´
            task_start = time.time()
            logger.info(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) START at {task_start:.3f}")
            
            # å¼€å§‹å›è°ƒ
            on_task_start(stage_num)
            
            try:
                # æ‰§è¡ŒæŠ½å–
                result = await self.engine.run(
                    spec=task.spec,
                    retriever=self.retriever,
                    llm=self.llm,
                    project_id=task.project_id,
                    model_id=model_id,
                    run_id=run_id,
                    embedding_provider=embedding_provider,
                    stage=task.stage,
                    stage_name=task.stage_name,
                    module_name=task.module_name,
                )
                
                # è®°å½•ç²¾ç¡®å®Œæˆæ—¶é—´
                task_end = time.time()
                duration = task_end - task_start
                logger.info(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) END at {task_end:.3f}, duration={duration:.2f}s")
                
                # å®Œæˆå›è°ƒ
                on_task_complete(stage_num)
                
                return result
                
            except Exception as e:
                task_end = time.time()
                duration = task_end - task_start
                logger.error(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) FAILED at {task_end:.3f}, duration={duration:.2f}s, error={e}", exc_info=True)
                on_task_complete(stage_num)
                raise
        
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        try:
            results = await asyncio.gather(*[execute_with_callbacks(task) for task in tasks], return_exceptions=True)
            
            # å¤„ç†ç»“æœ
            for idx, result in enumerate(results):
                stage_info = stages[idx]
                stage_key = stage_info['key']
                
                if isinstance(result, Exception):
                    logger.error(f"ExtractV2: Stage {stage_info['stage']} failed: {result}")
                    stage_results[stage_key] = {}
                else:
                    # æå–æ•°æ®
                    stage_data = result.data.get(stage_key) if isinstance(result.data, dict) else result.data
                    if not stage_data:
                        stage_data = {}
                    
                    stage_results[stage_key] = stage_data
                    
                    # æ”¶é›†è¯æ®
                    all_evidence_chunk_ids.update(result.evidence_chunk_ids)
                    all_evidence_spans.extend(result.evidence_spans)
                    
                    # æ”¶é›†è¿½è¸ªä¿¡æ¯
                    if result.retrieval_trace:
                        all_traces.append({
                            "stage": stage_info['stage'],
                            "name": stage_info['name'],
                            "trace": result.retrieval_trace.__dict__
                        })
                    
                    logger.info(f"ExtractV2: Stage {stage_info['stage']}/6 completed in parallel mode")
            
        except Exception as e:
            logger.error(f"ExtractV2: Parallel extraction failed: {e}", exc_info=True)
            # å¤±è´¥æ—¶è®¾ç½®é»˜è®¤å€¼
            for stage_info in stages:
                if stage_info['key'] not in stage_results:
                    stage_results[stage_info['key']] = {}
        
        # åˆå¹¶æ‰€æœ‰é˜¶æ®µç»“æœï¼ˆV3ç»“æ„ï¼‰
        final_data = {
            "schema_version": "tender_info_v3",
            **{stage["key"]: stage_results.get(stage["key"], {}) for stage in stages}
        }
        
        parallel_end = time.time()
        total_duration = parallel_end - parallel_start
        logger.info(f"[PARALLEL_TIMING] ========== PARALLEL EXTRACTION END at {parallel_end:.3f}, TOTAL={total_duration:.2f}s ==========")
        
        logger.info(
            f"ExtractV2: PARALLEL extraction completed - "
            f"stages_completed={len([r for r in results if not isinstance(r, Exception)])}/6"
        )
        
        # âŒ å·²ç§»é™¤ï¼šè¿½åŠ è°ƒç”¨ requirements æŠ½å–
        # åŸå› ï¼šä¸å•ç‹¬çš„"æ‹›æ ‡è¦æ±‚æå–"åŠŸèƒ½é‡å¤
        # ç°åœ¨ç”¨æˆ·éœ€è¦å•ç‹¬ç‚¹å‡»"æ‹›æ ‡è¦æ±‚æå–"æŒ‰é’®æ¥ç”Ÿæˆ tender_requirements
        
        return {
            "schema_version": "tender_info_v3",
            **final_data,
            "evidence_chunk_ids": list(all_evidence_chunk_ids),
            "evidence_spans": all_evidence_spans,
            "retrieval_trace": all_traces,
        }
    
    async def extract_requirements_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        checklist_template: str = "engineering",
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        æŠ½å–æ‹›æ ‡è¦æ±‚ (v2) - æ¡†æ¶å¼è‡ªä¸»æå–
        
        âœ¨ æ–°æ–¹æ³•ï¼šç³»ç»Ÿå®šæ¡†æ¶ï¼ŒLLMè‡ªä¸»åˆ†æè¯†åˆ«æ‰€æœ‰è¦æ±‚
        
        ä¼˜åŠ¿ï¼š
        1. çµæ´»æ€§é«˜ï¼šä¸å—é¢„è®¾é—®é¢˜é™åˆ¶ï¼Œèƒ½æ•æ‰ç‰¹æ®Šå’Œç‹¬ç‰¹è¦æ±‚
        2. å®Œæ•´æ€§å¼ºï¼šLLMä¸»åŠ¨æœç´¢ï¼Œä¸æ˜“é—æ¼
        3. ç»“æ„åŒ–è¾“å‡ºï¼šä»ä¿æŒç»´åº¦ã€ç±»å‹ç­‰ç»“æ„ï¼Œä¾¿äºå®¡æ ¸
        4. æ™ºèƒ½è¿‡æ»¤ï¼šè‡ªåŠ¨æ’é™¤åˆåŒæ¡æ¬¾å’Œæ ¼å¼èŒƒä¾‹
        
        Args:
            project_id: é¡¹ç›®ID
            model_id: æ¨¡å‹IDï¼ˆé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®ï¼‰
            checklist_template: ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™å‚æ•°å…¼å®¹æ€§ï¼‰
            run_id: è¿è¡ŒIDï¼ˆå¯é€‰ï¼‰
        
        Returns:
            {
                "count": æå–çš„è¦æ±‚æ•°é‡,
                "requirements": [...],
                "extraction_method": "framework_autonomous",
                "schema_version": "requirements_v2_framework"
            }
        """
        logger.info(
            f"ExtractV2: extract_requirements_v2 start (framework mode) "
            f"project_id={project_id}"
        )
        
        try:
            # 1. ä½¿ç”¨æ¡†æ¶å¼Prompt Builder
            from .framework_prompt_builder import FrameworkPromptBuilder
            
            prompt_builder = FrameworkPromptBuilder()
            logger.info("Using framework-guided autonomous extraction")
            
            # 2. æ£€ç´¢æ‹›æ ‡æ–‡ä»¶ä¸Šä¸‹æ–‡
            logger.info("Retrieving tender document context...")
            
            # ä½¿ç”¨RetrievalFacadeæ£€ç´¢
            # âœ… æ‰©å±•æŸ¥è¯¢è¯ä»¥æ”¯æŒå¤šç§æ‹›æ ‡/é‡‡è´­ç±»å‹ï¼ˆå·¥ç¨‹ã€è´§ç‰©ã€æœåŠ¡ã€ç£‹å•†ç­‰ï¼‰
            context_chunks = await self.retriever.retrieve(
                query="æ‹›æ ‡æ–‡ä»¶ æŠ•æ ‡äººé¡»çŸ¥ è¯„åˆ†æ ‡å‡† æŠ€æœ¯è¦æ±‚ èµ„æ ¼æ¡ä»¶ å•†åŠ¡æ¡æ¬¾ å·¥æœŸ è´¨ä¿ ä»·æ ¼ ç£‹å•† èµ„ä¿¡ æŠ¥ä»· æ–¹æ¡ˆ åˆåŒ æˆæƒ èµ„è´¨ ä¿è¯é‡‘ æ‰¿è¯º è¯æ˜ ææ–™",
                project_id=project_id,
                doc_types=["tender"],
                top_k=150,  # è·å–è¶³å¤Ÿå¤šçš„ä¸Šä¸‹æ–‡
            )
            
            logger.info(f"Retrieved {len(context_chunks)} context chunks")
            
            # æ‹¼æ¥ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨å®é™…segment_idä½œä¸ºæ ‡è®°ï¼‰
            context_text = "\n\n".join([
                f"[SEG:{chunk.chunk_id}] {chunk.text}"
                for i, chunk in enumerate(context_chunks[:100])  # é™åˆ¶tokenæ•°
            ])
            
            # æ„å»ºsegment_idæ˜ å°„è¡¨ï¼ˆç”¨äºåç»­evidenceéªŒè¯ï¼‰
            segment_id_map = {chunk.chunk_id: chunk for chunk in context_chunks[:100]}
            
            if len(context_text) < 100:
                logger.warning("Context text too short, may not have enough information")
            
            # 3. æ„å»ºPromptå¹¶è°ƒç”¨LLMï¼ˆæ¡†æ¶å¼è‡ªä¸»æå–ï¼‰
            prompt = prompt_builder.build_prompt(context_text)
            
            logger.info(f"Built framework prompt, length: {len(prompt)} chars")
            
            # è°ƒç”¨LLMè¿›è¡Œè‡ªä¸»æå–
            messages = [{"role": "user", "content": prompt}]
            llm_response = await self.llm.achat(
                messages=messages,
                model_id=model_id,
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=16000,  # è‡ªä¸»æå–å¯èƒ½è¾“å‡ºè¾ƒå¤šè¦æ±‚
            )
            
            # æå–content
            llm_output = llm_response.get("choices", [{}])[0].get("message", {}).get("content")
            if llm_output is None:
                llm_output = "[]"  # Fallback to empty array
                logger.warning("LLM returned None content, using empty array")
            
            logger.info(f"Got LLM response, length: {len(llm_output)} chars")
            
            # 4. è§£æLLMè¿”å›çš„è¦æ±‚åˆ—è¡¨
            try:
                llm_requirements = prompt_builder.parse_llm_response(llm_output)
                logger.info(f"Parsed {len(llm_requirements)} requirements from LLM")
            except Exception as e:
                logger.error(f"Failed to parse LLM response: {e}")
                return {
                    "count": 0,
                    "requirements": [],
                    "error": f"LLM response parsing failed: {str(e)}",
                    "schema_version": "requirements_v2_framework"
                }
            
            # 5. éªŒè¯å¹¶è½¬æ¢ä¸ºæ•°æ®åº“æ ¼å¼
            # è·å–æ–‡æ¡£ç‰ˆæœ¬ID
            doc_version_id = await self._get_doc_version_id(project_id, "tender")
            
            requirements = prompt_builder.convert_to_db_format(
                llm_requirements=llm_requirements,
                project_id=project_id,
                doc_version_id=doc_version_id or 0,
            )
            
            logger.info(f"Converted to DB format: {len(requirements)} requirements")
            
            # 6. å»é‡ï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼åº¦ï¼‰
            seen_texts = {}
            unique_requirements = []
            for req in requirements:
                text = req.get("requirement_text", "").strip()
                text_normalized = text[:100].lower()  # ä½¿ç”¨å‰100å­—ç¬¦ä½œä¸ºæŒ‡çº¹
                
                if text_normalized and text_normalized not in seen_texts:
                    seen_texts[text_normalized] = req.get("item_id")
                    unique_requirements.append(req)
                else:
                    logger.warning(f"Duplicate content: {req.get('item_id')}")
            
            requirements = unique_requirements
            logger.info(f"After deduplication: {len(requirements)} requirements")
            
            # 7. åå¤„ç†ï¼šæ¨æ–­eval_method, must_rejectç­‰å­—æ®µ
            from .requirement_postprocessor import generate_bid_response_extraction_guide
            
            for req in requirements:
                # æ¨æ–­å®¡æ ¸æ–¹æ³•å’Œæƒé‡
                eval_method, must_reject, expected_evidence, rubric, weight = self._infer_routing_fields(req)
                
                req["eval_method"] = req.get("eval_method") or eval_method
                req["must_reject"] = req.get("must_reject") or must_reject
                if expected_evidence and not req.get("expected_evidence_json"):
                    req["expected_evidence_json"] = expected_evidence
                if rubric:
                    req["rubric_json"] = rubric
                if weight is not None:
                    req["weight"] = weight
            
            logger.info(f"Applied eval_method inference to {len(requirements)} requirements")
            
            # 8. ä¿å­˜åˆ°æ•°æ®åº“
            logger.info("Saving requirements to database...")
            
            import uuid
            from psycopg.types.json import Json
            
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    # å…ˆåˆ é™¤è¯¥é¡¹ç›®çš„æ—§requirementsï¼ˆé¿å…é‡å¤ï¼‰
                    cur.execute("DELETE FROM tender_requirements WHERE project_id = %s", (project_id,))
                    logger.info(f"Deleted old requirements for project {project_id}")
                    
                    for req in requirements:
                        # å¤„ç†JSONBå­—æ®µ
                        value_schema = req.get("value_schema_json")
                        if value_schema and not isinstance(value_schema, Json):
                            value_schema = Json(value_schema)
                        
                        expected_evidence_json = req.get("expected_evidence_json")
                        if expected_evidence_json and not isinstance(expected_evidence_json, Json):
                            expected_evidence_json = Json(expected_evidence_json)
                        
                        rubric_json = req.get("rubric_json")
                        if rubric_json and not isinstance(rubric_json, Json):
                            rubric_json = Json(rubric_json)
                        
                        # æ˜ å°„å­—æ®µåï¼šrequirement_type -> req_type, is_mandatory -> is_hard
                        req_type = req.get("requirement_type") or req.get("req_type", "semantic")
                        is_hard = req.get("is_mandatory") or req.get("is_hard", False)
                        requirement_id = req.get("item_id") or req.get("requirement_id", f"auto_{uuid.uuid4().hex[:8]}")
                        
                        # åˆå¹¶meta_jsonåˆ°value_schema_json
                        meta_json = req.get("meta_json", {})
                        if meta_json and value_schema:
                            # å¦‚æœå·²æœ‰value_schemaï¼Œåˆå¹¶metaä¿¡æ¯
                            combined_schema = value_schema if isinstance(value_schema, dict) else {}
                            combined_schema.update({"meta": meta_json})
                            value_schema = Json(combined_schema)
                        elif meta_json and not value_schema:
                            # å¦‚æœæ²¡æœ‰value_schemaï¼Œå°†metaä½œä¸ºvalue_schema
                            value_schema = Json(meta_json)
                        
                        cur.execute("""
                            INSERT INTO tender_requirements (
                                id, project_id, requirement_id, dimension, req_type,
                                requirement_text, is_hard, allow_deviation, 
                                value_schema_json, evidence_chunk_ids,
                                eval_method, must_reject, expected_evidence_json, rubric_json, weight
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            str(uuid.uuid4()),
                            project_id,
                            requirement_id,
                            req.get("dimension", "other"),
                            req_type,
                            req.get("requirement_text", ""),
                            is_hard,
                            req.get("allow_deviation", True),
                            value_schema,
                            req.get("evidence_chunk_ids", []),
                            req.get("eval_method", "SEMANTIC"),
                            req.get("must_reject", False),
                            expected_evidence_json,
                            rubric_json,
                            req.get("weight", 1.0),
                        ))
                    conn.commit()
            
            logger.info(f"ExtractV2: Saved {len(requirements)} requirements to DB")
            
            # 9. ç”Ÿæˆextraction_guideï¼ˆç”¨äºåç»­æŠ•æ ‡å“åº”æŠ½å–ï¼‰
            logger.info("Generating extraction guide for bid responses...")
            try:
                extraction_guide = generate_bid_response_extraction_guide(requirements)
                
                # ä¿å­˜åˆ°tender_projects.meta_json
                await self._update_project_meta(project_id, {
                    "extraction_guide": extraction_guide
                })
                
                logger.info(f"Extraction guide generated: {len(extraction_guide.get('categories', []))} categories")
            except Exception as e:
                logger.warning(f"Failed to generate extraction guide: {e}")
            
            # 10. ç»Ÿè®¡ç»´åº¦åˆ†å¸ƒ
            dimension_stats = {}
            for req in requirements:
                dim = req.get("dimension", "other")
                dimension_stats[dim] = dimension_stats.get(dim, 0) + 1
            
            logger.info(
                f"ExtractV2: extract_requirements_v2 complete - "
                f"{len(requirements)} requirements extracted, "
                f"dimensions: {dimension_stats}"
            )
            
            return {
                "count": len(requirements),
                "requirements": requirements,
                "dimension_distribution": dimension_stats,
                "extraction_method": "framework_autonomous",
                "schema_version": "requirements_v2_framework",
            }
        
        except Exception as e:
            logger.error(f"ExtractV2: extract_requirements_v2 failed: {e}", exc_info=True)
            raise
    
    async def _get_project_name(self, project_id: str) -> str:
        """è·å–é¡¹ç›®åç§°"""
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT name FROM tender_projects WHERE project_id = %s",
                        (project_id,)
                    )
                    row = cur.fetchone()
                    
                    if row:
                        return row[0]
        except Exception as e:
            logger.warning(f"Failed to get project name: {e}")
        
        return "æœ¬é¡¹ç›®"
    
    async def _get_doc_version_id(self, project_id: str, doc_type: str) -> Optional[int]:
        """è·å–æ–‡æ¡£ç‰ˆæœ¬ID"""
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT id FROM doc_versions WHERE project_id = %s AND doc_type = %s ORDER BY uploaded_at DESC LIMIT 1",
                        (project_id, doc_type)
                    )
                    row = cur.fetchone()
                    
                    if row:
                        return row[0]
        except Exception as e:
            logger.warning(f"Failed to get doc_version_id: {e}")
        
        return None
    
    async def _update_project_meta(self, project_id: str, meta_update: Dict[str, Any]):
        """æ›´æ–°é¡¹ç›®meta_json"""
        from psycopg.types.json import Json
        
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        UPDATE tender_projects
                        SET meta_json = COALESCE(meta_json, '{}'::jsonb) || %s::jsonb
                        WHERE id = %s
                        """,
                        (Json(meta_update), project_id)
                    )
                conn.commit()
                
                logger.info(f"Updated project meta for {project_id}")
        except Exception as e:
            logger.error(f"Failed to update project meta: {e}")
    
    async def _supplement_requirements_fulltext(
        self,
        project_id: str,
        model_id: Optional[str],
        existing_requirements: List[Dict[str, Any]],
        context_chunks: List[Any],
    ) -> List[Dict[str, Any]]:
        """
        P1ä¼˜åŒ–ï¼šå…¨æ–‡è¡¥å……æå–Checklistæœªè¦†ç›–çš„æ‹›æ ‡è¦æ±‚
        
        ç­–ç•¥ï¼š
        1. è®©LLMå…¨æ–‡æ‰«ææ‹›æ ‡æ–‡ä»¶
        2. è¯†åˆ«Checklistæœªè¦†ç›–çš„è¦æ±‚
        3. ç‰¹åˆ«å…³æ³¨ï¼šæŠ€æœ¯å‚æ•°è¡¨ã€åŠŸèƒ½æ¸…å•ã€ç‰¹æ®Šæ¡æ¬¾ã€é™„ä»¶è¦æ±‚ã€éšå«è¦æ±‚
        
        Args:
            project_id: é¡¹ç›®ID
            model_id: æ¨¡å‹ID
            existing_requirements: å·²æå–çš„è¦æ±‚åˆ—è¡¨
            context_chunks: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡chunks
        
        Returns:
            è¡¥å……çš„è¦æ±‚åˆ—è¡¨
        """
        logger.info(f"[è¡¥å……æå–] å¼€å§‹å…¨æ–‡æ‰«æï¼Œå·²æœ‰ {len(existing_requirements)} æ¡è¦æ±‚")
        
        try:
            # 1. æ„å»ºå·²æå–è¦æ±‚çš„æ‘˜è¦ï¼ˆç”¨äºå»é‡ï¼‰
            existing_summary = []
            for req in existing_requirements[:50]:  # é™åˆ¶æ‘˜è¦é•¿åº¦
                dim = req.get("dimension", "")
                text = req.get("requirement_text", "")[:60]
                existing_summary.append(f"[{dim}] {text}")
            
            existing_text = "\n".join(existing_summary)
            
            # 2. å‡†å¤‡å…¨æ–‡ä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ›´å¤šchunksï¼‰
            fulltext_context = "\n\n".join([
                f"[SEG:{chunk.chunk_id}] {chunk.text}"
                for chunk in context_chunks[:150]  # æ‰©å±•åˆ°150ä¸ªchunks
            ])
            
            # 3. æ„å»ºè¡¥å……æå–prompt
            supplement_prompt = f"""# æ‹›æ ‡è¦æ±‚è¡¥å……æå–ä»»åŠ¡

## èƒŒæ™¯
å·²é€šè¿‡æ ‡å‡†æ¸…å•æå–äº† {len(existing_requirements)} æ¡æ‹›æ ‡è¦æ±‚ï¼Œç°éœ€è¦å…¨æ–‡æ‰«ææ‹›æ ‡æ–‡ä»¶ï¼Œè¯†åˆ«é—æ¼çš„è¦æ±‚ã€‚

## å·²æå–è¦æ±‚æ‘˜è¦ï¼ˆå‰50æ¡ï¼‰
{existing_text}

## æ‹›æ ‡æ–‡ä»¶å…¨æ–‡
{fulltext_context}

## ä»»åŠ¡è¦æ±‚
è¯·å…¨æ–‡æ‰«ææ‹›æ ‡æ–‡ä»¶ï¼Œè¯†åˆ«ä»¥ä¸‹**æœªè¢«ç°æœ‰æ¸…å•è¦†ç›–çš„è¦æ±‚**ï¼š

### é‡ç‚¹å…³æ³¨ï¼ˆé«˜é¢‘é—æ¼ï¼‰
1. **æŠ€æœ¯å‚æ•°è¡¨ã€åŠŸèƒ½æ¸…å•**ï¼šè¯¦ç»†çš„æŠ€æœ¯æŒ‡æ ‡ã€æ€§èƒ½è¦æ±‚
2. **ç‰¹æ®Šæ¡æ¬¾ã€è¡¥å……è¯´æ˜**ï¼šåˆåŒç‰¹æ®Šæ¡æ¬¾ã€é™„åŠ è¦æ±‚
3. **é™„ä»¶æ¸…å•**ï¼šå¿…é¡»æä¾›çš„é™„ä»¶ã€è¯æ˜ææ–™
4. **è¯„åˆ†é¡¹éšå«è¦æ±‚**ï¼šè¯„åˆ†æ ‡å‡†ä¸­éšå«çš„äº¤ä»˜ç‰©è¦æ±‚
5. **æŠ•æ ‡ä¹¦ç›®å½•ç»“æ„è¦æ±‚**ï¼šç« èŠ‚ç»„æˆã€æ ¼å¼è¦æ±‚
6. **åç¦»è¡¨è¦æ±‚**ï¼šæŠ€æœ¯/å•†åŠ¡åç¦»è¡¨
7. **æ ·å“ã€æ¼”ç¤ºè¦æ±‚**ï¼šå¦‚æœ‰
8. **ç‰¹å®šèµ„è´¨/è®¤è¯è¦æ±‚**ï¼šè¡Œä¸šç‰¹å®šèµ„è´¨

### è¾“å‡ºæ ¼å¼
è¿”å›JSONæ•°ç»„ï¼Œ**åªåŒ…å«æ–°å‘ç°çš„ã€æœªè¢«å·²æå–è¦æ±‚è¦†ç›–çš„æ¡ç›®**ï¼š

```json
{{
  "supplement_requirements": [
    {{
      "dimension": "qualification|technical|business|price|doc_structure|schedule_quality|other",
      "req_type": "å…·ä½“ç±»å‹",
      "requirement_text": "è¦æ±‚å†…å®¹ï¼ˆè¯¦ç»†ä¸”å®Œæ•´ï¼‰",
      "is_hard": true/false,
      "eval_method": "PRESENCE|NUMERIC|EXACT_MATCH|SEMANTIC",
      "evidence_segment_ids": ["seg_xxx", "seg_yyy"],
      "reasoning": "ä¸ºä»€ä¹ˆè¿™æ˜¯ä¸€ä¸ªæ–°è¦æ±‚ï¼ˆæœªè¢«å·²æå–æ¸…å•è¦†ç›–ï¼‰"
    }}
  ]
}}
```

### å»é‡åŸåˆ™
- **ä»”ç»†å¯¹æ¯”å·²æå–æ¸…å•**ï¼šå¦‚æœæŸè¦æ±‚å·²è¢«è¦†ç›–ï¼ˆå³ä½¿è¡¨è¿°ä¸åŒï¼‰ï¼Œ**ä¸è¦é‡å¤æå–**
- **å®ç¼ºæ¯‹æ»¥**ï¼šä¸ç¡®å®šæ˜¯å¦é‡å¤æ—¶ï¼Œé€‰æ‹©ä¸æå–
- **èšç„¦ç‰¹æ®Šæ€§**ï¼šä¼˜å…ˆæå–ç‰¹æ®Šã€å…·ä½“çš„è¦æ±‚ï¼Œè€Œéé€šç”¨è¦æ±‚

### ç¤ºä¾‹
âœ… åº”è¯¥è¡¥å……ï¼š
- "æä¾›XXå“ç‰Œè®¤è¯è¯ä¹¦æˆ–åŒç­‰äº§å“è®¤è¯"ï¼ˆç‰¹å®šè®¤è¯è¦æ±‚ï¼‰
- "æŠ€æœ¯æ–¹æ¡ˆåº”åŒ…å«ç³»ç»Ÿæ¶æ„å›¾ã€æ•°æ®æµå›¾ã€éƒ¨ç½²æ‹“æ‰‘å›¾"ï¼ˆå…·ä½“äº¤ä»˜ç‰©ï¼‰
- "æŠ•æ ‡ä¹¦é¡»æä¾›åŸå‚æˆæƒä¹¦åŸä»¶"ï¼ˆç‰¹å®šæ–‡ä»¶è¦æ±‚ï¼‰

âŒ ä¸åº”è¡¥å……ï¼ˆå·²è¢«æ¸…å•è¦†ç›–ï¼‰ï¼š
- "è¥ä¸šæ‰§ç…§"ï¼ˆæ¸…å•qual_001å·²è¦†ç›–ï¼‰
- "èµ„è´¨è¯ä¹¦"ï¼ˆæ¸…å•qual_002å·²è¦†ç›–ï¼‰
- "æŠ•æ ‡æ€»ä»·"ï¼ˆæ¸…å•price_001å·²è¦†ç›–ï¼‰

è¯·å¼€å§‹åˆ†æå¹¶è¾“å‡ºJSONã€‚"""

            # 4. è°ƒç”¨LLM
            messages = [{"role": "user", "content": supplement_prompt}]
            llm_response = await self.llm.achat(
                messages=messages,
                model_id=model_id,
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=4096,
            )
            
            # 5. è§£æå“åº”
            import json
            llm_output = llm_response.get("choices", [{}])[0].get("message", {}).get("content")
            if not llm_output:
                logger.warning("[è¡¥å……æå–] LLMè¿”å›ç©ºå†…å®¹")
                return []
            
            try:
                result_data = json.loads(llm_output)
                supplement_items = result_data.get("supplement_requirements", [])
            except json.JSONDecodeError as e:
                logger.error(f"[è¡¥å……æå–] JSONè§£æå¤±è´¥: {e}")
                return []
            
            if not supplement_items:
                logger.info("[è¡¥å……æå–] æœªå‘ç°é¢å¤–è¦æ±‚")
                return []
            
            logger.info(f"[è¡¥å……æå–] LLMè¿”å› {len(supplement_items)} æ¡è¡¥å……è¦æ±‚")
            
            # 6. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            import uuid
            supplement_requirements = []
            
            for idx, item in enumerate(supplement_items):
                requirement_id = f"supplement_{idx+1:03d}"
                
                supplement_requirements.append({
                    "project_id": project_id,
                    "requirement_id": requirement_id,
                    "dimension": item.get("dimension", "other"),
                    "req_type": item.get("req_type", "other"),
                    "requirement_text": item.get("requirement_text", ""),
                    "is_hard": item.get("is_hard", False),
                    "allow_deviation": not item.get("is_hard", False),
                    "eval_method": item.get("eval_method", "SEMANTIC"),
                    "must_reject": False,
                    "evidence_chunk_ids": item.get("evidence_segment_ids", []),
                    "meta_json": {
                        "source": "fulltext_supplement",
                        "reasoning": item.get("reasoning", ""),
                    }
                })
            
            logger.info(f"[è¡¥å……æå–] æˆåŠŸè½¬æ¢ {len(supplement_requirements)} æ¡è¡¥å……è¦æ±‚")
            return supplement_requirements
            
        except Exception as e:
            logger.error(f"[è¡¥å……æå–] å¤±è´¥: {e}", exc_info=True)
            return []
    
    def _save_nodes_to_db(self, project_id: str, nodes: List[Dict[str, Any]]):
        """
        å°†èŠ‚ç‚¹åˆ—è¡¨ä¿å­˜åˆ° tender_directory_nodes è¡¨
        
        å‚è€ƒ directory_augment_v1.py çš„å®ç°
        
        âš ï¸ é‡è¦ï¼šæ­£ç¡®è®¡ç®— order_noï¼Œç¡®ä¿å­èŠ‚ç‚¹ç´§è·Ÿåœ¨çˆ¶èŠ‚ç‚¹ä¹‹å
        """
        import hashlib
        import json
        
        # âœ¨ æ­¥éª¤1ï¼šé‡æ–°è®¡ç®— order_noï¼Œç¡®ä¿æ˜¾ç¤ºé¡ºåºæ­£ç¡®
        # åŸå› ï¼šLLMè¿”å›çš„èŠ‚ç‚¹æ˜¯æ‰å¹³åˆ—è¡¨ï¼Œå­èŠ‚ç‚¹å¯èƒ½ä¸çˆ¶èŠ‚ç‚¹ä¸ç›¸é‚»
        # ç›®æ ‡ï¼šLevel 1èŠ‚ç‚¹æŒ‰é¡ºåºæ’åˆ—ï¼Œæ¯ä¸ªLevel 1çš„å­èŠ‚ç‚¹ç´§è·Ÿå…¶å
        
        # å…ˆä¸ºæ‰€æœ‰èŠ‚ç‚¹ç”ŸæˆIDï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
        for i, node in enumerate(nodes):
            if not node.get("id"):
                id_str = f"{project_id}_{node.get('title', '')}_{node.get('level', 0)}_{i}"
                node["id"] = f"dn_{hashlib.md5(id_str.encode()).hexdigest()[:16]}"
        
        # å»ºç«‹ title -> id æ˜ å°„ï¼ˆç”¨äº parent_ref è§£æï¼‰
        title_to_id = {node.get("title"): node.get("id") for node in nodes if node.get("title")}
        
        # è§£æ parent_id
        for node in nodes:
            if not node.get("parent_id") and node.get("parent_ref"):
                parent_title = node.get("parent_ref")
                node["parent_id"] = title_to_id.get(parent_title)
        
        # é‡æ–°è®¡ç®— order_noï¼šå…ˆLevel 1ï¼Œå†æ¯ä¸ªLevel 1çš„å­èŠ‚ç‚¹
        new_order = 1
        for node in nodes:
            if node.get("level") == 1:  # Level 1 èŠ‚ç‚¹
                node["_computed_order_no"] = new_order
                new_order += 1
                
                # æ‰¾åˆ°è¿™ä¸ªLevel 1èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹
                node_id = node.get("id")
                for child in nodes:
                    if child.get("parent_id") == node_id:
                        child["_computed_order_no"] = new_order
                        new_order += 1
        
        # å¯¹äºæ²¡æœ‰è¢«åˆ†é…order_noçš„èŠ‚ç‚¹ï¼ˆå­¤ç«‹èŠ‚ç‚¹ï¼‰ï¼ŒæŒ‰åŸé¡ºåºåˆ†é…
        for node in nodes:
            if "_computed_order_no" not in node:
                node["_computed_order_no"] = new_order
                new_order += 1
                logger.warning(f"Node '{node.get('title')}' (level={node.get('level')}) has no parent, assigned order_no={new_order-1}")
        
        # âœ¨ æ­¥éª¤2ï¼šä¿å­˜åˆ°æ•°æ®åº“
        with self.pool.connection() as conn:
            with conn.cursor() as cur:
                for node in nodes:
                    node_id = node.get("id")
                    parent_id = node.get("parent_id")
                    
                    # æ„å»º meta_json
                    meta_json = {
                        "notes": node.get("notes", ""),
                        "volume": node.get("volume", ""),
                        "template_chunk_ids": node.get("template_chunk_ids", []),
                    }
                    if node.get("parent_ref"):
                        meta_json["parent_ref"] = node.get("parent_ref")
                    
                    # INSERTèŠ‚ç‚¹
                    cur.execute("""
                        INSERT INTO tender_directory_nodes (
                            id, project_id, parent_id, order_no, level, numbering,
                            title, is_required, source, evidence_chunk_ids, meta_json,
                            created_at
                        ) VALUES (
                            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, CURRENT_TIMESTAMP
                        )
                        ON CONFLICT (id) DO UPDATE SET
                            title = EXCLUDED.title,
                            parent_id = EXCLUDED.parent_id,
                            order_no = EXCLUDED.order_no,
                            level = EXCLUDED.level,
                            numbering = EXCLUDED.numbering,
                            is_required = EXCLUDED.is_required,
                            source = EXCLUDED.source,
                            evidence_chunk_ids = EXCLUDED.evidence_chunk_ids,
                            meta_json = EXCLUDED.meta_json,
                            updated_at = CURRENT_TIMESTAMP
                    """, [
                        node_id,
                        project_id,
                        parent_id,
                        node.get("_computed_order_no"),  # âœ… ä½¿ç”¨é‡æ–°è®¡ç®—çš„ order_no
                        node.get("level", 1),
                        node.get("numbering", ""),
                        node.get("title", "æœªå‘½å"),
                        node.get("required", True) or node.get("is_required", True),
                        node.get("source", "LLM_GENERATED"),
                        node.get("evidence_chunk_ids", []),
                        json.dumps(meta_json, ensure_ascii=False)
                    ])
                
                conn.commit()
                logger.info(f"_save_nodes_to_db: Committed {len(nodes)} nodes for project={project_id} with corrected order_no")
    
