"""
新抽取服务 (v2) - Step 3
基于平台 ExtractionEngine 的项目信息/风险抽取/目录生成
"""
import logging
from typing import Any, Dict, List, Optional

from psycopg_pool import ConnectionPool

from app.platform.extraction.engine import ExtractionEngine
from app.platform.retrieval.facade import RetrievalFacade
from app.services.embedding_provider_store import get_embedding_store
from .extraction_specs.directory_v2 import build_directory_spec_async

logger = logging.getLogger(__name__)


class ExtractV2Service:
    """新抽取服务 - 使用平台 ExtractionEngine"""
    
    def __init__(self, pool: ConnectionPool, llm_orchestrator: Any = None):
        self.pool = pool
        self.retriever = RetrievalFacade(pool)
        self.llm = llm_orchestrator
        self.engine = ExtractionEngine()
        # 创建DAO用于更新run状态和保存数据
        from app.services.dao.tender_dao import TenderDAO
        self.dao = TenderDAO(pool)
    
    async def extract_project_info_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
        use_staged: bool = True,
    ) -> Dict[str, Any]:
        """
        抽取项目信息 (v3) - 九阶段顺序抽取
        
        ⚠️ V3 版本：六大类招标信息抽取（合并版）
        
        优先从数据库加载最新的prompt模板，如果数据库中没有则使用文件fallback
        
        Args:
            project_id: 项目ID
            model_id: 模型ID
            run_id: 运行ID
            use_staged: 是否使用九阶段抽取（默认True）
        
        Returns:
            {
                "schema_version": "tender_info_v3",
                "project_overview": {...},
                "scope_and_lots": {...},
                "schedule_and_submission": {...},
                "bidder_qualification": {...},
                "evaluation_and_scoring": {...},
                "business_terms": {...},
                "technical_requirements": {...},
                "document_preparation": {...},
                "bid_security": {...},
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "retrieval_trace": {...}
            }
        """
        logger.info(f"ExtractV2: extract_project_info_v3 start project_id={project_id} use_staged={use_staged}")
        
        # 1. 获取 embedding provider
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        # 2. 构建 spec（使用异步版本，支持数据库加载）
        from app.works.tender.extraction_specs.project_info_v2 import build_project_info_spec_async
        
        if use_staged:
            # 使用九阶段抽取（V3版本）
            # 从环境变量读取是否启用并行
            import os
            parallel_enabled = os.getenv("EXTRACT_PROJECT_INFO_PARALLEL", "false").lower() in ("true", "1", "yes")
            
            return await self._extract_project_info_staged(
                project_id=project_id,
                model_id=model_id,
                run_id=run_id,
                embedding_provider=embedding_provider,
                parallel=parallel_enabled,
            )
        else:
            # 使用原有的一次性抽取（不推荐，返回V3结构）
            spec = await build_project_info_spec_async(self.pool)
            
            # 3. 调用引擎
            result = await self.engine.run(
                spec=spec,
                retriever=self.retriever,
                llm=self.llm,
                project_id=project_id,
                model_id=model_id,
                run_id=run_id,
                embedding_provider=embedding_provider,
            )
            
            # 调试日志：查看实际返回的数据结构
            logger.info(
                f"ExtractV2: extract_project_info_v3 done "
                f"evidence={len(result.evidence_chunk_ids)} "
                f"data_keys={list(result.data.keys()) if isinstance(result.data, dict) else 'NOT_DICT'} "
                f"data_type={type(result.data).__name__} "
                f"data_empty={not bool(result.data)}"
            )
            
            # 如果 data 是空的，记录警告
            if not result.data or (isinstance(result.data, dict) and not result.data):
                logger.warning(
                    f"ExtractV2: extract_project_info_v3 returned EMPTY data! "
                    f"project_id={project_id} "
                    f"result.data={result.data}"
            )
            
            # 4. 返回结果（V3结构：顶层直接是六大类）
            return {
                "schema_version": "tender_info_v3",
                **result.data,  # 展开六大类
                "evidence_chunk_ids": result.evidence_chunk_ids,
                "evidence_spans": result.evidence_spans,
                "retrieval_trace": result.retrieval_trace.__dict__ if result.retrieval_trace else {}
            }
    
    async def _extract_project_info_staged(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str],
        embedding_provider: str,
        parallel: bool = False,
    ) -> Dict[str, Any]:
        """
        六阶段顺序抽取项目信息（V3版本 - Checklist-based）
        
        ✨ 新方法：基于Checklist的P0+P1两阶段提取框架
        
        Stage 1: project_overview (项目概览 - 含范围、进度、保证金)
        Stage 2: bidder_qualification (投标人资格)
        Stage 3: evaluation_and_scoring (评审与评分)
        Stage 4: business_terms (商务条款)
        Stage 5: technical_requirements (技术要求)
        Stage 6: document_preparation (文件编制)
        
        每个stage采用：
        - P0阶段：基于checklist的结构化提取
        - P1阶段：补充扫描遗漏信息
        - 验证：检查必填字段和证据
        
        Args:
            parallel: 是否并行执行所有Stage（支持最多6个并行）
        """
        import os
        import asyncio
        from app.works.tender.project_info_extractor import ProjectInfoExtractor
        from app.works.tender.tender_context_retriever import TenderContextRetriever
        
        logger.info(
            f"ExtractV2: Starting CHECKLIST-BASED extraction (V3 - 6 stages) "
            f"for project={project_id} parallel={parallel}"
        )
        
        # 定义六个阶段元数据（用于进度展示）
        stages_meta = [
            {"stage": 1, "name": "项目概览", "key": "project_overview"},
            {"stage": 2, "name": "投标人资格", "key": "bidder_qualification"},
            {"stage": 3, "name": "评审与评分", "key": "evaluation_and_scoring"},
            {"stage": 4, "name": "商务条款", "key": "business_terms"},
            {"stage": 5, "name": "技术要求", "key": "technical_requirements"},
            {"stage": 6, "name": "文件编制", "key": "document_preparation"},
        ]
        
        try:
            # ===== 步骤1：统一检索招标文档上下文 =====
            logger.info("Step 1: 检索招标文档上下文（一次性检索）")
            
            if run_id:
                self.dao.update_run(
                    run_id, "running", progress=0.05, 
                    message="正在检索招标文档..."
                )
            
            context_retriever = TenderContextRetriever(self.retriever)
            context_data = await context_retriever.retrieve_tender_context(
                project_id=project_id,
                top_k=150,  # 获取足够多的上下文
                max_context_chunks=100,
                sort_by_position=True
            )
            
            if context_data.used_chunks == 0:
                raise ValueError(f"未检索到招标文档内容，project_id={project_id}")
            
            logger.info(
                f"上下文检索完成: total={context_data.total_chunks}, "
                f"used={context_data.used_chunks}, "
                f"context_length={len(context_data.context_text)}"
            )
            
            # ===== 步骤2：创建Checklist提取器 =====
            logger.info("Step 2: 初始化Checklist提取器")
            
            extractor = ProjectInfoExtractor(llm=self.llm)
            
            # ===== 步骤3：执行6个stage（支持并行或顺序） =====
            logger.info(f"Step 3: 开始6阶段checklist提取 (parallel={parallel})")
            
            all_stage_results = {}
            all_evidence_ids = set()
            
            if parallel:
                # ===== 并行模式：所有stage同时执行 =====
                logger.info("使用并行模式，所有6个stage同时执行")
                
                # 获取最大并发数
                max_concurrent = int(os.getenv("EXTRACT_MAX_CONCURRENT", "6"))
                logger.info(f"最大并发数: {max_concurrent}")
                
                # 创建信号量限制并发
                semaphore = asyncio.Semaphore(max_concurrent)
                
                async def extract_stage_with_semaphore(stage_meta):
                    async with semaphore:
                        stage_num = stage_meta["stage"]
                        stage_name = stage_meta["name"]
                        stage_key = stage_meta["key"]
                        
                        logger.info(f"=== 开始并行提取 Stage {stage_num}/6: {stage_name} ===")
                        
                        try:
                            stage_result = await extractor.extract_stage(
                                stage=stage_num,
                                context_text=context_data.context_text,
                                segment_id_map=context_data.segment_id_map,
                                model_id=model_id,
                                context_info=None,  # 并行模式下不传递context
                                enable_p1=True
                            )
                            
                            logger.info(
                                f"Stage {stage_num} 完成: "
                                f"fields={len(stage_result['data'])}, "
                                f"evidence={len(stage_result['evidence_segment_ids'])}"
                            )
                            
                            return stage_key, stage_result
                            
                        except Exception as e:
                            logger.error(f"Stage {stage_num} 失败: {e}", exc_info=True)
                            return stage_key, {
                                "data": {},
                                "evidence_segment_ids": [],
                                "p1_supplements_count": 0
                            }
                
                # 并行执行所有stage
                if run_id:
                    self.dao.update_run(
                        run_id, "running", progress=0.10,
                        message="正在并行提取所有阶段..."
                    )
                
                tasks = [extract_stage_with_semaphore(meta) for meta in stages_meta]
                results = await asyncio.gather(*tasks)
                
                # 收集结果
                for stage_key, stage_result in results:
                    all_stage_results[stage_key] = stage_result["data"]
                    all_evidence_ids.update(stage_result["evidence_segment_ids"])
                
                # 更新进度
                if run_id:
                    self.dao.update_run(
                        run_id, "running", progress=0.90,
                        message="所有阶段提取完成，正在保存..."
                    )
                
            else:
                # ===== 顺序模式：一个接一个执行（支持context传递） =====
                logger.info("使用顺序模式，stage依次执行")
                context_info = None  # 用于传递前序stage的结果
                
                for stage_meta in stages_meta:
                    stage_num = stage_meta["stage"]
                    stage_name = stage_meta["name"]
                    stage_key = stage_meta["key"]
                    
                    logger.info(f"=== Extracting Stage {stage_num}/6: {stage_name} ===")
                    
                    # 更新进度
                    if run_id:
                        progress = 0.05 + (stage_num - 1) * 0.15  # 0.05, 0.20, 0.35, 0.50, 0.65, 0.80
                        self.dao.update_run(
                            run_id, "running", progress=progress,
                            message=f"正在抽取：{stage_name} (P0+P1)..."
                        )
                    
                    try:
                        # 调用extractor提取单个stage
                        stage_result = await extractor.extract_stage(
                            stage=stage_num,
                            context_text=context_data.context_text,
                            segment_id_map=context_data.segment_id_map,
                            model_id=model_id,
                            context_info=context_info,  # 传递前序stage的结果
                            enable_p1=True  # 启用P1补充扫描
                        )
                        
                        # 保存结果
                        all_stage_results[stage_key] = stage_result["data"]
                        all_evidence_ids.update(stage_result["evidence_segment_ids"])
                        
                        # 传递context给下一个stage
                        if context_info is None:
                            context_info = {}
                        context_info[stage_key] = stage_result["data"]
                        
                        logger.info(
                            f"Stage {stage_num} complete: "
                            f"fields={len(stage_result['data'])}, "
                            f"evidence={len(stage_result['evidence_segment_ids'])}, "
                            f"p1_supplements={stage_result['p1_supplements_count']}"
                        )
                        
                        # ✅ 增量保存到数据库（顺序模式）
                        incremental_data = {
                            "schema_version": "tender_info_v3",
                            **{k: all_stage_results.get(k, {}) for k in [s["key"] for s in stages_meta]}
                        }
                        self.dao.upsert_project_info(
                            project_id,
                            data_json=incremental_data,
                            evidence_chunk_ids=list(all_evidence_ids)
                        )
                        
                        # 更新完成进度
                        if run_id:
                            progress = 0.05 + stage_num * 0.15  # 0.20, 0.35, 0.50, 0.65, 0.80, 0.95
                            self.dao.update_run(
                                run_id, "running", progress=progress,
                                message=f"{stage_name}已完成"
                            )
                        
                    except Exception as e:
                        logger.error(f"Stage {stage_num} failed: {e}", exc_info=True)
                        # 失败时设置空数据，继续执行后续stage
                        all_stage_results[stage_key] = {}
                        
                        # 记录错误但不中断流程
                        if run_id:
                            self.dao.update_run(
                                run_id, "running",
                                message=f"{stage_name}提取失败，继续下一阶段"
                            )
            
            # ===== 步骤4：最终保存（并行模式需要在这里保存） =====
            if parallel:
                incremental_data = {
                    "schema_version": "tender_info_v3",
                    **{k: all_stage_results.get(k, {}) for k in [s["key"] for s in stages_meta]}
                }
                self.dao.upsert_project_info(
                    project_id,
                    data_json=incremental_data,
                    evidence_chunk_ids=list(all_evidence_ids)
                )
            
            # ===== 步骤5：验证提取结果 =====
            logger.info("Step 5: 验证提取结果")
            
            final_result = {
                "schema_version": "tender_info_v3",
                **all_stage_results,
                "evidence_chunk_ids": list(all_evidence_ids)
            }
            
            validation_report = extractor.validate_result(final_result)
            
            if not validation_report["is_valid"]:
                logger.warning(
                    f"Validation failed: errors={validation_report['errors']}"
                )
            
            if validation_report["warnings"]:
                logger.warning(
                    f"Validation warnings: {validation_report['warnings']}"
                )
            
            # ===== 步骤6：构建最终返回结果 =====
            logger.info(
                f"ExtractV2: CHECKLIST-BASED extraction complete - "
                f"mode={'parallel' if parallel else 'sequential'}, "
                f"stages_completed={len([r for r in all_stage_results.values() if r])}/6, "
                f"evidence_segments={len(all_evidence_ids)}"
            )
            
            # 构建evidence_spans（兼容旧格式）
            evidence_spans = []
            for seg_id in list(all_evidence_ids)[:50]:  # 限制数量
                chunk = context_data.segment_id_map.get(seg_id)
                if chunk:
                    meta = chunk.meta or {}
                    evidence_spans.append({
                        "source": meta.get("doc_version_id", ""),
                        "page_no": meta.get("page_no", 0),
                        "snippet": chunk.text[:200]
                    })
            
            # ===== 步骤6：构建完整的结果（确保包含所有6个stage） =====
            final_result = {
                "schema_version": "tender_info_v3",
                # 明确列出所有6个stage，即使某些为空
                "project_overview": all_stage_results.get("project_overview", {}),
                "bidder_qualification": all_stage_results.get("bidder_qualification", {}),
                "evaluation_and_scoring": all_stage_results.get("evaluation_and_scoring", {}),
                "business_terms": all_stage_results.get("business_terms", {}),
                "technical_requirements": all_stage_results.get("technical_requirements", {}),
                "document_preparation": all_stage_results.get("document_preparation", {}),
                "evidence_chunk_ids": list(all_evidence_ids),
                "evidence_spans": evidence_spans,
                "retrieval_trace": {
                    "mode": "checklist_based_v3",
                    "method": "P0+P1",
                    "stages": len(all_stage_results),
                    "validation": validation_report
                }
            }
            
            # ===== 步骤7：最终确认保存（确保数据完整） =====
            # 最后再保存一次，确保所有stage的数据都已保存
            logger.info("最终保存项目信息到数据库...")
            data_to_save_final = {
                "schema_version": "tender_info_v3",
                "project_overview": all_stage_results.get("project_overview", {}),
                "bidder_qualification": all_stage_results.get("bidder_qualification", {}),
                "evaluation_and_scoring": all_stage_results.get("evaluation_and_scoring", {}),
                "business_terms": all_stage_results.get("business_terms", {}),
                "technical_requirements": all_stage_results.get("technical_requirements", {}),
                "document_preparation": all_stage_results.get("document_preparation", {}),
            }
            self.dao.upsert_project_info(
                project_id,
                data_json=data_to_save_final,
                evidence_chunk_ids=list(all_evidence_ids)
            )
            logger.info("项目信息已保存到数据库")
            
            # ===== 步骤8：更新run进度为接近完成 =====
            if run_id:
                logger.info(f"更新run进度: run_id={run_id}")
                self.dao.update_run(
                    run_id, 
                    "running",  # 保持running状态，由TenderService最终更新为success
                    progress=0.98,
                    message="项目信息提取完成，正在保存..."
                )
            
            return final_result
            
        except Exception as e:
            logger.error(f"Checklist-based extraction failed: {e}", exc_info=True)
            
            if run_id:
                self.dao.update_run(run_id, "failed", message=f"提取失败: {str(e)}")
            
            raise
    
    # extract_risks_v2 已删除，请使用 extract_requirements_v1
    # risks模块已废弃，统一使用requirements模块提取招标要求
    
    async def extract_requirements_v1(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """
        ❌ 已废弃：V1招标要求提取已废弃
        
        请使用 extract_requirements_v2（标准清单方式）
        
        废弃时间：2025-12-29
        废弃原因：V2标准清单方式提供更高质量的数据（100% norm_key覆盖）
        """
        raise NotImplementedError(
            "❌ V1招标要求提取已废弃，请使用 extract_requirements_v2（标准清单方式）"
        )
    
    def _infer_routing_fields(self, req: Dict[str, Any]) -> tuple:
        """
        推断路由字段（Step 2）
        
        根据 requirement 的特征推断 eval_method、must_reject 等字段
        
        Returns:
            (eval_method, must_reject, expected_evidence_json, rubric_json, weight)
        """
        dimension = req.get("dimension", "")
        req_type = req.get("req_type", "")
        requirement_text = req.get("requirement_text", "")
        is_hard = req.get("is_hard", False)
        value_schema = req.get("value_schema_json", {})
        
        # 默认值
        eval_method = "PRESENCE"  # 默认为存在性检查
        must_reject = False
        expected_evidence = None
        rubric = None
        weight = 1.0
        
        # 1. 根据 dimension 和 req_type 推断 eval_method
        if dimension == "qualification":
            # 资格类多为存在性/有效性检查
            if "营业执照" in requirement_text or "资质证书" in requirement_text or "许可证" in requirement_text:
                eval_method = "VALIDITY"
                expected_evidence = {"doc_types": ["license", "certificate"], "fields": ["expire_date", "scope"]}
            elif "业绩" in requirement_text or "项目经验" in requirement_text:
                eval_method = "VALIDITY"
                expected_evidence = {"doc_types": ["performance"], "fields": ["project_name", "contract_amount", "completion_date"]}
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "price":
            # 价格类多为数值比较
            eval_method = "NUMERIC"
            if value_schema and isinstance(value_schema, dict):
                expected_evidence = {
                    "type": "numeric",
                    "constraints": value_schema
                }
        
        elif dimension == "technical":
            # 技术类：参数表为 TABLE_COMPARE，评分点为 SEMANTIC
            if req_type == "scoring":
                eval_method = "SEMANTIC"
                # 提取评分细则作为 rubric
                score = self._extract_score(requirement_text)
                rubric = {
                    "criteria": requirement_text,
                    "scoring_method": "LLM",
                    "max_points": score
                }
            elif "参数表" in requirement_text or "规格表" in requirement_text:
                eval_method = "TABLE_COMPARE"
            elif ("参数" in requirement_text or "规格" in requirement_text or "指标" in requirement_text or
                  "不低于" in requirement_text or "不超过" in requirement_text or "≥" in requirement_text or "≤" in requirement_text):
                # 包含数值比较关键词，使用 NUMERIC
                eval_method = "NUMERIC"
            elif "偏离" in requirement_text and "不允许" in requirement_text:
                eval_method = "EXACT_MATCH"
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "business":
            # 商务类：工期/质保/付款等为数值检查，评分点为语义
            if req_type == "scoring":
                eval_method = "SEMANTIC"
                score = self._extract_score(requirement_text)
                rubric = {
                    "criteria": requirement_text,
                    "scoring_method": "LLM",
                    "max_points": score
                }
            elif "工期" in requirement_text or "质保" in requirement_text or "付款" in requirement_text:
                eval_method = "NUMERIC"
            else:
                eval_method = "PRESENCE"
        
        elif dimension == "doc_structure":
            # 文档结构类多为存在性检查
            eval_method = "PRESENCE"
        
        # 2. 根据 is_hard 和 req_type 推断 must_reject
        if is_hard and req_type in ("threshold", "must_provide", "must_not_deviate"):
            must_reject = True
        
        # 3. 权重：评分项权重较高
        if req_type == "scoring":
            score = self._extract_score(requirement_text)
            weight = score if score else 5.0
        elif must_reject:
            weight = 10.0  # 必须项权重最高
        
        return eval_method, must_reject, expected_evidence, rubric, weight
    
    def _extract_score(self, text: str) -> Optional[float]:
        """从文本中提取分值"""
        import re
        # 匹配"XX分"、"X-XX分"、"最多XX分"、"不超过XX分"等模式
        patterns = [
            r'最多\s*(\d+(?:\.\d+)?)\s*分',
            r'不超过\s*(\d+(?:\.\d+)?)\s*分',
            r'(?:得|为)\s*(\d+(?:\.\d+)?)\s*分',
            r'(\d+(?:\.\d+)?)\s*分',
            r'(\d+(?:\.\d+)?)\s*points?',
        ]
        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    return float(match.group(1))
                except:
                    pass
        return None
    
    async def generate_directory_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str] = None,
        use_fast_mode: bool = True,
        enable_refinement: bool = True,  # 阶段4-A：规则细化
        enable_bracket_parsing: bool = True,  # 阶段4-B：LLM括号解析
        enable_template_matching: bool = True,  # ✨ 阶段5：格式范本自动填充
    ) -> Dict[str, Any]:
        """
        生成目录 (v2) - 多阶段生成
        
        阶段1（快速模式）：如果有项目信息，直接构建骨架
        阶段2（LLM补充）：检索补全细节或全新生成
        阶段3（目录增强）：补充遗漏的必填节点
        阶段4-A（规则细化）：基于招标要求细化评分标准、资格审查等节点
        阶段4-B（LLM括号解析）：解析括号说明，生成L4细分节点
        阶段5（格式范本填充）：自动识别并填充格式范本到节点正文 ✨ 新增
        
        Args:
            use_fast_mode: 是否启用快速模式（默认True）
            enable_refinement: 是否启用规则细化（默认True，设为False可回退）
            enable_bracket_parsing: 是否启用LLM括号解析（默认True，设为False可回退）
            enable_template_matching: 是否启用格式范本匹配（默认True，设为False可回退）
        
        Returns:
            {
                "data": {
                    "nodes": [...]
                },
                "evidence_chunk_ids": [...],
                "evidence_spans": [...],
                "generation_mode": "fast" | "llm" | "hybrid",
                "refinement_stats": {...},
                "bracket_parsing_stats": {...},
                "template_matching_stats": {...}  # ✨ 新增：范本填充统计
            }
        """
        logger.info(f"ExtractV2: generate_directory start project_id={project_id}, fast_mode={use_fast_mode}")
        
        # 阶段1：尝试快速模式
        fast_nodes = []
        fast_stats = {}
        generation_mode = "llm"  # 默认全LLM
        
        if use_fast_mode:
            tender_info = self.dao.get_project_info(project_id)
            if tender_info and tender_info.get("schema_version") == "tender_info_v3":
                try:
                    from app.works.tender.directory_fast_builder import build_directory_from_project_info
                    
                    fast_nodes, fast_stats = build_directory_from_project_info(
                        project_id=project_id,
                        pool=self.pool,
                        tender_info=tender_info
                    )
                    
                    if fast_nodes and len(fast_nodes) >= 5:  # 至少5个节点才认为有效
                        logger.info(
                            f"ExtractV2: Fast mode success - {len(fast_nodes)} nodes, "
                            f"skip LLM generation"
                        )
                        generation_mode = "fast"
                        
                        # 快速模式成功，直接返回
                        return {
                            "data": {"nodes": fast_nodes},
                            "evidence_chunk_ids": [],
                            "evidence_spans": [],
                            "retrieval_trace": {},
                            "generation_mode": generation_mode,
                            "fast_stats": fast_stats
                        }
                    else:
                        logger.info(f"ExtractV2: Fast mode insufficient ({len(fast_nodes)} nodes), fallback to LLM")
                        generation_mode = "hybrid"
                        
                except Exception as e:
                    logger.warning(f"ExtractV2: Fast mode failed (non-fatal): {e}")
                    generation_mode = "llm"
            else:
                logger.info("ExtractV2: No project_info available, using LLM mode")
        
        # 阶段2：LLM生成（全新或补充）
        logger.info(f"ExtractV2: Starting LLM generation mode={generation_mode}")
        
        # 1. 获取 embedding provider
        embedding_provider = get_embedding_store().get_default()
        if not embedding_provider:
            raise ValueError("No embedding provider configured")
        
        # 2. 构建 spec
        spec = await build_directory_spec_async(self.pool)
        
        # 3. 调用引擎
        result = await self.engine.run(
            spec=spec,
            retriever=self.retriever,
            llm=self.llm,
            project_id=project_id,
            model_id=model_id,
            run_id=run_id,
            embedding_provider=embedding_provider,
        )
        
        # 4. 验证结果
        if not result.data or not isinstance(result.data, dict):
            logger.error(f"ExtractV2: directory data invalid, type={type(result.data)}")
            raise ValueError("Directory extraction returned invalid data")
        
        nodes = result.data.get("nodes", [])
        if not nodes:
            logger.warning(f"ExtractV2: no directory nodes extracted for project={project_id}")
        
        # 5. 如果是混合模式，合并快速节点和LLM节点
        if generation_mode == "hybrid" and fast_nodes:
            logger.info(f"ExtractV2: Merging fast nodes ({len(fast_nodes)}) with LLM nodes ({len(nodes)})")
            # 简单策略：优先使用快速节点，LLM节点作为补充
            nodes = fast_nodes + nodes
        
        logger.info(f"ExtractV2: generate_directory done nodes={len(nodes)}, mode={generation_mode}")
        
        # 5. 目录增强 - 利用 tender_info_v3 补充必填节点
        try:
            logger.info(f"ExtractV2: Attempting directory augmentation for project={project_id}")
            
            # 读取 tender_project_info
            tender_info = self.dao.get_project_info(project_id)
            if tender_info and tender_info.get("schema_version") == "tender_info_v3":
                from app.works.tender.directory_augment_v1 import augment_directory_from_tender_info_v3
                
                augment_result = augment_directory_from_tender_info_v3(
                    project_id=project_id,
                    pool=self.pool,
                    tender_info=tender_info
                )
                
                logger.info(
                    f"ExtractV2: Directory augmentation done - "
                    f"added={augment_result['added_count']}, "
                    f"titles={augment_result['enhanced_titles'][:5]}"
                )
        except Exception as e:
            logger.warning(f"ExtractV2: Directory augmentation failed (non-fatal): {e}")
        
        # ✨ 6. 规则细化 - 基于招标要求细化评分标准、资格审查等节点（新增阶段4）
        refinement_stats = {}
        try:
            if enable_refinement:
                logger.info(f"ExtractV2: Starting rule-based refinement for project={project_id}")
                
                from app.works.tender.directory_refinement_rule import refine_directory_from_requirements
                
                refinement_result = refine_directory_from_requirements(
                    project_id=project_id,
                    pool=self.pool,
                    nodes=nodes,
                    enable_refinement=True,
                )
                
                # 更新节点列表
                nodes = refinement_result["refined_nodes"]
                refinement_stats = refinement_result["stats"]
                
                logger.info(
                    f"ExtractV2: Rule-based refinement done - "
                    f"added={refinement_result['added_count']} nodes, "
                    f"total={len(nodes)}, "
                    f"refined_parents={refinement_result['refined_parents']}"
                )
            else:
                logger.info(f"ExtractV2: Rule-based refinement disabled")
                refinement_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: Rule-based refinement failed (non-fatal): {e}")
            refinement_stats = {"error": str(e)}
        
        # ✨ 7. LLM括号解析 - 解析L3节点的括号说明，生成L4子节点（新增阶段4-B）
        bracket_parsing_stats = {}
        try:
            if enable_bracket_parsing:
                logger.info(f"ExtractV2: Starting LLM-based bracket parsing for project={project_id}")
                
                from app.works.tender.directory_bracket_parser import parse_brackets_with_llm
                
                bracket_result = await parse_brackets_with_llm(
                    nodes=nodes,
                    llm=self.llm,
                    model_id=model_id,
                    enable_parsing=True,
                )
                
                # 更新节点列表
                nodes = bracket_result["enhanced_nodes"]
                bracket_parsing_stats = bracket_result["stats"]
                
                logger.info(
                    f"ExtractV2: LLM bracket parsing done - "
                    f"added={bracket_result['added_count']} L4 nodes, "
                    f"total={len(nodes)}, "
                    f"parsed_parents={bracket_result['parsed_parents']}"
                )
            else:
                logger.info(f"ExtractV2: LLM bracket parsing disabled")
                bracket_parsing_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: LLM bracket parsing failed (non-fatal): {e}")
            bracket_parsing_stats = {"error": str(e)}
        
        # ✨ 8. 格式范本匹配与填充 - 自动识别并填充格式范本到节点正文（新增阶段5）
        template_matching_stats = {}
        try:
            if enable_template_matching:
                logger.info(f"ExtractV2: Starting template matching and auto-fill for project={project_id}")
                
                from app.works.tender.template_matcher import match_templates_to_nodes, auto_fill_template_bodies
                
                # 8.1 匹配范本到节点
                match_result = await match_templates_to_nodes(
                    nodes=nodes,
                    project_id=project_id,
                    pool=self.pool,
                    llm=self.llm,
                    model_id=model_id,
                    enable_matching=True,
                )
                
                matches = match_result.get("matches", [])
                match_stats = match_result.get("stats", {})
                
                # 8.2 自动填充匹配的范本
                fill_result = {}
                if matches:
                    fill_result = await auto_fill_template_bodies(
                        matches=matches,
                        project_id=project_id,
                        pool=self.pool,
                    )
                    
                    logger.info(
                        f"ExtractV2: Template auto-fill done - "
                        f"{fill_result.get('filled_count', 0)}/{len(matches)} nodes filled"
                    )
                
                template_matching_stats = {
                    "enabled": True,
                    **match_stats,
                    **fill_result,
                }
            else:
                logger.info(f"ExtractV2: Template matching disabled")
                template_matching_stats = {"enabled": False}
        except Exception as e:
            logger.warning(f"ExtractV2: Template matching failed (non-fatal): {e}")
            template_matching_stats = {"error": str(e)}
        
        # 9. 返回结果
        return {
            "data": {"nodes": nodes},
            "evidence_chunk_ids": result.evidence_chunk_ids,
            "evidence_spans": result.evidence_spans,
            "retrieval_trace": result.retrieval_trace.__dict__ if result.retrieval_trace else {},
            "generation_mode": generation_mode,
            "fast_stats": fast_stats if generation_mode in ["fast", "hybrid"] else {},
            "refinement_stats": refinement_stats,
            "bracket_parsing_stats": bracket_parsing_stats,
            "template_matching_stats": template_matching_stats,  # ✨ 新增：范本填充统计
        }
    
    def _generate_evidence_spans(
        self,
        chunks: List,
        evidence_chunk_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        生成 evidence_spans（基于 meta.page_no）
        此方法保留用于向后兼容，但新代码应使用 ExtractionEngine 内置方法
        
        Returns:
            [
                {
                    "source": "asset_id or doc_version_id",
                    "page_no": 5,
                    "snippet": "证据片段..."
                }
            ]
        """
        spans = []
        chunk_map = {c.chunk_id: c for c in chunks}
        
        for chunk_id in evidence_chunk_ids:
            chunk = chunk_map.get(chunk_id)
            if not chunk:
                continue
            
            meta = chunk.meta or {}
            page_no = meta.get("page_no") or meta.get("chunk_position", 0)
            doc_version_id = meta.get("doc_version_id", "")
            
            spans.append({
                "source": doc_version_id,
                "page_no": page_no,
                "snippet": chunk.text[:200]  # 只取前 200 字符作为片段
            })
        
        return spans
    
    async def _extract_stages_parallel(
        self,
        stages: List[Dict],
        spec: Any,
        project_id: str,
        model_id: Optional[str],
        run_id: Optional[str],
        embedding_provider: str,
        stage_results: Dict,
        all_evidence_chunk_ids: set,
        all_evidence_spans: List,
        all_traces: List,
    ) -> Dict[str, Any]:
        """
        并行执行所有Stage（6个Stage同时执行）
        
        实时更新message字段，展示所有正在进行的Stage
        """
        import json
        import asyncio
        from app.platform.extraction.parallel import ParallelExtractor, ParallelExtractionTask
        import time
        
        parallel_start = time.time()
        logger.info(f"[PARALLEL_TIMING] ========== PARALLEL EXTRACTION START at {parallel_start:.3f} ==========")
        logger.info(f"ExtractV2: Starting PARALLEL extraction with {len(stages)} stages")
        
        # 创建并行任务
        tasks = []
        for stage_info in stages:
            task = ParallelExtractionTask(
                task_id=f"stage_{stage_info['stage']}",
                spec=spec,
                project_id=project_id,
                stage=stage_info['stage'],
                stage_name=stage_info['name'],
                module_name="project_info",
            )
            tasks.append(task)
        
        # 跟踪正在执行的Stage
        active_stages = set()
        completed_stages = set()
        
        def update_progress_message():
            """更新进度消息，显示所有活跃的Stage"""
            if run_id:
                active_names = [stages[s-1]['name'] for s in sorted(active_stages)]
                completed_count = len(completed_stages)
                total = len(stages)
                progress = completed_count / total
                
                if active_names:
                    # 构建消息：正在抽取：项目概览、投标人资格、评审与评分...
                    msg = f"正在抽取：{('、').join(active_names)}"
                    if completed_count > 0:
                        msg += f" ({completed_count}/{total}已完成)"
                else:
                    msg = f"并行抽取完成 ({completed_count}/{total})"
                
                self.dao.update_run(run_id, "running", progress=progress, message=msg)
                logger.info(f"[ParallelExtract] {msg}")
        
        def on_task_start(stage_num: int):
            """任务开始回调"""
            active_stages.add(stage_num)
            update_progress_message()
        
        def on_task_complete(stage_num: int):
            """任务完成回调"""
            if stage_num in active_stages:
                active_stages.remove(stage_num)
            completed_stages.add(stage_num)
            update_progress_message()
        
        # 创建并行抽取器
        extractor = ParallelExtractor(max_concurrent=6)  # 6个Stage并发
        
        # 包装执行函数以添加回调
        async def execute_with_callbacks(task: ParallelExtractionTask):
            """执行单个任务并触发回调"""
            import time
            stage_num = task.stage
            stage_name = task.stage_name
            
            # 记录精确开始时间
            task_start = time.time()
            logger.info(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) START at {task_start:.3f}")
            
            # 开始回调
            on_task_start(stage_num)
            
            try:
                # 执行抽取
                result = await self.engine.run(
                    spec=task.spec,
                    retriever=self.retriever,
                    llm=self.llm,
                    project_id=task.project_id,
                    model_id=model_id,
                    run_id=run_id,
                    embedding_provider=embedding_provider,
                    stage=task.stage,
                    stage_name=task.stage_name,
                    module_name=task.module_name,
                )
                
                # 记录精确完成时间
                task_end = time.time()
                duration = task_end - task_start
                logger.info(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) END at {task_end:.3f}, duration={duration:.2f}s")
                
                # 完成回调
                on_task_complete(stage_num)
                
                return result
                
            except Exception as e:
                task_end = time.time()
                duration = task_end - task_start
                logger.error(f"[PARALLEL_TIMING] Stage {stage_num} ({stage_name}) FAILED at {task_end:.3f}, duration={duration:.2f}s, error={e}", exc_info=True)
                on_task_complete(stage_num)
                raise
        
        # 并行执行所有任务
        try:
            results = await asyncio.gather(*[execute_with_callbacks(task) for task in tasks], return_exceptions=True)
            
            # 处理结果
            for idx, result in enumerate(results):
                stage_info = stages[idx]
                stage_key = stage_info['key']
                
                if isinstance(result, Exception):
                    logger.error(f"ExtractV2: Stage {stage_info['stage']} failed: {result}")
                    stage_results[stage_key] = {}
                else:
                    # 提取数据
                    stage_data = result.data.get(stage_key) if isinstance(result.data, dict) else result.data
                    if not stage_data:
                        stage_data = {}
                    
                    stage_results[stage_key] = stage_data
                    
                    # 收集证据
                    all_evidence_chunk_ids.update(result.evidence_chunk_ids)
                    all_evidence_spans.extend(result.evidence_spans)
                    
                    # 收集追踪信息
                    if result.retrieval_trace:
                        all_traces.append({
                            "stage": stage_info['stage'],
                            "name": stage_info['name'],
                            "trace": result.retrieval_trace.__dict__
                        })
                    
                    logger.info(f"ExtractV2: Stage {stage_info['stage']}/6 completed in parallel mode")
            
        except Exception as e:
            logger.error(f"ExtractV2: Parallel extraction failed: {e}", exc_info=True)
            # 失败时设置默认值
            for stage_info in stages:
                if stage_info['key'] not in stage_results:
                    stage_results[stage_info['key']] = {}
        
        # 合并所有阶段结果（V3结构）
        final_data = {
            "schema_version": "tender_info_v3",
            **{stage["key"]: stage_results.get(stage["key"], {}) for stage in stages}
        }
        
        parallel_end = time.time()
        total_duration = parallel_end - parallel_start
        logger.info(f"[PARALLEL_TIMING] ========== PARALLEL EXTRACTION END at {parallel_end:.3f}, TOTAL={total_duration:.2f}s ==========")
        
        logger.info(
            f"ExtractV2: PARALLEL extraction completed - "
            f"stages_completed={len([r for r in results if not isinstance(r, Exception)])}/6"
        )
        
        # ❌ 已移除：追加调用 requirements 抽取
        # 原因：与单独的"招标要求提取"功能重复
        # 现在用户需要单独点击"招标要求提取"按钮来生成 tender_requirements
        
        return {
            "schema_version": "tender_info_v3",
            **final_data,
            "evidence_chunk_ids": list(all_evidence_chunk_ids),
            "evidence_spans": all_evidence_spans,
            "retrieval_trace": all_traces,
        }
    
    async def extract_requirements_v2(
        self,
        project_id: str,
        model_id: Optional[str],
        checklist_template: str = "engineering",
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        抽取招标要求 (v2) - 框架式自主提取
        
        ✨ 新方法：系统定框架，LLM自主分析识别所有要求
        
        优势：
        1. 灵活性高：不受预设问题限制，能捕捉特殊和独特要求
        2. 完整性强：LLM主动搜索，不易遗漏
        3. 结构化输出：仍保持维度、类型等结构，便于审核
        4. 智能过滤：自动排除合同条款和格式范例
        
        Args:
            project_id: 项目ID
            model_id: 模型ID（默认使用全局配置）
            checklist_template: （已废弃，保留参数兼容性）
            run_id: 运行ID（可选）
        
        Returns:
            {
                "count": 提取的要求数量,
                "requirements": [...],
                "extraction_method": "framework_autonomous",
                "schema_version": "requirements_v2_framework"
            }
        """
        logger.info(
            f"ExtractV2: extract_requirements_v2 start (framework mode) "
            f"project_id={project_id}"
        )
        
        try:
            # 1. 使用框架式Prompt Builder
            from .framework_prompt_builder import FrameworkPromptBuilder
            
            prompt_builder = FrameworkPromptBuilder()
            logger.info("Using framework-guided autonomous extraction")
            
            # 2. 检索招标文件上下文
            logger.info("Retrieving tender document context...")
            
            # 使用RetrievalFacade检索
            # ✅ 扩展查询词以支持多种招标/采购类型（工程、货物、服务、磋商等）
            context_chunks = await self.retriever.retrieve(
                query="招标文件 投标人须知 评分标准 技术要求 资格条件 商务条款 工期 质保 价格 磋商 资信 报价 方案 合同 授权 资质 保证金 承诺 证明 材料",
                project_id=project_id,
                doc_types=["tender"],
                top_k=150,  # 获取足够多的上下文
            )
            
            logger.info(f"Retrieved {len(context_chunks)} context chunks")
            
            # 拼接上下文（使用实际segment_id作为标记）
            context_text = "\n\n".join([
                f"[SEG:{chunk.chunk_id}] {chunk.text}"
                for i, chunk in enumerate(context_chunks[:100])  # 限制token数
            ])
            
            # 构建segment_id映射表（用于后续evidence验证）
            segment_id_map = {chunk.chunk_id: chunk for chunk in context_chunks[:100]}
            
            if len(context_text) < 100:
                logger.warning("Context text too short, may not have enough information")
            
            # 3. 构建Prompt并调用LLM（框架式自主提取）
            prompt = prompt_builder.build_prompt(context_text)
            
            logger.info(f"Built framework prompt, length: {len(prompt)} chars")
            
            # 调用LLM进行自主提取
            messages = [{"role": "user", "content": prompt}]
            llm_response = await self.llm.achat(
                messages=messages,
                model_id=model_id,
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=16000,  # 自主提取可能输出较多要求
            )
            
            # 提取content
            llm_output = llm_response.get("choices", [{}])[0].get("message", {}).get("content")
            if llm_output is None:
                llm_output = "[]"  # Fallback to empty array
                logger.warning("LLM returned None content, using empty array")
            
            logger.info(f"Got LLM response, length: {len(llm_output)} chars")
            
            # 4. 解析LLM返回的要求列表
            try:
                llm_requirements = prompt_builder.parse_llm_response(llm_output)
                logger.info(f"Parsed {len(llm_requirements)} requirements from LLM")
            except Exception as e:
                logger.error(f"Failed to parse LLM response: {e}")
                return {
                    "count": 0,
                    "requirements": [],
                    "error": f"LLM response parsing failed: {str(e)}",
                    "schema_version": "requirements_v2_framework"
                }
            
            # 5. 验证并转换为数据库格式
            # 获取文档版本ID
            doc_version_id = await self._get_doc_version_id(project_id, "tender")
            
            requirements = prompt_builder.convert_to_db_format(
                llm_requirements=llm_requirements,
                project_id=project_id,
                doc_version_id=doc_version_id or 0,
            )
            
            logger.info(f"Converted to DB format: {len(requirements)} requirements")
            
            # 6. 去重（基于内容相似度）
            seen_texts = {}
            unique_requirements = []
            for req in requirements:
                text = req.get("requirement_text", "").strip()
                text_normalized = text[:100].lower()  # 使用前100字符作为指纹
                
                if text_normalized and text_normalized not in seen_texts:
                    seen_texts[text_normalized] = req.get("item_id")
                    unique_requirements.append(req)
                else:
                    logger.warning(f"Duplicate content: {req.get('item_id')}")
            
            requirements = unique_requirements
            logger.info(f"After deduplication: {len(requirements)} requirements")
            
            # 7. 后处理：推断eval_method, must_reject等字段
            from .requirement_postprocessor import generate_bid_response_extraction_guide
            
            for req in requirements:
                # 推断审核方法和权重
                eval_method, must_reject, expected_evidence, rubric, weight = self._infer_routing_fields(req)
                
                req["eval_method"] = req.get("eval_method") or eval_method
                req["must_reject"] = req.get("must_reject") or must_reject
                if expected_evidence and not req.get("expected_evidence_json"):
                    req["expected_evidence_json"] = expected_evidence
                if rubric:
                    req["rubric_json"] = rubric
                if weight is not None:
                    req["weight"] = weight
            
            logger.info(f"Applied eval_method inference to {len(requirements)} requirements")
            
            # 8. 保存到数据库
            logger.info("Saving requirements to database...")
            
            import uuid
            from psycopg.types.json import Json
            
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    # 先删除该项目的旧requirements（避免重复）
                    cur.execute("DELETE FROM tender_requirements WHERE project_id = %s", (project_id,))
                    logger.info(f"Deleted old requirements for project {project_id}")
                    
                    for req in requirements:
                        # 处理JSONB字段
                        value_schema = req.get("value_schema_json")
                        if value_schema and not isinstance(value_schema, Json):
                            value_schema = Json(value_schema)
                        
                        expected_evidence_json = req.get("expected_evidence_json")
                        if expected_evidence_json and not isinstance(expected_evidence_json, Json):
                            expected_evidence_json = Json(expected_evidence_json)
                        
                        rubric_json = req.get("rubric_json")
                        if rubric_json and not isinstance(rubric_json, Json):
                            rubric_json = Json(rubric_json)
                        
                        # 映射字段名：requirement_type -> req_type, is_mandatory -> is_hard
                        req_type = req.get("requirement_type") or req.get("req_type", "semantic")
                        is_hard = req.get("is_mandatory") or req.get("is_hard", False)
                        requirement_id = req.get("item_id") or req.get("requirement_id", f"auto_{uuid.uuid4().hex[:8]}")
                        
                        # 合并meta_json到value_schema_json
                        meta_json = req.get("meta_json", {})
                        if meta_json and value_schema:
                            # 如果已有value_schema，合并meta信息
                            combined_schema = value_schema if isinstance(value_schema, dict) else {}
                            combined_schema.update({"meta": meta_json})
                            value_schema = Json(combined_schema)
                        elif meta_json and not value_schema:
                            # 如果没有value_schema，将meta作为value_schema
                            value_schema = Json(meta_json)
                        
                        cur.execute("""
                            INSERT INTO tender_requirements (
                                id, project_id, requirement_id, dimension, req_type,
                                requirement_text, is_hard, allow_deviation, 
                                value_schema_json, evidence_chunk_ids,
                                eval_method, must_reject, expected_evidence_json, rubric_json, weight
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            str(uuid.uuid4()),
                            project_id,
                            requirement_id,
                            req.get("dimension", "other"),
                            req_type,
                            req.get("requirement_text", ""),
                            is_hard,
                            req.get("allow_deviation", True),
                            value_schema,
                            req.get("evidence_chunk_ids", []),
                            req.get("eval_method", "SEMANTIC"),
                            req.get("must_reject", False),
                            expected_evidence_json,
                            rubric_json,
                            req.get("weight", 1.0),
                        ))
                    conn.commit()
            
            logger.info(f"ExtractV2: Saved {len(requirements)} requirements to DB")
            
            # 9. 生成extraction_guide（用于后续投标响应抽取）
            logger.info("Generating extraction guide for bid responses...")
            try:
                extraction_guide = generate_bid_response_extraction_guide(requirements)
                
                # 保存到tender_projects.meta_json
                await self._update_project_meta(project_id, {
                    "extraction_guide": extraction_guide
                })
                
                logger.info(f"Extraction guide generated: {len(extraction_guide.get('categories', []))} categories")
            except Exception as e:
                logger.warning(f"Failed to generate extraction guide: {e}")
            
            # 10. 统计维度分布
            dimension_stats = {}
            for req in requirements:
                dim = req.get("dimension", "other")
                dimension_stats[dim] = dimension_stats.get(dim, 0) + 1
            
            logger.info(
                f"ExtractV2: extract_requirements_v2 complete - "
                f"{len(requirements)} requirements extracted, "
                f"dimensions: {dimension_stats}"
            )
            
            return {
                "count": len(requirements),
                "requirements": requirements,
                "dimension_distribution": dimension_stats,
                "extraction_method": "framework_autonomous",
                "schema_version": "requirements_v2_framework",
            }
        
        except Exception as e:
            logger.error(f"ExtractV2: extract_requirements_v2 failed: {e}", exc_info=True)
            raise
    
    async def _get_project_name(self, project_id: str) -> str:
        """获取项目名称"""
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT name FROM tender_projects WHERE id = %s",
                        (project_id,)
                    )
                    row = cur.fetchone()
                    
                    if row:
                        return row[0]
        except Exception as e:
            logger.warning(f"Failed to get project name: {e}")
        
        return "本项目"
    
    async def _get_doc_version_id(self, project_id: str, doc_type: str) -> Optional[int]:
        """获取文档版本ID"""
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        "SELECT id FROM doc_versions WHERE project_id = %s AND doc_type = %s ORDER BY uploaded_at DESC LIMIT 1",
                        (project_id, doc_type)
                    )
                    row = cur.fetchone()
                    
                    if row:
                        return row[0]
        except Exception as e:
            logger.warning(f"Failed to get doc_version_id: {e}")
        
        return None
    
    async def _update_project_meta(self, project_id: str, meta_update: Dict[str, Any]):
        """更新项目meta_json"""
        from psycopg.types.json import Json
        
        try:
            with self.pool.connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        UPDATE tender_projects
                        SET meta_json = COALESCE(meta_json, '{}'::jsonb) || %s::jsonb
                        WHERE id = %s
                        """,
                        (Json(meta_update), project_id)
                    )
                conn.commit()
                
                logger.info(f"Updated project meta for {project_id}")
        except Exception as e:
            logger.error(f"Failed to update project meta: {e}")
    
    async def _supplement_requirements_fulltext(
        self,
        project_id: str,
        model_id: Optional[str],
        existing_requirements: List[Dict[str, Any]],
        context_chunks: List[Any],
    ) -> List[Dict[str, Any]]:
        """
        P1优化：全文补充提取Checklist未覆盖的招标要求
        
        策略：
        1. 让LLM全文扫描招标文件
        2. 识别Checklist未覆盖的要求
        3. 特别关注：技术参数表、功能清单、特殊条款、附件要求、隐含要求
        
        Args:
            project_id: 项目ID
            model_id: 模型ID
            existing_requirements: 已提取的要求列表
            context_chunks: 检索到的上下文chunks
        
        Returns:
            补充的要求列表
        """
        logger.info(f"[补充提取] 开始全文扫描，已有 {len(existing_requirements)} 条要求")
        
        try:
            # 1. 构建已提取要求的摘要（用于去重）
            existing_summary = []
            for req in existing_requirements[:50]:  # 限制摘要长度
                dim = req.get("dimension", "")
                text = req.get("requirement_text", "")[:60]
                existing_summary.append(f"[{dim}] {text}")
            
            existing_text = "\n".join(existing_summary)
            
            # 2. 准备全文上下文（使用更多chunks）
            fulltext_context = "\n\n".join([
                f"[SEG:{chunk.chunk_id}] {chunk.text}"
                for chunk in context_chunks[:150]  # 扩展到150个chunks
            ])
            
            # 3. 构建补充提取prompt
            supplement_prompt = f"""# 招标要求补充提取任务

## 背景
已通过标准清单提取了 {len(existing_requirements)} 条招标要求，现需要全文扫描招标文件，识别遗漏的要求。

## 已提取要求摘要（前50条）
{existing_text}

## 招标文件全文
{fulltext_context}

## 任务要求
请全文扫描招标文件，识别以下**未被现有清单覆盖的要求**：

### 重点关注（高频遗漏）
1. **技术参数表、功能清单**：详细的技术指标、性能要求
2. **特殊条款、补充说明**：合同特殊条款、附加要求
3. **附件清单**：必须提供的附件、证明材料
4. **评分项隐含要求**：评分标准中隐含的交付物要求
5. **投标书目录结构要求**：章节组成、格式要求
6. **偏离表要求**：技术/商务偏离表
7. **样品、演示要求**：如有
8. **特定资质/认证要求**：行业特定资质

### 输出格式
返回JSON数组，**只包含新发现的、未被已提取要求覆盖的条目**：

```json
{{
  "supplement_requirements": [
    {{
      "dimension": "qualification|technical|business|price|doc_structure|schedule_quality|other",
      "req_type": "具体类型",
      "requirement_text": "要求内容（详细且完整）",
      "is_hard": true/false,
      "eval_method": "PRESENCE|NUMERIC|EXACT_MATCH|SEMANTIC",
      "evidence_segment_ids": ["seg_xxx", "seg_yyy"],
      "reasoning": "为什么这是一个新要求（未被已提取清单覆盖）"
    }}
  ]
}}
```

### 去重原则
- **仔细对比已提取清单**：如果某要求已被覆盖（即使表述不同），**不要重复提取**
- **宁缺毋滥**：不确定是否重复时，选择不提取
- **聚焦特殊性**：优先提取特殊、具体的要求，而非通用要求

### 示例
✅ 应该补充：
- "提供XX品牌认证证书或同等产品认证"（特定认证要求）
- "技术方案应包含系统架构图、数据流图、部署拓扑图"（具体交付物）
- "投标书须提供原厂授权书原件"（特定文件要求）

❌ 不应补充（已被清单覆盖）：
- "营业执照"（清单qual_001已覆盖）
- "资质证书"（清单qual_002已覆盖）
- "投标总价"（清单price_001已覆盖）

请开始分析并输出JSON。"""

            # 4. 调用LLM
            messages = [{"role": "user", "content": supplement_prompt}]
            llm_response = await self.llm.achat(
                messages=messages,
                model_id=model_id,
                response_format={"type": "json_object"},
                temperature=0.1,
                max_tokens=4096,
            )
            
            # 5. 解析响应
            import json
            llm_output = llm_response.get("choices", [{}])[0].get("message", {}).get("content")
            if not llm_output:
                logger.warning("[补充提取] LLM返回空内容")
                return []
            
            try:
                result_data = json.loads(llm_output)
                supplement_items = result_data.get("supplement_requirements", [])
            except json.JSONDecodeError as e:
                logger.error(f"[补充提取] JSON解析失败: {e}")
                return []
            
            if not supplement_items:
                logger.info("[补充提取] 未发现额外要求")
                return []
            
            logger.info(f"[补充提取] LLM返回 {len(supplement_items)} 条补充要求")
            
            # 6. 转换为标准格式
            import uuid
            supplement_requirements = []
            
            for idx, item in enumerate(supplement_items):
                requirement_id = f"supplement_{idx+1:03d}"
                
                supplement_requirements.append({
                    "project_id": project_id,
                    "requirement_id": requirement_id,
                    "dimension": item.get("dimension", "other"),
                    "req_type": item.get("req_type", "other"),
                    "requirement_text": item.get("requirement_text", ""),
                    "is_hard": item.get("is_hard", False),
                    "allow_deviation": not item.get("is_hard", False),
                    "eval_method": item.get("eval_method", "SEMANTIC"),
                    "must_reject": False,
                    "evidence_chunk_ids": item.get("evidence_segment_ids", []),
                    "meta_json": {
                        "source": "fulltext_supplement",
                        "reasoning": item.get("reasoning", ""),
                    }
                })
            
            logger.info(f"[补充提取] 成功转换 {len(supplement_requirements)} 条补充要求")
            return supplement_requirements
            
        except Exception as e:
            logger.error(f"[补充提取] 失败: {e}", exc_info=True)
            return []
            raise
    
    async def prepare_tender_for_audit(
        self,
        project_id: str,
        model_id: Optional[str] = None,
        checklist_template: str = "engineering",
    ) -> Dict[str, Any]:
        """
        招标侧统一准备（一次性完成：项目信息 + 招标要求）
        
        优势：
        1. 一次检索，两次使用（复用上下文）
        2. 数据一致性（基准信息和规则库来自相同证据）
        3. 减少成本（避免重复检索）
        
        Args:
            project_id: 项目ID
            model_id: 模型ID
            checklist_template: 清单模板名称
            
        Returns:
            {
                "project_info": {...},  # 项目信息抽取结果
                "requirements": {...},  # 招标要求抽取结果
                "extraction_guide": {...},  # 提取指南
                "context_stats": {
                    "total_chunks": 150,
                    "used_chunks": 100,
                    "context_length": 50000
                }
            }
        """
        logger.info(
            f"ExtractV2: prepare_tender_for_audit start "
            f"project_id={project_id}, checklist_template={checklist_template}"
        )
        
        try:
            # 1. 使用公共检索组件获取招标上下文（一次检索）
            from .tender_context_retriever import TenderContextRetriever
            
            context_retriever = TenderContextRetriever(self.retriever)
            
            logger.info("Step 1: 检索招标文档上下文（公共检索）...")
            context_data = await context_retriever.retrieve_tender_context(
                project_id=project_id,
                top_k=150,
                max_context_chunks=100,
                sort_by_position=True,
            )
            
            logger.info(
                f"上下文检索完成: total={context_data.total_chunks}, "
                f"used={context_data.used_chunks}, length={len(context_data.context_text)}"
            )
            
            if context_data.used_chunks == 0:
                raise ValueError(f"未检索到招标文档内容，project_id={project_id}")
            
            # 2. 使用同一份上下文先抽取项目信息
            logger.info("Step 2: 抽取项目信息（复用上下文）...")
            project_info_result = await self._extract_project_info_with_context(
                project_id=project_id,
                model_id=model_id,
                context_data=context_data,
            )
            
            logger.info(f"项目信息抽取完成: {project_info_result.get('status')}")
            
            # 3. 再用同一份上下文抽取招标要求
            logger.info("Step 3: 抽取招标要求（复用上下文）...")
            requirements_result = await self._extract_requirements_with_context(
                project_id=project_id,
                model_id=model_id,
                checklist_template=checklist_template,
                context_data=context_data,
            )
            
            logger.info(
                f"招标要求抽取完成: {requirements_result.get('count')} 条要求"
            )
            
            # 4. 生成并保存 extraction_guide（统一键名）
            logger.info("Step 4: 生成投标响应提取指南...")
            from .requirement_postprocessor import generate_bid_response_extraction_guide
            
            requirements = requirements_result.get("requirements", [])
            extraction_guide = generate_bid_response_extraction_guide(requirements)
            
            # 保存到 tender_projects.meta_json（使用统一键名 "extraction_guide"）
            await self._update_project_meta(project_id, {
                "extraction_guide": extraction_guide
            })
            
            logger.info("提取指南已生成并保存")
            
            # 5. 返回完整结果
            result = {
                "project_info": project_info_result,
                "requirements": requirements_result,
                "extraction_guide": extraction_guide,
                "context_stats": {
                    "total_chunks": context_data.total_chunks,
                    "used_chunks": context_data.used_chunks,
                    "context_length": len(context_data.context_text),
                },
                "status": "success",
            }
            
            logger.info(
                f"ExtractV2: prepare_tender_for_audit complete - "
                f"项目信息=OK, 招标要求={requirements_result.get('count')}条, "
                f"extraction_guide=已生成"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"ExtractV2: prepare_tender_for_audit failed: {e}", exc_info=True)
            raise
    
    async def _extract_project_info_with_context(
        self,
        project_id: str,
        model_id: Optional[str],
        context_data,
    ) -> Dict[str, Any]:
        """使用已检索的上下文抽取项目信息（内部方法）"""
        # TODO: 复用 extract_project_info_v2/v3 的逻辑，但使用提供的 context_data
        # 暂时调用原方法（后续可优化为直接使用 context_data）
        return await self.extract_project_info_v2(
            project_id=project_id,
            model_id=model_id,
            use_staged=False,
        )
    
    async def _extract_requirements_with_context(
        self,
        project_id: str,
        model_id: Optional[str],
        checklist_template: str,
        context_data,
    ) -> Dict[str, Any]:
        """使用已检索的上下文抽取招标要求（内部方法）"""
        # TODO: 复用 extract_requirements_v2 的逻辑，但使用提供的 context_data
        # 暂时调用原方法（后续可优化为直接使用 context_data）
        return await self.extract_requirements_v2(
            project_id=project_id,
            model_id=model_id,
            checklist_template=checklist_template,
        )
    
    def _filter_out_format_and_contract(self, requirements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        过滤排除合同条款和投标文件格式范例
        
        排除规则：
        1. 合同类：requirement_text包含"合同"、"协议"、"甲方"、"乙方"等
        2. 格式类：requirement_text包含"格式"、"范本"、"样本"、"模板"等
        
        Args:
            requirements: 原始要求列表
            
        Returns:
            过滤后的要求列表
        """
        # 合同关键词
        contract_keywords = [
            "合同范本", "合同草案", "拟签订的合同", "合同协议书", "合同文本", "合同条款",
            "甲方应", "乙方应", "甲方负责", "乙方负责", "甲方权利", "乙方义务",
            "违约责任", "争议解决", "合同签订", "合同生效", "合同终止"
        ]
        
        # 格式范例关键词
        format_keywords = [
            "投标文件格式", "编制格式", "参考格式", "格式范本", "格式要求",
            "样本", "样表", "模板", "范本", "格式如下", "格式见附件",
            "授权书格式", "承诺函格式", "报价表格式", "封面格式"
        ]
        
        filtered = []
        
        for req in requirements:
            requirement_text = req.get("requirement_text", "")
            if not requirement_text:
                filtered.append(req)
                continue
            
            # 检查是否包含合同关键词
            is_contract = any(keyword in requirement_text for keyword in contract_keywords)
            
            # 检查是否包含格式关键词
            is_format = any(keyword in requirement_text for keyword in format_keywords)
            
            # 不是合同也不是格式，保留
            if not is_contract and not is_format:
                filtered.append(req)
            else:
                reason = "合同条款" if is_contract else "格式范例"
                logger.debug(f"Filtered out requirement ({reason}): {requirement_text[:50]}...")
        
        return filtered


