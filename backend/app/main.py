from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .routers import (
    health,
    chat,
    llm,
    export,
    template_analysis,
    llm_config,
    app_settings,
    history,
    kb,
    kb_category,
    embedding_providers,
    auth,
    asr_ws,
    recordings,
    asr_configs,
    attachments,
    tender,
    tender_snippets,
    debug,
    declare,
    permissions,
    custom_rules,
    user_documents,
    organizations,  # æ–°å¢ä¼ä¸šç®¡ç†è·¯ç”±
)
from .services.db.postgres import init_db, _get_pool
from .services.llm_client import get_default_llm_model
import httpx
import json
import logging
import asyncio

logger = logging.getLogger(__name__)

init_db()
app = FastAPI(title="äº¿æ—äº¿é—® Backend", version="0.2.0")


# ========== åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† ==========

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ğŸš€ åº”ç”¨å¯åŠ¨ä¸­...")
    
    # å¯åŠ¨ä»»åŠ¡ç›‘æ§å™¨ï¼ˆè‡ªåŠ¨æ¸…ç†å¡æ­»çš„ä»»åŠ¡ï¼‰
    try:
        import os
        import psycopg
        from psycopg.rows import dict_row
        from app.services.task_monitor import TaskMonitor
        
        # åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥ï¼ˆä¸ä½¿ç”¨è¿æ¥æ± ï¼‰
        monitor_conn = psycopg.connect(
            host=os.getenv("POSTGRES_HOST", "postgres"),
            port=int(os.getenv("POSTGRES_PORT", "5432")),
            dbname=os.getenv("POSTGRES_DB", "localgpt"),
            user=os.getenv("POSTGRES_USER", "localgpt"),
            password=os.getenv("POSTGRES_PASSWORD", "localgpt"),
            row_factory=dict_row,
            autocommit=False
        )
        
        # åˆ›å»ºå¹¶å¯åŠ¨ç›‘æ§å™¨
        # è¶…æ—¶é˜ˆå€¼ï¼š10åˆ†é’Ÿï¼Œæ£€æŸ¥é—´éš”ï¼š60ç§’
        monitor = TaskMonitor(monitor_conn, timeout_minutes=20, check_interval_seconds=60)
        
        # ä¿å­˜åˆ°app.stateä»¥ä¾¿åç»­è®¿é—®
        app.state.task_monitor = monitor
        app.state.task_monitor_conn = monitor_conn
        
        # å¯åŠ¨ç›‘æ§å¾ªç¯
        monitor.start()
        
        logger.info("âœ… ä»»åŠ¡ç›‘æ§å™¨å·²å¯åŠ¨ï¼ˆè¶…æ—¶é˜ˆå€¼ï¼š10åˆ†é’Ÿï¼Œæ£€æŸ¥é—´éš”ï¼š60ç§’ï¼‰")
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡ç›‘æ§å™¨å¯åŠ¨å¤±è´¥: {e}", exc_info=True)


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    logger.info("ğŸ›‘ åº”ç”¨å…³é—­ä¸­...")
    
    # åœæ­¢ä»»åŠ¡ç›‘æ§å™¨
    if hasattr(app.state, "task_monitor"):
        try:
            await app.state.task_monitor.stop()
            logger.info("âœ… ä»»åŠ¡ç›‘æ§å™¨å·²åœæ­¢")
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ç›‘æ§å™¨åœæ­¢å¤±è´¥: {e}")
    
    # å…³é—­æ•°æ®åº“è¿æ¥
    if hasattr(app.state, "task_monitor_conn"):
        try:
            app.state.task_monitor_conn.close()
            logger.info("âœ… ä»»åŠ¡ç›‘æ§å™¨æ•°æ®åº“è¿æ¥å·²å…³é—­")
        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡ç›‘æ§å™¨æ•°æ®åº“è¿æ¥å…³é—­å¤±è´¥: {e}")


# LLM Orchestrator åŒ…è£…å™¨ - ç”¨äº TenderService
class SimpleLLMOrchestrator:
    """
    ç®€å•çš„ LLM orchestrator åŒ…è£…å™¨ï¼Œå…¼å®¹ TenderService çš„ duck typing æ¥å£
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. MOCK æ¨¡å¼ï¼ˆMOCK_LLM=true + DEBUG=trueï¼‰ï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼Œä¸è®¿é—®å¤–éƒ¨æœåŠ¡
    2. REAL æ¨¡å¼ï¼šè°ƒç”¨çœŸå® LLMï¼ˆé€šè¿‡ llm_models è¡¨é…ç½®ï¼‰
    
    åˆå§‹åŒ–ç­–ç•¥ï¼ˆå¯é™çº§ï¼‰ï¼š
    - MOCK æ¨¡å¼ï¼šæ— éœ€ä»»ä½•å¤–éƒ¨ä¾èµ–ï¼Œç›´æ¥è¿”å› mock æ•°æ®
    - REAL æ¨¡å¼ï¼šä¾èµ– llm_models è¡¨é…ç½®ï¼Œå¦‚æœé…ç½®ç¼ºå¤±ï¼Œåœ¨è°ƒç”¨æ—¶æŠ›å‡ºæ˜ç¡®é”™è¯¯
    """
    
    def chat(self, messages: list, model_id: str = None, **kwargs) -> dict:
        """
        è°ƒç”¨ LLM ç”Ÿæˆå›ç­”ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            model_id: å¯é€‰çš„æ¨¡å‹ ID
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokens, top_p ç­‰ï¼‰
        
        Returns:
            OpenAI æ ¼å¼çš„å“åº”: {"choices": [{"message": {"content": "..."}}]}
        
        Raises:
            RuntimeError: å½“ LLM è°ƒç”¨å¤±è´¥æ—¶
        """
        # æ£€æŸ¥ MOCK_LLM æ¨¡å¼
        # å®‰å…¨æ£€æŸ¥ï¼šMOCK_LLM åªåœ¨ DEBUG=true æ—¶å…è®¸ç”Ÿæ•ˆ
        import os
        mock_llm_enabled = os.getenv("MOCK_LLM", "false").lower() in ("true", "1", "yes")
        debug_enabled = os.getenv("DEBUG", "false").lower() in ("true", "1", "yes")
        
        if mock_llm_enabled:
            if not debug_enabled:
                logger.warning("[SimpleLLMOrchestrator] MOCK_LLM=true but DEBUG=false, MOCK ignored for safety")
                mock_llm_enabled = False
            else:
                logger.info("[SimpleLLMOrchestrator] MOCK_LLM enabled, returning mock response")
        
        # MOCK æ¨¡å¼ï¼šè¿”å›æ¨¡æ‹Ÿæ•°æ®ï¼ˆä¸è®¿é—®ä»»ä½•å¤–éƒ¨æœåŠ¡ï¼Œä¸æŸ¥è¯¢ DBï¼‰
        if mock_llm_enabled:
            # è¿”å›ä¸€ä¸ªç¬¦åˆ prompt è¦æ±‚çš„å››æ¿å— JSON
            mock_response = {
                "data": {
                    "base": {
                        "projectName": "æµ‹è¯•é¡¹ç›®",
                        "ownerName": "æµ‹è¯•æ‹›æ ‡äºº",
                        "agencyName": "æµ‹è¯•ä»£ç†æœºæ„",
                        "bidDeadline": "2024-12-31",
                        "bidOpeningTime": "2024-12-31 10:00",
                        "budget": "100ä¸‡å…ƒ",
                        "maxPrice": "",
                        "bidBond": "2ä¸‡å…ƒ",
                        "schedule": "60ä¸ªå·¥ä½œæ—¥",
                        "quality": "åˆæ ¼",
                        "location": "æµ‹è¯•åœ°ç‚¹",
                        "contact": "å¼ ä¸‰ 13800138000"
                    },
                    "technical_parameters": [
                        {
                            "category": "æŠ€æœ¯å‚æ•°",
                            "item": "æµ‹è¯•é¡¹",
                            "requirement": "æµ‹è¯•è¦æ±‚",
                            "parameters": [],
                            "evidence_chunk_ids": ["CHUNK_001"]
                        }
                    ],
                    "business_terms": [
                        {
                            "term": "ä»˜æ¬¾æ–¹å¼",
                            "requirement": "åˆåŒç­¾è®¢åæ”¯ä»˜30%",
                            "evidence_chunk_ids": ["CHUNK_002"]
                        }
                    ],
                    "scoring_criteria": {
                        "evaluationMethod": "ç»¼åˆè¯„åˆ†æ³•",
                        "items": [
                            {
                                "category": "ä»·æ ¼",
                                "item": "æŠ•æ ‡æŠ¥ä»·",
                                "score": "30",
                                "rule": "æœ€ä½ä»·å¾—æ»¡åˆ†",
                                "evidence_chunk_ids": ["CHUNK_003"]
                            }
                        ]
                    }
                },
                "evidence_chunk_ids": ["CHUNK_001", "CHUNK_002", "CHUNK_003"]
            }
            return {"choices": [{"message": {"content": json.dumps(mock_response, ensure_ascii=False)}}]}
        
        # REAL æ¨¡å¼ï¼šè°ƒç”¨çœŸå® LLM
        try:
            # è·å–æ¨¡å‹é…ç½®ï¼ˆä¾èµ– llm_models è¡¨ï¼‰
            if model_id:
                from app.services.llm_client import get_llm_model_by_id
                model = get_llm_model_by_id(model_id)
            else:
                model = get_default_llm_model()
            
            if not model:
                error_msg = (
                    "No LLM model configured. "
                    "Please ensure llm_models table has at least one active model. "
                    "Alternatively, set MOCK_LLM=true and DEBUG=true for testing."
                )
                logger.error(f"[SimpleLLMOrchestrator] {error_msg}")
                raise RuntimeError(error_msg)
            
            # æ„å»ºè¯·æ±‚ URL
            base_url = model.base_url.rstrip("/")
            endpoint_path = model.endpoint_path or "/v1/chat/completions"
            
            # å•ç‚¹å…œåº•ï¼šç¡®ä¿ max_tokens æœ‰åˆç†é»˜è®¤å€¼
            # å¦‚æœè°ƒç”¨æ–¹æ²¡ä¼  max_tokensï¼Œé»˜è®¤ç»™ 4096
            if "max_tokens" not in kwargs:
                kwargs["max_tokens"] = 4096
                logger.debug(f"[SimpleLLMOrchestrator] max_tokens not provided, defaulting to 4096")
            
            # åˆ¤æ–­è¯·æ±‚ç±»å‹
            if "ollama" in base_url.lower():
                # Ollama æ ¼å¼
                endpoint = f"{base_url}/api/chat"
                payload = {
                    "model": model.model,
                    "messages": messages,
                    "stream": False,
                }
                # åº”ç”¨è¦†ç›–å‚æ•°
                options = {}
                if "temperature" in kwargs:
                    options["temperature"] = kwargs["temperature"]
                if "max_tokens" in kwargs:
                    options["num_predict"] = kwargs["max_tokens"]
                if options:
                    payload["options"] = options
            else:
                # OpenAI å…¼å®¹æ ¼å¼
                endpoint = f"{base_url}{endpoint_path}"
                payload = {
                    "model": model.model,
                    "messages": messages,
                    "stream": False,
                }
                # åº”ç”¨è¦†ç›–å‚æ•°
                if "temperature" in kwargs:
                    payload["temperature"] = kwargs["temperature"]
                if "max_tokens" in kwargs:
                    payload["max_tokens"] = kwargs["max_tokens"]
                if "top_p" in kwargs:
                    payload["top_p"] = kwargs["top_p"]
            
            # å‡†å¤‡è¯·æ±‚å¤´
            headers = {"Content-Type": "application/json"}
            if model.api_key:
                headers["Authorization"] = f"Bearer {model.api_key}"
            
            # å‘é€åŒæ­¥è¯·æ±‚ï¼ˆå¢åŠ è¶…æ—¶æ—¶é—´åˆ°600ç§’ï¼Œç”¨äºå¤„ç†å¤§æ–‡æœ¬å’Œ16K tokensè¾“å‡ºï¼‰
            # æ³¨æ„ï¼šverify=False ç”¨äºè·³è¿‡SSLè¯ä¹¦éªŒè¯ï¼ˆè‡ªç­¾åè¯ä¹¦ï¼‰
            actual_max_tokens = kwargs.get('max_tokens', 'default')
            
            # è¯¦ç»†çš„è°ƒç”¨ä¿¡æ¯
            logger.info("=" * 80)
            logger.info("[LLMè°ƒç”¨] å¼€å§‹")
            logger.info(f"  ç«¯ç‚¹: {endpoint}")
            logger.info(f"  æ¨¡å‹: {model.model}")
            logger.info(f"  max_tokens: {payload.get('max_tokens', 'not set')}")
            logger.info(f"  temperature: {payload.get('temperature', 'not set')}")
            logger.info(f"  top_p: {payload.get('top_p', 'not set')}")
            logger.info(f"  æ¶ˆæ¯æ•°é‡: {len(messages)}")
            if messages:
                first_msg = messages[0]
                content_preview = first_msg.get('content', '')[:200]
                logger.info(f"  ç¬¬ä¸€æ¡æ¶ˆæ¯é¢„è§ˆ: {content_preview}...")
            logger.info(f"  è¯·æ±‚å¤´: Authorization={'å·²è®¾ç½®' if model.api_key else 'æœªè®¾ç½®'}")
            logger.info("=" * 80)
            
            with httpx.Client(timeout=600.0, verify=False) as client:
                response = client.post(endpoint, json=payload, headers=headers)
                response.raise_for_status()
                result = response.json()
            
            logger.info("=" * 80)
            logger.info("[LLMå“åº”] å®Œæˆ")
            logger.info(f"  çŠ¶æ€ç : {response.status_code}")
            logger.info(f"  å“åº”é•¿åº¦: {len(response.text)} å­—ç¬¦")
            logger.info("=" * 80)
            
            # æ£€æŸ¥è¿”å›çš„ usage å’Œ finish_reason
            if "usage" in result:
                print(f"[DEBUG] LLM usage: {json.dumps(result['usage'], ensure_ascii=False)}")
            
            if "choices" in result and result["choices"]:
                first_choice = result["choices"][0]
                finish_reason = first_choice.get("finish_reason", "unknown")
                print(f"[DEBUG] LLM finish_reason: {finish_reason}")
                
                if finish_reason == "length":
                    print(f"[WARNING] LLM stopped due to LENGTH limit!")
                elif finish_reason == "stop":
                    print(f"[INFO] LLM stopped naturally (stop sequence)")
                    
            # è¿”å›ç»Ÿä¸€æ ¼å¼
            if "choices" in result:
                return result
            elif "message" in result:  # Ollama æ ¼å¼
                return {
                    "choices": [{
                        "message": {
                            "content": result["message"].get("content", "")
                        }
                    }]
                }
            else:
                logger.warning(f"Unexpected LLM response format: {result}")
                return {"choices": [{"message": {"content": str(result)}}]}
                
        except httpx.ConnectTimeout as e:
            logger.error("=" * 80)
            logger.error("[LLMè°ƒç”¨å¤±è´¥] è¿æ¥è¶…æ—¶")
            logger.error(f"  é”™è¯¯: {e}")
            logger.error(f"  ç«¯ç‚¹: {endpoint if 'endpoint' in locals() else 'unknown'}")
            logger.error("  å¯èƒ½åŸå› :")
            logger.error("    1. LLMæœåŠ¡å™¨ä¸å¯è¾¾")
            logger.error("    2. ç½‘ç»œè¿æ¥é—®é¢˜")
            logger.error("    3. é˜²ç«å¢™é˜»æ­¢è¿æ¥")
            logger.error("=" * 80)
            raise RuntimeError(f"LLM è¯·æ±‚è¶…æ—¶ï¼Œæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨") from e
        except httpx.HTTPStatusError as e:
            logger.error("=" * 80)
            logger.error("[LLMè°ƒç”¨å¤±è´¥] HTTPé”™è¯¯")
            logger.error(f"  çŠ¶æ€ç : {e.response.status_code}")
            logger.error(f"  å“åº”: {e.response.text[:500]}")
            logger.error("=" * 80)
            raise RuntimeError(f"LLM è¯·æ±‚å¤±è´¥: HTTP {e.response.status_code}") from e
        except Exception as e:
            logger.error("=" * 80)
            logger.error("[LLMè°ƒç”¨å¤±è´¥] æœªçŸ¥é”™è¯¯")
            logger.error(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error("=" * 80)
            logger.error("è¯¦ç»†å †æ ˆ:", exc_info=True)
            raise RuntimeError(f"LLM è¯·æ±‚å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡") from e
    
    async def achat(self, messages: list, model_id: str = None, **kwargs) -> dict:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„ chat æ–¹æ³•
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user", "content": "..."}]
            model_id: å¯é€‰çš„æ¨¡å‹ ID
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆtemperature, max_tokens, top_p, response_format ç­‰ï¼‰
        
        Returns:
            OpenAI æ ¼å¼çš„å“åº”: {"choices": [{"message": {"content": "..."}}]}
        
        Raises:
            RuntimeError: å½“ LLM è°ƒç”¨å¤±è´¥æ—¶
        """
        import asyncio
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥çš„chatæ–¹æ³•
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, lambda: self.chat(messages, model_id, **kwargs))
    
    # ä¸ºå…¼å®¹æ€§æä¾›åˆ«å
    complete = chat
    generate = chat
    run = chat


# åˆå§‹åŒ–å¹¶æ³¨å…¥åˆ° app.state
app.state.llm_orchestrator = SimpleLLMOrchestrator()

# CORSï¼šå¼€å‘é˜¶æ®µå…ˆæ”¾å¼€ï¼Œç”Ÿäº§å¯æŒ‰åŸŸåæ”¶ç´§
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Force mode middleware (dev-only)
from .middleware.force_mode import ForceModeMiddleware
app.add_middleware(ForceModeMiddleware)

app.include_router(health.router)
app.include_router(debug.router)
app.include_router(auth.router)
app.include_router(asr_ws.router)
app.include_router(asr_configs.router)
app.include_router(recordings.router)
app.include_router(attachments.router)
app.include_router(chat.router)
app.include_router(llm.router)
app.include_router(llm_config.router)
app.include_router(app_settings.router)
app.include_router(embedding_providers.router)
app.include_router(history.router)
app.include_router(kb.router)
app.include_router(kb_category.router)
app.include_router(tender.router)
app.include_router(tender_snippets.router)
app.include_router(declare.router)
app.include_router(export.router)
app.include_router(template_analysis.router)

# Promptç®¡ç†
from app.routers import prompts
app.include_router(prompts.router)

# æƒé™ç®¡ç†
app.include_router(permissions.router)

# è‡ªå®šä¹‰è§„åˆ™ç®¡ç†
app.include_router(custom_rules.router)

# ç”¨æˆ·æ–‡æ¡£ç®¡ç†
app.include_router(user_documents.router)

# ä¼ä¸šç®¡ç†
app.include_router(organizations.router)

# Legacy tender APIs å·²åˆ é™¤
# if os.getenv("LEGACY_TENDER_APIS_ENABLED", "false").lower() in ("true", "1", "yes"):
#     ... (legacy APIs removed)


@app.get("/")
async def root():
    return {"message": "äº¿æ—äº¿é—® Backend is running"}
